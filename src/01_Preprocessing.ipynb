{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_Preprocessing\n",
    "Tips for data preprocessing to create search indexes for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document format examples and extraction tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Format Examples and Extraction Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD\n",
    "- サンプルユースケース：マニュアルの検索\n",
    "- データセットの用意：PDF, Word\n",
    "- サンプルコードの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.24.5-cp312-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting python-docx\n",
      "  Using cached python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting PyMuPDFb==1.24.3 (from PyMuPDF)\n",
      "  Using cached PyMuPDFb-1.24.3-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Downloading lxml-5.2.2-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading PyMuPDF-1.24.5-cp312-none-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/3.2 MB 2.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.4/3.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.8/3.2 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.2/3.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.6/3.2 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.2/3.2 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.6/3.2 MB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.1/3.2 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.2/3.2 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 7.9 MB/s eta 0:00:00\n",
      "Using cached PyMuPDFb-1.24.3-py3-none-win_amd64.whl (12.4 MB)\n",
      "Using cached python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading lxml-5.2.2-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 16.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.8/3.8 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.5/3.8 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.0/3.8 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.4/3.8 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.0/3.8 MB 11.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.3/3.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 8.7 MB/s eta 0:00:00\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, PyMuPDFb, lxml, python-docx, PyMuPDF, beautifulsoup4\n",
      "Successfully installed PyMuPDF-1.24.5 PyMuPDFb-1.24.3 beautifulsoup4-4.12.3 lxml-5.2.2 python-docx-1.1.2 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "! pip install PyMuPDF python-docx beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# PDF to text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# Word to text\n",
    "def extract_text_from_word(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "\n",
    "# HTML to text\n",
    "def extract_text_from_html(html_path):\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. This survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG. It aims to illuminate the evolution\\nof retrieval augmentation techniques, assess the strengths and\\nweaknesses of various approaches in their respective contexts,\\nand speculate on upcoming trends and innovations.\\nOur contributions are as follows:\\n• In this survey, we present a thorough and systematic\\nreview of the state-of-the-art RAG methods, delineating\\nits evolution through paradigms including naive RAG,\\narXiv:2312.10997v5  [cs.CL]  27 Mar 2024\\n2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current assessment methods of\\nRAG, covering 26 tasks, nearly 50 datasets, outlining\\nthe evaluation objectives and metrics, as well as the\\ncurrent evaluation benchmarks and tools. Additionally,\\nwe anticipate future directions for RAG, emphasizing\\npotential enhancements to tackle current challenges.\\nThe paper unfolds as follows: Section II introduces the\\nmain concept and current paradigms of RAG. The following\\nthree sections explore core components—“Retrieval”, “Gen-\\neration” and “Augmentation”, respectively. Section III focuses\\non optimization methods in retrieval,including indexing, query\\nand embedding optimization. Section IV concentrates on post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. Section VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. OVERVIEW OF RAG\\nA typical application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely discussed news. Given ChatGPT’s reliance on pre-\\ntraining data, it initially lacks the capacity to provide up-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from external\\ndatabases. In this case, it gathers relevant news articles related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the\\n3\\nFig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\\nencoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the final answer.\\nwidespread adoption of ChatGPT. The Naive RAG follows\\na traditional process that includes indexing, retrieval, and\\ngeneration, which is also characterized as a “Retrieve-Read”\\nframework [7].\\nIndexing starts with the cleaning and extraction of raw data\\nin diverse formats like PDF, HTML, Word, and Markdown,\\nwhich is then converted into a uniform plain text format. To\\naccommodate the context limitations of language models, text\\nis segmented into smaller, digestible chunks. Chunks are then\\nencoded into vector representations using an embedding model\\nand stored in vector database. This step is crucial for enabling\\nefficient similarity searches in the subsequent retrieval phase.\\nRetrieval. Upon receipt of a user query, the RAG system\\nemploys the same encoding model utilized during the indexing\\nphase to transform the query into a vector representation.\\nIt then computes the similarity scores between the query\\nvector and the vector of chunks within the indexed corpus.\\nThe system prioritizes and retrieves the top K chunks that\\ndemonstrate the greatest similarity to the query. These chunks\\nare subsequently used as the expanded context in prompt.\\nGeneration. The posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s\\napproach to answering may vary depending on task-specific\\ncriteria, allowing it to either draw upon its inherent parametric\\nknowledge or restrict its responses to the information con-\\ntained within the provided documents. In cases of ongoing\\ndialogues, any existing conversational history can be integrated\\ninto the prompt, enabling the model to engage in multi-turn\\ndialogue interactions effectively.\\nHowever, Naive RAG encounters notable drawbacks:\\nRetrieval Challenges. The retrieval phase often struggles\\nwith precision and recall, leading to the selection of misaligned\\nor irrelevant chunks, and the missing of crucial information.\\nGeneration Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can\\nalso suffer from irrelevance, toxicity, or bias in the outputs,\\ndetracting from the quality and reliability of the responses.\\nAugmentation Hurdles. Integrating retrieved information\\nwith the different task can be challenging, sometimes resulting\\nin disjointed or incoherent outputs. The process may also\\nencounter redundancy when similar information is retrieved\\nfrom multiple sources, leading to repetitive responses. Deter-\\nmining the significance and relevance of various passages and\\nensuring stylistic and tonal consistency add further complexity.\\nFacing complex issues, a single retrieval based on the original\\nquery may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that\\nsimply echo retrieved content without adding insightful or\\nsynthesized information.\\nB. Advanced RAG\\nAdvanced RAG introduces specific improvements to over-\\ncome the limitations of Naive RAG. Focusing on enhancing re-\\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\\ngies. To tackle the indexing issues, Advanced RAG refines\\nits indexing techniques through the use of a sliding window\\napproach, fine-grained segmentation, and the incorporation of\\nmetadata. Additionally, it incorporates several optimization\\nmethods to streamline the retrieval process [8].\\n4\\nFig. 3.\\nComparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\\ngeneration; it includes methods such as iterative and adaptive retrieval.\\nPre-retrieval process. In this stage, the primary focus is\\non optimizing the indexing structure and the original query.\\nThe goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,\\nalignment optimization, and mixed retrieval. While the goal\\nof query optimization is to make the user’s original question\\nclearer and more suitable for the retrieval task. Common\\nmethods include query rewriting query transformation, query\\nexpansion and other techniques [7], [9]–[11].\\nPost-Retrieval Process. Once relevant context is retrieved,\\nit’s crucial to integrate it effectively with the query. The main\\nmethods in post-retrieval process include rerank chunks and\\ncontext compressing. Re-ranking the retrieved information to\\nrelocate the most relevant content to the edges of the prompt is\\na key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre-ranking to uncover both explicit and transformative knowl-\\nedge [16]. The Memory module leverages the LLM’s memory\\nto guide retrieval, creating an unbounded memory pool that\\n5\\naligns the text more closely with data distribution through iter-\\native self-enhancement [17], [18]. Routing in the RAG system\\nnavigates through diverse data sources, selecting the optimal\\npathway for a query, whether it involves summarization,\\nspecific database searches, or merging different information\\nstreams [19]. The Predict module aims to reduce redundancy\\nand noise by generating context directly through the LLM,\\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\\nmodule tailors RAG to various downstream tasks, automating\\nprompt retrieval for zero-shot inputs and creating task-specific\\nretrievers through few-shot query generation [20], [21] .This\\ncomprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handle knowledge-intensive tasks.\\nHybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings\\n(HyDE) [11] seeks to improve retrieval relevance by focusing\\non embedding similarities between generated answers and real\\ndocuments.\\nAdjustments in module arrangement and interaction, such\\nas the Demonstrate-Search-Predict (DSP) [23] framework\\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\\nRETGEN [14], showcase the dynamic use of module out-\\nputs to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].\\nD. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes with higher latency and ethical considerations regarding\\ndata retrieval. On the other hand, FT is more static, requiring\\nretraining for updates but enabling deep customization of the\\nmodel’s behavior and style. It demands significant compu-\\ntational resources for dataset preparation and training, and\\nwhile it can reduce hallucinations, it may face challenges with\\nunfamiliar data.\\nIn multiple evaluations of their performance on various\\nknowledge-intensive tasks across different topics, [28] re-\\nvealed that while unsupervised fine-tuning shows some im-\\nprovement, RAG consistently outperforms it, for both exist-\\ning knowledge encountered during training and entirely new\\nknowledge. Additionally, it was found that LLMs struggle\\nto learn new factual information through unsupervised fine-\\ntuning. The choice between RAG and FT depends on the\\nspecific needs for data dynamics, customization, and com-\\nputational capabilities in the application context. RAG and\\nFT are not mutually exclusive and can complement each\\nother, enhancing a model’s capabilities at different levels.\\nIn some instances, their combined use may lead to optimal\\nperformance. The optimization process involving RAG and FT\\nmay require multiple iterations to achieve satisfactory results.\\nIII. RETRIEVAL\\nIn the context of RAG, it is crucial to efficiently retrieve\\nrelevant documents from the data source. There are several\\nkey issues involved, such as the retrieval source, retrieval\\ngranularity, pre-processing of the retrieval, and selection of\\nthe corresponding embedding model.\\nA. Retrieval Source\\nRAG relies on external knowledge to enhance LLMs, while\\nthe type of retrieval source and the granularity of retrieval\\nunits both affect the final generation results.\\n1) Data Structure: Initially, text is s the mainstream source\\nof retrieval. Subsequently, the retrieval source expanded to in-\\nclude semi-structured data (PDF) and structured data (Knowl-\\nedge Graph, KG) for enhancement. In addition to retrieving\\nfrom original external sources, there is also a growing trend in\\nrecent researches towards utilizing content generated by LLMs\\nthemselves for retrieval and enhancement purposes.\\n6\\nTABLE I\\nSUMMARY OF RAG METHODS\\nMethod\\nRetrieval Source\\nRetrieval\\nData Type\\nRetrieval\\nGranularity\\nAugmentation\\nStage\\nRetrieval\\nprocess\\nCoG [29]\\nWikipedia\\nText\\nPhrase\\nPre-training\\nIterative\\nDenseX [30]\\nFactoidWiki\\nText\\nProposition\\nInference\\nOnce\\nEAR [31]\\nDataset-base\\nText\\nSentence\\nTuning\\nOnce\\nUPRISE [20]\\nDataset-base\\nText\\nSentence\\nTuning\\nOnce\\nRAST [32]\\nDataset-base\\nText\\nSentence\\nTuning\\nOnce\\nSelf-Mem [17]\\nDataset-base\\nText\\nSentence\\nTuning\\nIterative\\nFLARE [24]\\nSearch Engine,Wikipedia\\nText\\nSentence\\nTuning\\nAdaptive\\nPGRA [33]\\nWikipedia\\nText\\nSentence\\nInference\\nOnce\\nFILCO [34]\\nWikipedia\\nText\\nSentence\\nInference\\nOnce\\nRADA [35]\\nDataset-base\\nText\\nSentence\\nInference\\nOnce\\nFilter-rerank [36]\\nSynthesized dataset\\nText\\nSentence\\nInference\\nOnce\\nR-GQA [37]\\nDataset-base\\nText\\nSentence Pair\\nTuning\\nOnce\\nLLM-R [38]\\nDataset-base\\nText\\nSentence Pair\\nInference\\nIterative\\nTIGER [39]\\nDataset-base\\nText\\nItem-base\\nPre-training\\nOnce\\nLM-Indexer [40]\\nDataset-base\\nText\\nItem-base\\nTuning\\nOnce\\nBEQUE [9]\\nDataset-base\\nText\\nItem-base\\nTuning\\nOnce\\nCT-RAG [41]\\nSynthesized dataset\\nText\\nItem-base\\nTuning\\nOnce\\nAtlas [42]\\nWikipedia, Common Crawl\\nText\\nChunk\\nPre-training\\nIterative\\nRAVEN [43]\\nWikipedia\\nText\\nChunk\\nPre-training\\nOnce\\nRETRO++ [44]\\nPre-training Corpus\\nText\\nChunk\\nPre-training\\nIterative\\nINSTRUCTRETRO [45]\\nPre-training corpus\\nText\\nChunk\\nPre-training\\nIterative\\nRRR [7]\\nSearch Engine\\nText\\nChunk\\nTuning\\nOnce\\nRA-e2e [46]\\nDataset-base\\nText\\nChunk\\nTuning\\nOnce\\nPROMPTAGATOR [21]\\nBEIR\\nText\\nChunk\\nTuning\\nOnce\\nAAR [47]\\nMSMARCO,Wikipedia\\nText\\nChunk\\nTuning\\nOnce\\nRA-DIT [27]\\nCommon Crawl,Wikipedia\\nText\\nChunk\\nTuning\\nOnce\\nRAG-Robust [48]\\nWikipedia\\nText\\nChunk\\nTuning\\nOnce\\nRA-Long-Form [49]\\nDataset-base\\nText\\nChunk\\nTuning\\nOnce\\nCoN [50]\\nWikipedia\\nText\\nChunk\\nTuning\\nOnce\\nSelf-RAG [25]\\nWikipedia\\nText\\nChunk\\nTuning\\nAdaptive\\nBGM [26]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nCoQ [51]\\nWikipedia\\nText\\nChunk\\nInference\\nIterative\\nToken-Elimination [52]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nPaperQA [53]\\nArxiv,Online Database,PubMed\\nText\\nChunk\\nInference\\nIterative\\nNoiseRAG [54]\\nFactoidWiki\\nText\\nChunk\\nInference\\nOnce\\nIAG [55]\\nSearch Engine,Wikipedia\\nText\\nChunk\\nInference\\nOnce\\nNoMIRACL [56]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nToC [57]\\nSearch Engine,Wikipedia\\nText\\nChunk\\nInference\\nRecursive\\nSKR [58]\\nDataset-base,Wikipedia\\nText\\nChunk\\nInference\\nAdaptive\\nITRG [59]\\nWikipedia\\nText\\nChunk\\nInference\\nIterative\\nRAG-LongContext [60]\\nDataset-base\\nText\\nChunk\\nInference\\nOnce\\nITER-RETGEN [14]\\nWikipedia\\nText\\nChunk\\nInference\\nIterative\\nIRCoT [61]\\nWikipedia\\nText\\nChunk\\nInference\\nRecursive\\nLLM-Knowledge-Boundary [62]\\nWikipedia\\nText\\nChunk\\nInference\\nOnce\\nRAPTOR [63]\\nDataset-base\\nText\\nChunk\\nInference\\nRecursive\\nRECITE [22]\\nLLMs\\nText\\nChunk\\nInference\\nOnce\\nICRALM [64]\\nPile,Wikipedia\\nText\\nChunk\\nInference\\nIterative\\nRetrieve-and-Sample [65]\\nDataset-base\\nText\\nDoc\\nTuning\\nOnce\\nZemi [66]\\nC4\\nText\\nDoc\\nTuning\\nOnce\\nCRAG [67]\\nArxiv\\nText\\nDoc\\nInference\\nOnce\\n1-PAGER [68]\\nWikipedia\\nText\\nDoc\\nInference\\nIterative\\nPRCA [69]\\nDataset-base\\nText\\nDoc\\nInference\\nOnce\\nQLM-Doc-ranking [70]\\nDataset-base\\nText\\nDoc\\nInference\\nOnce\\nRecomp [71]\\nWikipedia\\nText\\nDoc\\nInference\\nOnce\\nDSP [23]\\nWikipedia\\nText\\nDoc\\nInference\\nIterative\\nRePLUG [72]\\nPile\\nText\\nDoc\\nInference\\nOnce\\nARM-RAG [73]\\nDataset-base\\nText\\nDoc\\nInference\\nIterative\\nGenRead [13]\\nLLMs\\nText\\nDoc\\nInference\\nIterative\\nUniMS-RAG [74]\\nDataset-base\\nText\\nMulti\\nTuning\\nOnce\\nCREA-ICL [19]\\nDataset-base\\nCrosslingual,Text\\nSentence\\nInference\\nOnce\\nPKG [75]\\nLLM\\nTabular,Text\\nChunk\\nInference\\nOnce\\nSANTA [76]\\nDataset-base\\nCode,Text\\nItem\\nPre-training\\nOnce\\nSURGE [77]\\nFreebase\\nKG\\nSub-Graph\\nTuning\\nOnce\\nMK-ToD [78]\\nDataset-base\\nKG\\nEntity\\nTuning\\nOnce\\nDual-Feedback-ToD [79]\\nDataset-base\\nKG\\nEntity Sequence\\nTuning\\nOnce\\nKnowledGPT [15]\\nDataset-base\\nKG\\nTriplet\\nInference\\nMuti-time\\nFABULA [80]\\nDataset-base,Graph\\nKG\\nEntity\\nInference\\nOnce\\nHyKGE [81]\\nCMeKG\\nKG\\nEntity\\nInference\\nOnce\\nKALMV [82]\\nWikipedia\\nKG\\nTriplet\\nInference\\nIterative\\nRoG [83]\\nFreebase\\nKG\\nTriplet\\nInference\\nIterative\\nG-Retriever [84]\\nDataset-base\\nTextGraph\\nSub-Graph\\nInference\\nOnce\\n7\\nFig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt\\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\\nUnstructured Data, such as text, is the most widely used\\nretrieval source, which are mainly gathered from corpus. For\\nopen-domain question-answering (ODQA) tasks, the primary\\nretrieval sources are Wikipedia Dump with the current major\\nversions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-\\nspecific data (such as medical [67]and legal domains [29]).\\nSemi-structured data. typically refers to data that contains a\\ncombination of text and table information, such as PDF. Han-\\ndling semi-structured data poses challenges for conventional\\nRAG systems due to two main reasons. Firstly, text splitting\\nprocesses may inadvertently separate tables, leading to data\\ncorruption during retrieval. Secondly, incorporating tables into\\nthe data can complicate semantic similarity searches. When\\ndealing with semi-structured data, one approach involves lever-\\naging the code capabilities of LLMs to execute Text-2-SQL\\nqueries on tables within databases, such as TableGPT [85].\\nAlternatively, tables can be transformed into text format for\\nfurther analysis using text-based methods [75]. However, both\\nof these methods are not optimal solutions, indicating substan-\\ntial research opportunities in this area.\\nStructured data, such as knowledge graphs (KGs) [86] ,\\nwhich are typically verified and can provide more precise in-\\nformation. KnowledGPT [15] generates KB search queries and\\nstores knowledge in a personalized base, enhancing the RAG\\nmodel’s knowledge richness. In response to the limitations of\\nLLMs in understanding and answering questions about textual\\ngraphs, G-Retriever [84] integrates Graph Neural Networks\\n4https://hotpotqa.github.io/wiki-readme.html\\n5https://github.com/facebookresearch/DPR\\n(GNNs), LLMs and RAG, enhancing graph comprehension\\nand question-answering capabilities through soft prompting\\nof the LLM, and employs the Prize-Collecting Steiner Tree\\n(PCST) optimization problem for targeted graph retrieval. On\\nthe contrary, it requires additional effort to build, validate,\\nand maintain structured databases. On the contrary, it requires\\nadditional effort to build, validate, and maintain structured\\ndatabases.\\nLLMs-Generated Content. Addressing the limitations of\\nexternal auxiliary information in RAG, some research has\\nfocused on exploiting LLMs’ internal knowledge. SKR [58]\\nclassifies questions as known or unknown, applying retrieval\\nenhancement selectively. GenRead [13] replaces the retriever\\nwith an LLM generator, finding that LLM-generated contexts\\noften contain more accurate answers due to better alignment\\nwith the pre-training objectives of causal language modeling.\\nSelfmem [17] iteratively creates an unbounded memory pool\\nwith a retrieval-enhanced generator, using a memory selec-\\ntor to choose outputs that serve as dual problems to the\\noriginal question, thus self-enhancing the generative model.\\nThese methodologies underscore the breadth of innovative\\ndata source utilization in RAG, striving to improve model\\nperformance and task effectiveness.\\n2) Retrieval Granularity: Another important factor besides\\nthe data format of the retrieval source is the granularity of\\nthe retrieved data. Coarse-grained retrieval units theoretically\\ncan provide more relevant information for the problem, but\\nthey may also contain redundant content, which could distract\\nthe retriever and language models in downstream tasks [50],\\n[87]. On the other hand, fine-grained retrieval unit granularity\\nincreases the burden of retrieval and does not guarantee seman-\\ntic integrity and meeting the required knowledge. Choosing\\n8\\nthe appropriate retrieval granularity during inference can be\\na simple and effective strategy to improve the retrieval and\\ndownstream task performance of dense retrievers.\\nIn text, retrieval granularity ranges from fine to coarse,\\nincluding Token, Phrase, Sentence, Proposition, Chunks, Doc-\\nument. Among them, DenseX [30]proposed the concept of\\nusing propositions as retrieval units. Propositions are defined\\nas atomic expressions in the text, each encapsulating a unique\\nfactual segment and presented in a concise, self-contained nat-\\nural language format. This approach aims to enhance retrieval\\nprecision and relevance. On the Knowledge Graph (KG),\\nretrieval granularity includes Entity, Triplet, and sub-Graph.\\nThe granularity of retrieval can also be adapted to downstream\\ntasks, such as retrieving Item IDs [40]in recommendation tasks\\nand Sentence pairs [38]. Detailed information is illustrated in\\nTable I.\\nB. Indexing Optimization\\nIn the Indexing phase, documents will be processed, seg-\\nmented, and transformed into Embeddings to be stored in a\\nvector database. The quality of index construction determines\\nwhether the correct context can be obtained in the retrieval\\nphase.\\n1) Chunking Strategy: The most common method is to split\\nthe document into chunks on a fixed number of tokens (e.g.,\\n100, 256, 512) [88]. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise. How-\\never, chunks leads to truncation within sentences, prompting\\nthe optimization of a recursive splits and sliding window meth-\\nods, enabling layered retrieval by merging globally related\\ninformation across multiple retrieval processes [89]. Never-\\ntheless, these approaches still cannot strike a balance between\\nsemantic completeness and context length. Therefore, methods\\nlike Small2Big have been proposed, where sentences (small)\\nare used as the retrieval unit, and the preceding and following\\nsentences are provided as (big) context to LLMs [90].\\n2) Metadata Attachments: Chunks can be enriched with\\nmetadata information such as page number, file name, au-\\nthor,category timestamp. Subsequently, retrieval can be filtered\\nbased on this metadata, limiting the scope of the retrieval.\\nAssigning different weights to document timestamps during\\nretrieval can achieve time-aware RAG, ensuring the freshness\\nof knowledge and avoiding outdated information.\\nIn addition to extracting metadata from the original doc-\\numents, metadata can also be artificially constructed. For\\nexample, adding summaries of paragraph, as well as intro-\\nducing hypothetical questions. This method is also known as\\nReverse HyDE. Specifically, using LLM to generate questions\\nthat can be answered by the document, then calculating the\\nsimilarity between the original question and the hypothetical\\nquestion during retrieval to reduce the semantic gap between\\nthe question and the answer.\\n3) Structural Index: One effective method for enhancing\\ninformation retrieval is to establish a hierarchical structure for\\nthe documents. By constructing In structure, RAG system can\\nexpedite the retrieval and processing of pertinent data.\\nHierarchical index structure. File are arranged in parent-\\nchild relationships, with chunks linked to them. Data sum-\\nmaries are stored at each node, aiding in the swift traversal\\nof data and assisting the RAG system in determining which\\nchunks to extract. This approach can also mitigate the illusion\\ncaused by block extraction issues.\\nKnowledge Graph index. Utilize KG in constructing the\\nhierarchical structure of documents contributes to maintaining\\nconsistency. It delineates the connections between different\\nconcepts and entities, markedly reducing the potential for\\nillusions. Another advantage is the transformation of the\\ninformation retrieval process into instructions that LLM can\\ncomprehend, thereby enhancing the accuracy of knowledge\\nretrieval and enabling LLM to generate contextually coherent\\nresponses, thus improving the overall efficiency of the RAG\\nsystem. To capture the logical relationship between document\\ncontent and structure, KGP [91] proposed a method of building\\nan index between multiple documents using KG. This KG\\nconsists of nodes (representing paragraphs or structures in the\\ndocuments, such as pages and tables) and edges (indicating\\nsemantic/lexical similarity between paragraphs or relationships\\nwithin the document structure), effectively addressing knowl-\\nedge retrieval and reasoning problems in a multi-document\\nenvironment.\\nC. Query Optimization\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nSometimes, the question itself is complex, and the language\\nis not well-organized. Another difficulty lies in language\\ncomplexity ambiguity. Language models often struggle when\\ndealing with specialized vocabulary or ambiguous abbrevi-\\nations with multiple meanings. For instance, they may not\\ndiscern whether “LLM” refers to large language model or a\\nMaster of Laws in a legal context.\\n1) Query Expansion: Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing\\nfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\nMulti-Query. By employing prompt engineering to expand\\nqueries via LLMs, these queries can then be executed in\\nparallel. The expansion of queries is not random, but rather\\nmeticulously designed.\\nSub-Query. The process of sub-question planning represents\\nthe generation of the necessary sub-questions to contextualize\\nand fully answer the original question when combined. This\\nprocess of adding relevant context is, in principle, similar\\nto query expansion. Specifically, a complex question can be\\ndecomposed into a series of simpler sub-questions using the\\nleast-to-most prompting method [92].\\nChain-of-Verification(CoVe). The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing halluci-\\nnations. Validated expanded queries typically exhibit higher\\nreliability [93].\\n9\\n2) Query Transformation: The core concept is to retrieve\\nchunks based on a transformed query instead of the user’s\\noriginal query.\\nQuery Rewrite.The original queries are not always optimal\\nfor LLM retrieval, especially in real-world scenarios. There-\\nfore, we can prompt LLM to rewrite the queries. In addition to\\nusing LLM for query rewriting, specialized smaller language\\nmodels, such as RRR (Rewrite-retrieve-read) [7]. The imple-\\nmentation of the query rewrite method in the Taobao, known\\nas BEQUE [9] has notably enhanced recall effectiveness for\\nlong-tail queries, resulting in a rise in GMV.\\nAnother query transformation method is to use prompt\\nengineering to let LLM generate a query based on the original\\nquery for subsequent retrieval. HyDE [11] construct hypothet-\\nical documents (assumed answers to the original query). It\\nfocuses on embedding similarity from answer to answer rather\\nthan seeking embedding similarity for the problem or query.\\nUsing the Step-back Prompting method [10], the original\\nquery is abstracted to generate a high-level concept question\\n(step-back question). In the RAG system, both the step-back\\nquestion and the original query are used for retrieval, and both\\nthe results are utilized as the basis for language model answer\\ngeneration.\\n3) Query Routing: Based on varying queries, routing to\\ndistinct RAG pipeline,which is suitable for a versatile RAG\\nsystem designed to accommodate diverse scenarios.\\nMetadata Router/ Filter. The first step involves extracting\\nkeywords (entity) from the query, followed by filtering based\\non the keywords and metadata within the chunks to narrow\\ndown the search scope.\\nSemantic Router is another method of routing involves\\nleveraging the semantic information of the query. Specific\\napprach see Semantic Router 6. Certainly, a hybrid routing\\napproach can also be employed, combining both semantic and\\nmetadata-based methods for enhanced query routing.\\nD. Embedding\\nIn RAG, retrieval is achieved by calculating the similarity\\n(e.g. cosine similarity) between the embeddings of the ques-\\ntion and document chunks, where the semantic representation\\ncapability of embedding models plays a key role. This mainly\\nincludes a sparse encoder (BM25) and a dense retriever (BERT\\narchitecture Pre-training language models). Recent research\\nhas introduced prominent embedding models such as AngIE,\\nVoyage, BGE,etc [94]–[96], which are benefit from multi-task\\ninstruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates\\nembedding models across 8 tasks, covering 58 datasests. Ad-\\nditionally, C-MTEB focuses on Chinese capability, covering\\n6 tasks and 35 datasets. There is no one-size-fits-all answer\\nto “which embedding model to use.” However, some specific\\nmodels are better suited for particular use cases.\\n1) Mix/hybrid Retrieval : Sparse and dense embedding\\napproaches capture different relevance features and can ben-\\nefit from each other by leveraging complementary relevance\\ninformation. For instance, sparse retrieval models can be used\\n6https://github.com/aurelio-labs/semantic-router\\n7https://huggingface.co/spaces/mteb/leaderboard\\nto provide initial search results for training dense retrieval\\nmodels. Additionally, pre-training language models (PLMs)\\ncan be utilized to learn term weights to enhance sparse\\nretrieval. Specifically, it also demonstrates that sparse retrieval\\nmodels can enhance the zero-shot retrieval capability of dense\\nretrieval models and assist dense retrievers in handling queries\\ncontaining rare entities, thereby improving robustness.\\n2) Fine-tuning Embedding Model: In instances where the\\ncontext significantly deviates from pre-training corpus, partic-\\nularly within highly specialized disciplines such as healthcare,\\nlegal practice, and other sectors replete with proprietary jargon,\\nfine-tuning the embedding model on your own domain dataset\\nbecomes essential to mitigate such discrepancies.\\nIn addition to supplementing domain knowledge, another\\npurpose of fine-tuning is to align the retriever and generator,\\nfor example, using the results of LLM as the supervision signal\\nfor fine-tuning, known as LSR (LM-supervised Retriever).\\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\\ngenerator to create task-specific retrievers, addressing chal-\\nlenges in supervised fine-tuning, particularly in data-scarce\\ndomains. Another approach, LLM-Embedder [97], exploits\\nLLMs to generate reward signals across multiple downstream\\ntasks. The retriever is fine-tuned with two types of supervised\\nsignals: hard labels for the dataset and soft rewards from\\nthe LLMs. This dual-signal approach fosters a more effective\\nfine-tuning process, tailoring the embedding model to diverse\\ndownstream applications. REPLUG [72] utilizes a retriever\\nand an LLM to calculate the probability distributions of the\\nretrieved documents and then performs supervised training\\nby computing the KL divergence. This straightforward and\\neffective training method enhances the performance of the\\nretrieval model by using an LM as the supervisory signal,\\neliminating the need for specific cross-attention mechanisms.\\nMoreover, inspired by RLHF (Reinforcement Learning from\\nHuman Feedback), utilizing LM-based feedback to reinforce\\nthe retriever through reinforcement learning.\\nE. Adapter\\nFine-tuning models may present challenges, such as in-\\ntegrating functionality through an API or addressing con-\\nstraints arising from limited local computational resources.\\nConsequently, some approaches opt to incorporate an external\\nadapter to aid in alignment.\\nTo optimize the multi-task capabilities of LLM, UP-\\nRISE [20] trained a lightweight prompt retriever that can\\nautomatically retrieve prompts from a pre-built prompt pool\\nthat are suitable for a given zero-shot task input. AAR\\n(Augmentation-Adapted Retriver) [47] introduces a universal\\nadapter designed to accommodate multiple downstream tasks.\\nWhile PRCA [69] add a pluggable reward-driven contextual\\nadapter to enhance performance on specific tasks. BGM [26]\\nkeeps the retriever and LLM fixed,and trains a bridge Seq2Seq\\nmodel in between. The bridge model aims to transform the\\nretrieved information into a format that LLMs can work with\\neffectively, allowing it to not only rerank but also dynami-\\ncally select passages for each query, and potentially employ\\nmore advanced strategies like repetition. Furthermore, PKG\\n10\\nintroduces an innovative method for integrating knowledge\\ninto white-box models via directive fine-tuning [75]. In this\\napproach, the retriever module is directly substituted to gen-\\nerate relevant documents according to a query. This method\\nassists in addressing the difficulties encountered during the\\nfine-tuning process and enhances model performance.\\nIV. GENERATION\\nAfter retrieval, it is not a good practice to directly input all\\nthe retrieved information to the LLM for answering questions.\\nFollowing will introduce adjustments from two perspectives:\\nadjusting the retrieved content and adjusting the LLM.\\nA. Context Curation\\nRedundant information can interfere with the final gener-\\nation of LLM, and overly long contexts can also lead LLM\\nto the “Lost in the middle” problem [98]. Like humans, LLM\\ntends to only focus on the beginning and end of long texts,\\nwhile forgetting the middle portion. Therefore, in the RAG\\nsystem, we typically need to further process the retrieved\\ncontent.\\n1) Reranking: Reranking fundamentally reorders document\\nchunks to highlight the most pertinent results first, effectively\\nreducing the overall document pool, severing a dual purpose\\nin information retrieval, acting as both an enhancer and a\\nfilter, delivering refined inputs for more precise language\\nmodel processing [70]. Reranking can be performed using\\nrule-based methods that depend on predefined metrics like\\nDiversity, Relevance, and MRR, or model-based approaches\\nlike Encoder-Decoder models from the BERT series (e.g.,\\nSpanBERT), specialized reranking models such as Cohere\\nrerank or bge-raranker-large, and general large language mod-\\nels like GPT [12], [99].\\n2) Context Selection/Compression: A common misconcep-\\ntion in the RAG process is the belief that retrieving as many\\nrelevant documents as possible and concatenating them to form\\na lengthy retrieval prompt is beneficial. However, excessive\\ncontext can introduce more noise, diminishing the LLM’s\\nperception of key information .\\n(Long) LLMLingua [100], [101] utilize small language\\nmodels (SLMs) such as GPT-2 Small or LLaMA-7B, to\\ndetect and remove unimportant tokens, transforming it into\\na form that is challenging for humans to comprehend but\\nwell understood by LLMs. This approach presents a direct\\nand practical method for prompt compression, eliminating the\\nneed for additional training of LLMs while balancing language\\nintegrity and compression ratio. PRCA tackled this issue by\\ntraining an information extractor [69]. Similarly, RECOMP\\nadopts a comparable approach by training an information\\ncondenser using contrastive learning [71]. Each training data\\npoint consists of one positive sample and five negative sam-\\nples, and the encoder undergoes training using contrastive loss\\nthroughout this process [102] .\\nIn addition to compressing the context, reducing the num-\\nber of documents aslo helps improve the accuracy of the\\nmodel’s answers. Ma et al. [103] propose the “Filter-Reranker”\\nparadigm, which combines the strengths of LLMs and SLMs.\\nIn this paradigm, SLMs serve as filters, while LLMs function\\nas reordering agents. The research shows that instructing\\nLLMs to rearrange challenging samples identified by SLMs\\nleads to significant improvements in various Information\\nExtraction (IE) tasks. Another straightforward and effective\\napproach involves having the LLM evaluate the retrieved\\ncontent before generating the final answer. This allows the\\nLLM to filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [104], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nB. LLM Fine-tuning\\nTargeted fine-tuning based on the scenario and data char-\\nacteristics on LLMs can yield better results. This is also one\\nof the greatest advantages of using on-premise LLMs. When\\nLLMs lack data in a specific domain, additional knowledge can\\nbe provided to the LLM through fine-tuning. Huggingface’s\\nfine-tuning data can also be used as an initial step.\\nAnother benefit of fine-tuning is the ability to adjust the\\nmodel’s input and output. For example, it can enable LLM to\\nadapt to specific data formats and generate responses in a par-\\nticular style as instructed [37]. For retrieval tasks that engage\\nwith structured data, the SANTA framework [76] implements\\na tripartite training regimen to effectively encapsulate both\\nstructural and semantic nuances. The initial phase focuses on\\nthe retriever, where contrastive learning is harnessed to refine\\nthe query and document embeddings.\\nAligning LLM outputs with human or retriever preferences\\nthrough reinforcement learning is a potential approach. For\\ninstance, manually annotating the final generated answers\\nand then providing feedback through reinforcement learning.\\nIn addition to aligning with human preferences, it is also\\npossible to align with the preferences of fine-tuned models\\nand retrievers [79]. When circumstances prevent access to\\npowerful proprietary models or larger parameter open-source\\nmodels, a simple and effective method is to distill the more\\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\\nbe coordinated with fine-tuning of the retriever to align pref-\\nerences. A typical approach, such as RA-DIT [27], aligns the\\nscoring functions between Retriever and Generator using KL\\ndivergence.\\nV. AUGMENTATION PROCESS IN RAG\\nIn the domain of RAG, the standard practice often involves\\na singular (once) retrieval step followed by generation, which\\ncan lead to inefficiencies and sometimes is typically insuffi-\\ncient for complex problems demanding multi-step reasoning,\\nas it provides a limited scope of information [105]. Many\\nstudies have optimized the retrieval process in response to this\\nissue, and we have summarised them in Figure 5.\\nA. Iterative Retrieval\\nIterative retrieval is a process where the knowledge base\\nis repeatedly searched based on the initial query and the text\\ngenerated so far, providing a more comprehensive knowledge\\n11\\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\\nbase for LLMs. This approach has been shown to enhance\\nthe robustness of subsequent answer generation by offering\\nadditional contextual references through multiple retrieval\\niterations. However, it may be affected by semantic discon-\\ntinuity and the accumulation of irrelevant information. ITER-\\nRETGEN [14] employs a synergistic approach that lever-\\nages “retrieval-enhanced generation” alongside “generation-\\nenhanced retrieval” for tasks that necessitate the reproduction\\nof specific information. The model harnesses the content\\nrequired to address the input task as a contextual basis for\\nretrieving pertinent knowledge, which in turn facilitates the\\ngeneration of improved responses in subsequent iterations.\\nB. Recursive Retrieval\\nRecursive retrieval is often used in information retrieval and\\nNLP to improve the depth and relevance of search results.\\nThe process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-\\nally converging on the most pertinent information through a\\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\\nthe retrieval process and refines the CoT with the obtained\\nretrieval results. ToC [57] creates a clarification tree that\\nsystematically optimizes the ambiguous parts in the Query. It\\ncan be particularly useful in complex search scenarios where\\nthe user’s needs are not entirely clear from the outset or where\\nthe information sought is highly specialized or nuanced. The\\nrecursive nature of the process allows for continuous learning\\nand adaptation to the user’s requirements, often resulting in\\nimproved satisfaction with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive\\nretrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summarizing\\nsections of a document or lengthy PDF before performing a\\nretrieval based on this summary. Subsequently, a secondary\\nretrieval within the document refines the search, embodying\\nthe recursive nature of the process. In contrast, multi-hop\\nretrieval is designed to delve deeper into graph-structured data\\nsources, extracting interconnected information [106].\\nC. Adaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare [24] and\\nSelf-RAG [25], refine the RAG framework by enabling LLMs\\nto actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operations, as seen\\nin model agents like AutoGPT, Toolformer, and Graph-\\nToolformer [107]–[109]. Graph-Toolformer, for instance, di-\\nvides its retrieval process into distinct steps where LLMs\\nproactively use retrievers, apply Self-Ask techniques, and em-\\nploy few-shot prompts to initiate search queries. This proactive\\nstance allows LLMs to decide when to search for necessary\\ninformation, akin to how an agent utilizes tools.\\nWebGPT [110] integrates a reinforcement learning frame-\\nwork to train the GPT-3 model in autonomously using a\\nsearch engine during text generation. It navigates this process\\nusing special tokens that facilitate actions such as search\\nengine queries, browsing results, and citing references, thereby\\nexpanding GPT-3’s capabilities through the use of external\\nsearch engines. Flare automates timing retrieval by monitoring\\nthe confidence of the generation process, as indicated by the\\n12\\nprobability of generated terms [24]. When the probability falls\\nbelow a certain threshold would activates the retrieval system\\nto collect relevant information, thus optimizing the retrieval\\ncycle. Self-RAG [25] introduces “reflection tokens” that allow\\nthe model to introspect its outputs. These tokens come in\\ntwo varieties: “retrieve” and “critic”. The model autonomously\\ndecides when to activate retrieval, or alternatively, a predefined\\nthreshold may trigger the process. During retrieval, the gen-\\nerator conducts a fragment-level beam search across multiple\\nparagraphs to derive the most coherent sequence. Critic scores\\nare used to update the subdivision scores, with the flexibility\\nto adjust these weights during inference, tailoring the model’s\\nbehavior. Self-RAG’s design obviates the need for additional\\nclassifiers or reliance on Natural Language Inference (NLI)\\nmodels, thus streamlining the decision-making process for\\nwhen to engage retrieval mechanisms and improving the\\nmodel’s autonomous judgment capabilities in generating ac-\\ncurate responses.\\nVI. TASK AND EVALUATION\\nThe rapid advancement and growing adoption of RAG\\nin the field of NLP have propelled the evaluation of RAG\\nmodels to the forefront of research in the LLMs community.\\nThe primary objective of this evaluation is to comprehend\\nand optimize the performance of RAG models across diverse\\napplication scenarios.This chapter will mainly introduce the\\nmain downstream tasks of RAG, datasets, and how to evaluate\\nRAG systems.\\nA. Downstream Task\\nThe core task of RAG remains Question Answering (QA),\\nincluding\\ntraditional\\nsingle-hop/multi-hop\\nQA,\\nmultiple-\\nchoice, domain-specific QA as well as long-form scenarios\\nsuitable for RAG. In addition to QA, RAG is continuously\\nbeing expanded into multiple downstream tasks, such as Infor-\\nmation Extraction (IE), dialogue generation, code search, etc.\\nThe main downstream tasks of RAG and their corresponding\\ndatasets are summarized in Table II.\\nB. Evaluation Target\\nHistorically, RAG models assessments have centered on\\ntheir execution in specific downstream tasks. These evaluations\\nemploy established metrics suitable to the tasks at hand. For\\ninstance, question answering evaluations might rely on EM\\nand F1 scores [7], [45], [59], [72], whereas fact-checking\\ntasks often hinge on Accuracy as the primary metric [4],\\n[14], [42]. BLEU and ROUGE metrics are also commonly\\nused to evaluate answer quality [26], [32], [52], [78]. Tools\\nlike RALLE, designed for the automatic evaluation of RAG\\napplications, similarly base their assessments on these task-\\nspecific metrics [160]. Despite this, there is a notable paucity\\nof research dedicated to evaluating the distinct characteristics\\nof RAG models.The main evaluation objectives include:\\nRetrieval Quality. Evaluating the retrieval quality is crucial\\nfor determining the effectiveness of the context sourced by\\nthe retriever component. Standard metrics from the domains\\nof search engines, recommendation systems, and information\\nretrieval systems are employed to measure the performance of\\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\\nNDCG are commonly utilized for this purpose [161], [162].\\nGeneration Quality. The assessment of generation quality\\ncenters on the generator’s capacity to synthesize coherent and\\nrelevant answers from the retrieved context. This evaluation\\ncan be categorized based on the content’s objectives: unlabeled\\nand labeled content. For unlabeled content, the evaluation\\nencompasses the faithfulness, relevance, and non-harmfulness\\nof the generated answers. In contrast, for labeled content,\\nthe focus is on the accuracy of the information produced by\\nthe model [161]. Additionally, both retrieval and generation\\nquality assessments can be conducted through manual or\\nautomatic evaluation methods [29], [161], [163].\\nC. Evaluation Aspects\\nContemporary evaluation practices of RAG models empha-\\nsize three primary quality scores and four essential abilities,\\nwhich collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.\\n1) Quality Scores: Quality scores include context rele-\\nvance, answer faithfulness, and answer relevance. These qual-\\nity scores evaluate the efficiency of the RAG model from\\ndifferent perspectives in the process of information retrieval\\nand generation [164]–[166].\\nContext Relevance evaluates the precision and specificity\\nof the retrieved context, ensuring relevance and minimizing\\nprocessing costs associated with extraneous content.\\nAnswer Faithfulness ensures that the generated answers\\nremain true to the retrieved context, maintaining consistency\\nand avoiding contradictions.\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively addressing\\nthe core inquiry.\\n2) Required Abilities: RAG evaluation also encompasses\\nfour abilities indicative of its adaptability and efficiency:\\nnoise robustness, negative rejection, information integration,\\nand counterfactual robustness [167], [168]. These abilities are\\ncritical for the model’s performance under various challenges\\nand complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in\\nrefraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\nInformation Integration evaluates the model’s proficiency in\\nsynthesizing information from multiple documents to address\\ncomplex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.\\nContext relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating the\\nquality of generation.\\n13\\nTABLE II\\nDOWNSTREAM TASKS AND DATASETS OF RAG\\nTask\\nSub Task\\nDataset\\nMethod\\nQA\\nSingle-hop\\nNatural Qustion(NQ) [111]\\n[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]\\n[3], [4], [22], [27], [40], [43], [54], [62], [71], [112]\\n[20], [44], [72]\\nTriviaQA(TQA) [113]\\n[13], [30], [34], [45], [50], [64]\\n[4], [27], [59], [62], [112]\\n[22], [25], [43], [44], [71], [72]\\nSQuAD [114]\\n[20], [23], [30], [32], [45], [69], [112]\\nWeb Questions(WebQ) [115]\\n[3], [4], [13], [30], [50], [68]\\nPopQA [116]\\n[7], [25], [67]\\nMS MARCO [117]\\n[4], [40], [52]\\nMulti-hop\\nHotpotQA [118]\\n[23], [26], [31], [34], [47], [51], [61], [82]\\n[7], [14], [22], [27], [59], [62], [69], [71], [91]\\n2WikiMultiHopQA [119]\\n[14], [24], [48], [59], [61], [91]\\nMuSiQue [120]\\n[14], [51], [61], [91]\\nLong-form QA\\nELI5 [121]\\n[27], [34], [43], [49], [51]\\nNarrativeQA(NQA) [122]\\n[45], [60], [63], [123]\\nASQA [124]\\n[24], [57]\\nQMSum(QM) [125]\\n[60], [123]\\nDomain QA\\nQasper [126]\\n[60], [63]\\nCOVID-QA [127]\\n[35], [46]\\nCMB [128],MMCU Medical [129]\\n[81]\\nMulti-Choice QA\\nQuALITY [130]\\n[60], [63]\\nARC [131]\\n[25], [67]\\nCommonsenseQA [132]\\n[58], [66]\\nGraph QA\\nGraphQA [84]\\n[84]\\nDialog\\nDialog Generation\\nWizard of Wikipedia (WoW) [133]\\n[13], [27], [34], [42]\\nPersonal Dialog\\nKBP [134]\\n[74], [135]\\nDuleMon [136]\\n[74]\\nTask-oriented Dialog\\nCamRest [137]\\n[78], [79]\\nRecommendation\\nAmazon(Toys,Sport,Beauty) [138]\\n[39], [40]\\nIE\\nEvent Argument Extraction\\nWikiEvent [139]\\n[13], [27], [37], [42]\\nRAMS [140]\\n[36], [37]\\nRelation Extraction\\nT-REx [141],ZsRE [142]\\n[27], [51]\\nReasoning\\nCommonsense Reasoning\\nHellaSwag [143]\\n[20], [66]\\nCoT Reasoning\\nCoT Reasoning [144]\\n[27]\\nComplex Reasoning\\nCSQA [145]\\n[55]\\nOthers\\nLanguage Understanding\\nMMLU [146]\\n[7], [27], [28], [42], [43], [47], [72]\\nLanguage Modeling\\nWikiText-103 [147]\\n[5], [29], [64], [71]\\nStrategyQA [148]\\n[14], [24], [48], [51], [55], [58]\\nFact Checking/Verification\\nFEVER [149]\\n[4], [13], [27], [34], [42], [50]\\nPubHealth [150]\\n[25], [67]\\nText Generation\\nBiography [151]\\n[67]\\nText Summarization\\nWikiASP [152]\\n[24]\\nXSum [153]\\n[17]\\nText Classification\\nVioLens [154]\\n[19]\\nTREC [155]\\n[33]\\nSentiment\\nSST-2 [156]\\n[20], [33], [38]\\nCode Search\\nCodeSearchNet [157]\\n[76]\\nRobustness Evaluation\\nNoMIRACL [56]\\n[56]\\nMath\\nGSM8K [158]\\n[73]\\nMachine Translation\\nJRC-Acquis [159]\\n[17]\\n14\\nTABLE III\\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\\nContext\\nRelevance\\nFaithfulness\\nAnswer\\nRelevance\\nNoise\\nRobustness\\nNegative\\nRejection\\nInformation\\nIntegration\\nCounterfactual\\nRobustness\\nAccuracy\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nEM\\n✓\\nRecall\\n✓\\nPrecision\\n✓\\n✓\\nR-Rate\\n✓\\nCosine Similarity\\n✓\\nHit Rate\\n✓\\nMRR\\n✓\\nNDCG\\n✓\\nBLEU\\n✓\\n✓\\n✓\\nROUGE/ROUGE-L\\n✓\\n✓\\n✓\\nThe specific metrics for each evaluation aspect are sum-\\nmarized in Table III. It is essential to recognize that these\\nmetrics, derived from related work, are traditional measures\\nand do not yet represent a mature or standardized approach for\\nquantifying RAG evaluation aspects. Custom metrics tailored\\nto the nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\nD. Evaluation Benchmarks and Tools\\nA series of benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-\\nmance but also enhance comprehension of the model’s capabil-\\nities across various evaluation aspects. Prominent benchmarks\\nsuch as RGB, RECALL and CRUD\\n[167]–[169] focus on\\nappraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. DISCUSSION AND FUTURE PROSPECTS\\nDespite the considerable progress in RAG technology, sev-\\neral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\nis continuously expanding [170]–[172]. Presently, LLMs can\\neffortlessly manage contexts exceeding 200,000 tokens 9. This\\ncapability signifies that long-document question answering,\\npreviously reliant on RAG, can now incorporate the entire\\ndocument directly into the prompt. This has also sparked\\ndiscussions on whether RAG is still necessary when LLMs\\n8https://www.trulens.org/trulens eval/core concepts rag triad/\\n9https://kimi.moonshot.cn\\nare not constrained by context. In fact, RAG still plays an\\nirreplaceable role. On one hand, providing LLMs with a\\nlarge amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other\\nhand, RAG-based generation can quickly locate the original\\nreferences for LLMs to help users verify the generated an-\\nswers. The entire retrieval and reasoning process is observable,\\nwhile generation solely relying on long context remains a\\nblack box. Conversely, the expansion of context provides new\\nopportunities for the development of RAG, enabling it to\\naddress more complex problems and integrative or summary\\nquestions that require reading a large amount of material to\\nanswer [49]. Developing new RAG methods in the context of\\nsuper-long contexts is one of the future research trends.\\nB. RAG Robustness\\nThe presence of noise or contradictory information during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal\\nthat including irrelevant documents can unexpectedly increase\\naccuracy by over 30%, contradicting the initial assumption\\nof reduced quality. These results underscore the importance\\nof developing specialized strategies to integrate retrieval with\\nlanguage generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches\\nCombining RAG with fine-tuning is emerging as a leading\\nstrategy. Determining the optimal integration of RAG and\\nfine-tuning whether sequential, alternating, or through end-to-\\nend joint training—and how to harness both parameterized\\n15\\nTABLE IV\\nSUMMARY OF EVALUATION FRAMEWORKS\\nEvaluation Framework\\nEvaluation Targets\\nEvaluation Aspects\\nQuantitative Metrics\\nRGB†\\nRetrieval Quality\\nGeneration Quality\\nNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual Robustness\\nAccuracy\\nEM\\nAccuracy\\nAccuracy\\nRECALL†\\nGeneration Quality\\nCounterfactual Robustness\\nR-Rate (Reappearance Rate)\\nRAGAS‡\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\nCosine Similarity\\nARES‡\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\nAccuracy\\nAccuracy\\nAccuracy\\nTruLens‡\\nRetrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\n*\\nCRUD†\\nRetrieval Quality\\nGeneration Quality\\nCreative Generation\\nKnowledge-intensive QA\\nError Correction\\nSummarization\\nBLEU\\nROUGE-L\\nBertScore\\nRAGQuestEval\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\nand non-parameterized advantages are areas ripe for explo-\\nration [27]. Another trend is to introduce SLMs with specific\\nfunctionalities into RAG and fine-tuned by the results of RAG\\nsystem. For example, CRAG [67] trains a lightweight retrieval\\nevaluator to assess the overall quality of the retrieved docu-\\nments for a query and triggers different knowledge retrieval\\nactions based on confidence levels.\\nD. Scaling laws of RAG\\nEnd-to-end RAG models and pre-trained models based\\non\\nRAG\\nare\\nstill\\none\\nof\\nthe\\nfocuses\\nof\\ncurrent\\nre-\\nsearchers [173].The parameters of these models are one of\\nthe key factors.While scaling laws [174] are established for\\nLLMs, their applicability to RAG remains uncertain. Initial\\nstudies like RETRO++ [44] have begun to address this, yet the\\nparameter count in RAG models still lags behind that of LLMs.\\nThe possibility of an Inverse Scaling Law 10, where smaller\\nmodels outperform larger ones, is particularly intriguing and\\nmerits further investigation.\\nE. Production-Ready RAG\\nRAG’s practicality and alignment with engineering require-\\nments have facilitated its adoption. However, enhancing re-\\ntrieval efficiency, improving document recall in large knowl-\\nedge bases, and ensuring data security—such as preventing\\n10https://github.com/inverse-scaling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.The\\nemerging technology stack, while not as rich in features as\\nLangChain and LLamaIndex, stands out through its specialized\\nproducts. For example, Flowise AI prioritizes a low-code\\napproach, allowing users to deploy AI applications, including\\nRAG, through a user-friendly drag-and-drop interface. Other\\ntechnologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric services. Weaviate’s Verba 11 is designed for\\npersonal assistant applications, while Amazon’s Kendra\\n12\\noffers intelligent enterprise search services, enabling users to\\nbrowse various content repositories using built-in connectors.\\nIn the development of RAG technology, there is a clear\\ntrend towards different specialization directions, such as: 1)\\nCustomization - tailoring RAG to meet specific requirements.\\n2) Simplification - making RAG easier to use to reduce the\\n11https://github.com/weaviate/Verba\\n12https://aws.amazon.com/cn/kendra/\\n16\\nFig. 6. Summary of RAG ecosystem\\ninitial learning curve. 3) Specialization - optimizing RAG to\\nbetter serve production environments.\\nThe mutual growth of RAG models and their technology\\nstacks is evident; technological advancements continuously\\nestablish new standards for existing infrastructure. In turn,\\nenhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF. Multi-modal RAG\\nRAG\\nhas\\ntranscended\\nits\\ninitial\\ntext-based\\nquestion-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBLIP-2 [177] leverages frozen image encoders alongside\\nLLMs for efficient visual language pre-training, enabling zero-\\nshot image-to-text conversions. The “Visualize Before You\\nWrite” method [178] employs image generation to steer the\\nLM’s text generation, showing promise in open-ended text\\ngeneration tasks.\\nAudio and Video. The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [179]. UEOP marks a significant ad-\\nvancement in end-to-end automatic speech recognition by\\nincorporating external, offline strategies for voice-to-text con-\\nversion [180]. Additionally, KNN-based attention fusion lever-\\nages audio embeddings and semantically related text embed-\\ndings to refine ASR, thereby accelerating domain adaptation.\\nVid2Seq augments language models with specialized temporal\\nmarkers, facilitating the prediction of event boundaries and\\ntextual descriptions within a unified output sequence [181].\\nCode. RBPS [182] excels in small-scale learning tasks by\\nretrieving code examples that align with developers’ objectives\\nthrough encoding and frequency analysis. This approach has\\ndemonstrated efficacy in tasks such as test assertion genera-\\ntion and program repair. For structured knowledge, the CoK\\nmethod [106] first extracts facts pertinent to the input query\\nfrom a knowledge graph, then integrates these facts as hints\\nwithin the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. CONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.\\n17\\nThe growing ecosystem of RAG is evidenced by the rise in\\nRAG-centric AI applications and the continuous development\\nof supportive tools. As RAG’s application landscape broadens,\\nthere is a need to refine evaluation methodologies to keep\\npace with its evolution. Ensuring accurate and representative\\nperformance assessments is crucial for fully capturing RAG’s\\ncontributions to the AI research and development community.\\nREFERENCES\\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large\\nlanguage models struggle to learn long-tail knowledge,” in Interna-\\ntional Conference on Machine Learning.\\nPMLR, 2023, pp. 15 696–\\n15 707.\\n[2] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY. Zhang, Y. Chen et al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158, 2023.\\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\\nH. K¨\\nuttler, M. Lewis, W.-t. Yih, T. Rockt¨\\naschel et al., “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\\n[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark et al.,\\n“Improving language models by retrieving from trillions of tokens,”\\nin International conference on machine learning.\\nPMLR, 2022, pp.\\n2206–2240.\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nneural information processing systems, vol. 35, pp. 27 730–27 744,\\n2022.\\n[7] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[8] I.\\nILIN,\\n“Advanced\\nrag\\ntechniques:\\nan\\nil-\\nlustrated\\noverview,”\\nhttps://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\\n2023.\\n[9] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al.,\\n“Large language model based long-tail query rewriting in taobao\\nsearch,” arXiv preprint arXiv:2311.03758, 2023.\\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117, 2023.\\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496, 2022.\\n[12] V. Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-\\nsityranker and lostinthemiddleranker,” https://towardsdatascience.com/\\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.\\n[13] W. Yu, D. Iter, S. Wang, Y. Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,\\nand M. Jiang, “Generate rather than retrieve: Large language models\\nare strong context generators,” arXiv preprint arXiv:2209.10063, 2022.\\n[14] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294, 2023.\\n[15] X. Wang, Q. Yang, Y. Qiu, J. Liang, Q. He, Z. Gu, Y. Xiao,\\nand W. Wang, “Knowledgpt: Enhancing large language models with\\nretrieval and storage access on knowledge bases,” arXiv preprint\\narXiv:2308.11761, 2023.\\n[16] A.\\nH.\\nRaudaschl,\\n“Forget\\nrag,\\nthe\\nfuture\\nis\\nrag-fusion,”\\nhttps://towardsdatascience.com/\\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\\n[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift\\nyourself up: Retrieval-augmented text generation with self memory,”\\narXiv preprint arXiv:2305.02437, 2023.\\n[18] S. Wang, Y. Xu, Y. Fang, Y. Liu, S. Sun, R. Xu, C. Zhu, and\\nM. Zeng, “Training data is more valuable than you think: A simple\\nand effective method by retrieving from training data,” arXiv preprint\\narXiv:2203.08773, 2022.\\n[19] X. Li, E. Nie, and S. Liang, “From classification to generation:\\nInsights into crosslingual retrieval augmented icl,” arXiv preprint\\narXiv:2311.06595, 2023.\\n[20] D. Cheng, S. Huang, J. Bi, Y. Zhan, J. Liu, Y. Wang, H. Sun,\\nF. Wei, D. Deng, and Q. Zhang, “Uprise: Universal prompt retrieval\\nfor improving zero-shot evaluation,” arXiv preprint arXiv:2303.08518,\\n2023.\\n[21] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\\nK. B. Hall, and M.-W. Chang, “Promptagator: Few-shot dense retrieval\\nfrom 8 examples,” arXiv preprint arXiv:2209.11755, 2022.\\n[22] Z. Sun, X. Wang, Y. Tay, Y. Yang, and D. Zhou, “Recitation-augmented\\nlanguage models,” arXiv preprint arXiv:2210.01296, 2022.\\n[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,\\nand M. Zaharia, “Demonstrate-search-predict: Composing retrieval\\nand language models for knowledge-intensive nlp,” arXiv preprint\\narXiv:2212.14024, 2022.\\n[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\\narXiv preprint arXiv:2305.06983, 2023.\\n[25] A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag:\\nLearning to retrieve, generate, and critique through self-reflection,”\\narXiv preprint arXiv:2310.11511, 2023.\\n[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,\\n“Bridging the preference gap between retrievers and llms,” arXiv\\npreprint arXiv:2401.06954, 2024.\\n[27] X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-\\ndriguez, J. Kahn, G. Szilvasy, M. Lewis et al., “Ra-dit: Retrieval-\\naugmented dual instruction tuning,” arXiv preprint arXiv:2310.01352,\\n2023.\\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or\\nretrieval? comparing knowledge injection in llms,” arXiv preprint\\narXiv:2312.05934, 2023.\\n[29] T. Lan, D. Cai, Y. Wang, H. Huang, and X.-L. Mao, “Copy is all\\nyou need,” in The Eleventh International Conference on Learning\\nRepresentations, 2022.\\n[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and\\nH. Zhang, “Dense x retrieval: What retrieval granularity should we\\nuse?” arXiv preprint arXiv:2312.06648, 2023.\\n[31] F. Luo and M. Surdeanu, “Divide & conquer for entailment-aware\\nmulti-hop evidence retrieval,” arXiv preprint arXiv:2311.02616, 2023.\\n[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y. Li, and N. Cam-Tu,\\n“Diversify question generation with retrieval-augmented style transfer,”\\narXiv preprint arXiv:2310.14503, 2023.\\n[33] Z. Guo, S. Cheng, Y. Wang, P. Li, and Y. Liu, “Prompt-guided re-\\ntrieval augmentation for non-knowledge-intensive tasks,” arXiv preprint\\narXiv:2305.17653, 2023.\\n[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning\\nto filter context for retrieval-augmented generation,” arXiv preprint\\narXiv:2311.08377, 2023.\\n[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented\\ndata augmentation for low-resource domain tasks,” arXiv preprint\\narXiv:2402.13482, 2024.\\n[36] Y. Ma, Y. Cao, Y. Hong, and A. Sun, “Large language model is not\\na good few-shot information extractor, but a good reranker for hard\\nsamples!” arXiv preprint arXiv:2303.08559, 2023.\\n[37] X. Du and H. Ji, “Retrieval-augmented generative question answering\\nfor event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.\\n[38] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context\\nexamples for large language models,” arXiv preprint arXiv:2307.07164,\\n2023.\\n[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,\\nL. Hong, Y. Tay, V. Q. Tran, J. Samost et al., “Recommender systems\\nwith generative retrieval,” arXiv preprint arXiv:2305.05065, 2023.\\n[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,\\nY. Li, H. Lu et al., “Language models as semantic indexers,” arXiv\\npreprint arXiv:2310.07815, 2023.\\n[41] R. Anantha, T. Bethi, D. Vodianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708,\\n2023.\\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-\\nzaro, “Raven: In-context learning with retrieval augmented encoder-\\ndecoder language models,” arXiv preprint arXiv:2308.07922, 2023.\\n18\\n[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y. Dong,\\nO. Kuchaiev, B. Li, C. Xiao et al., “Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study,” arXiv preprint\\narXiv:2304.06762, 2023.\\n[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-\\nzaro, “Instructretro: Instruction tuning post retrieval-augmented pre-\\ntraining,” arXiv preprint arXiv:2310.07713, 2023.\\n[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\\nand S. Nanayakkara, “Improving the domain adaptation of retrieval\\naugmented generation (rag) models for open domain question answer-\\ning,” Transactions of the Association for Computational Linguistics,\\nvol. 11, pp. 1–17, 2023.\\n[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, “Understanding re-\\ntrieval augmentation for long-form question answering,” arXiv preprint\\narXiv:2310.12150, 2023.\\n[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, “Chain-of-note:\\nEnhancing robustness in retrieval-augmented language models,” arXiv\\npreprint arXiv:2311.09210, 2023.\\n[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-the-\\nchain: Towards accurate, credible and traceable large language models\\nfor knowledgeintensive tasks,” CoRR, vol. abs/2304.14732, 2023.\\n[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\\n“Optimizing retrieval-augmented reader models via token elimination,”\\narXiv preprint arXiv:2310.13682, 2023.\\n[53] J. L´\\nala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,\\nand A. D. White, “Paperqa: Retrieval-augmented generative agent for\\nscientific research,” arXiv preprint arXiv:2312.07559, 2023.\\n[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY. Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n[55] Z. Zhang, X. Zhang, Y. Ren, S. Shi, M. Han, Y. Wu, R. Lai, and\\nZ. Cao, “Iag: Induction-augmented generation framework for answer-\\ning reasoning questions,” in Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing, 2023, pp. 1–14.\\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,\\nD. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,\\n“Nomiracl: Knowing when you don’t know for robust multilingual\\nretrieval-augmented generation,” arXiv preprint arXiv:2312.11361,\\n2023.\\n[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696, 2023.\\n[58] Y. Wang, P. Li, M. Sun, and Y. Liu, “Self-knowledge guided\\nretrieval augmentation for large language models,” arXiv preprint\\narXiv:2310.05002, 2023.\\n[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025,\\n2023.\\n[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509, 2022.\\n[62] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-\\nR. Wen, and H. Wang, “Investigating the factual knowledge boundary\\nof large language models with retrieval augmentation,” arXiv preprint\\narXiv:2307.11019, 2023.\\n[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\\nManning, “Raptor: Recursive abstractive processing for tree-organized\\nretrieval,” arXiv preprint arXiv:2401.18059, 2024.\\n[64] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, and Y. Shoham, “In-context retrieval-augmented language\\nmodels,” arXiv preprint arXiv:2302.00083, 2023.\\n[65] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-and-\\nsample: Document-level event argument extraction via hybrid retrieval\\naugmentation,” in Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers),\\n2023, pp. 293–306.\\n[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, “Zemi: Learning\\nzero-shot semi-parametric language models from multiple tasks,” arXiv\\npreprint arXiv:2210.00185, 2022.\\n[67] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, “Corrective retrieval\\naugmented generation,” arXiv preprint arXiv:2401.15884, 2024.\\n[68] P. Jain, L. B. Soares, and T. Kwiatkowski, “1-pager: One pass answer\\ngeneration and evidence retrieval,” arXiv preprint arXiv:2310.16568,\\n2023.\\n[69] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, “Prca:\\nFitting black-box large language models for retrieval question answer-\\ning via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, “Open-source large\\nlanguage models are strong zero-shot query likelihood models for\\ndocument ranking,” arXiv preprint arXiv:2310.13243, 2023.\\n[71] F. Xu, W. Shi, and E. Choi, “Recomp: Improving retrieval-augmented\\nlms with compression and selective augmentation,” arXiv preprint\\narXiv:2310.04408, 2023.\\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box lan-\\nguage models,” arXiv preprint arXiv:2301.12652, 2023.\\n[73] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary ra-\\ntionale memory for retrieval augmented generation,” arXiv preprint\\narXiv:2311.04177, 2023.\\n[74] H. Wang, W. Huang, Y. Deng, R. Wang, Z. Wang, Y. Wang, F. Mi,\\nJ. Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source\\nretrieval-augmented generation for personalized dialogue systems,”\\narXiv preprint arXiv:2401.13256, 2024.\\n[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,\\n“Augmented large language models with parametric knowledge guid-\\ning,” arXiv preprint arXiv:2305.04757, 2023.\\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y. Gu, Z. Liu, and G. Yu, “Structure-\\naware language model pretraining improves dense retrieval on struc-\\ntured data,” arXiv preprint arXiv:2305.19912, 2023.\\n[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge\\ngraph-augmented language models for knowledge-grounded dialogue\\ngeneration,” arXiv preprint arXiv:2305.18846, 2023.\\n[78] W. Shen, Y. Gao, C. Huang, F. Wan, X. Quan, and W. Bi, “Retrieval-\\ngeneration alignment for end-to-end task-oriented dialogue system,”\\narXiv preprint arXiv:2310.08877, 2023.\\n[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, “Dual-feedback\\nknowledge retrieval for task-oriented dialogue systems,” arXiv preprint\\narXiv:2310.14528, 2023.\\n[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation\\nusing retrieval-augmented narrative construction,” arXiv preprint\\narXiv:2310.13848, 2023.\\n[81] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang,\\nH. Ding, X. Chu, J. Zhao et al., “Think and retrieval: A hypothesis\\nknowledge graph enhanced medical large language models,” arXiv\\npreprint arXiv:2312.15883, 2023.\\n[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n[83] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful\\nand interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n[84] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun,\\nX. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation\\nfor textual graph understanding and question answering,” arXiv preprint\\narXiv:2402.07630, 2024.\\n[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,\\nX. Li, A. Su et al., “Tablegpt: Towards unifying tables, nature language\\nand commands into one gpt,” arXiv preprint arXiv:2307.08674, 2023.\\n[86] M. Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, “Iseeq: Information\\nseeking question generation using dynamic meta-information retrieval\\nand knowledge graphs,” in Proceedings of the AAAI Conference on\\nArtificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672–10 680.\\n[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch¨\\narli,\\nand D. Zhou, “Large language models can be easily distracted by\\nirrelevant context,” in International Conference on Machine Learning.\\nPMLR, 2023, pp. 31 210–31 227.\\n[88] R.\\nTeja,\\n“Evaluating\\nthe\\nideal\\nchunk\\nsize\\nfor\\na\\nrag\\nsystem\\nusing\\nllamaindex,”\\nhttps://www.llamaindex.ai/blog/\\nevaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\\n2023.\\n19\\n[89] Langchain, “Recursively split by character,” https://python.langchain.\\ncom/docs/modules/data connection/document transformers/recursive\\ntext splitter, 2023.\\n[90] S.\\nYang,\\n“Advanced\\nrag\\n01:\\nSmall-to-\\nbig\\nretrieval,”\\nhttps://towardsdatascience.com/\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\\n[91] Y. Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730, 2023.\\n[92] D. Zhou, N. Sch¨\\narli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n[93] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495, 2023.\\n[94] X. Li and J. Li, “Angle-optimized text embeddings,” arXiv preprint\\narXiv:2309.12871, 2023.\\n[95] VoyageAI, “Voyage’s embedding models,” https://docs.voyageai.com/\\nembeddings/, 2023.\\n[96] BAAI,\\n“Flagembedding,”\\nhttps://github.com/FlagOpen/\\nFlagEmbedding, 2023.\\n[97] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y. Nie, “Retrieve anything\\nto augment large language models,” arXiv preprint arXiv:2310.07554,\\n2023.\\n[98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni,\\nand P. Liang, “Lost in the middle: How language models use long\\ncontexts,” arXiv preprint arXiv:2307.03172, 2023.\\n[99] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524, 2023.\\n[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” in Proceedings\\nof the 15th Biennial Conference of the Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track\\nand Government Track), J. Campbell, S. Larocca, J. Marciano,\\nK. Savenkov, and A. Yanishevsky, Eds.\\nOrlando, USA: Association\\nfor Machine Translation in the Americas, Sep. 2022, pp. 202–209.\\n[Online]. Available: https://aclanthology.org/2022.amta-upg.14\\n[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y. Lin, Y. Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context\\nscenarios via prompt compression,” arXiv preprint arXiv:2310.06839,\\n2023.\\n[102] V. Karpukhin, B. O˘\\nguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\\nand W.-t. Yih, “Dense passage retrieval for open-domain question\\nanswering,” arXiv preprint arXiv:2004.04906, 2020.\\n[103] Y. Ma, Y. Cao, Y. Hong, and A. Sun, “Large language model is\\nnot a good few-shot information extractor, but a good reranker for\\nhard samples!” ArXiv, vol. abs/2303.08559, 2023. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:257532405\\n[104] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092, 2023.\\n[105] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[106] X. Li, R. Zhao, Y. K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\\n“Chain of knowledge: A framework for grounding large language mod-\\nels with structured knowledge bases,” arXiv preprint arXiv:2305.13269,\\n2023.\\n[107] H.\\nYang,\\nS.\\nYue,\\nand\\nY.\\nHe,\\n“Auto-gpt\\nfor\\nonline\\ndecision\\nmaking:\\nBenchmarks\\nand\\nadditional\\nopinions,”\\narXiv\\npreprint\\narXiv:2306.02224, 2023.\\n[108] T. Schick, J. Dwivedi-Yu, R. Dess`\\nı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761,\\n2023.\\n[109] J. Zhang, “Graph-toolformer: To empower llms with graph rea-\\nsoning ability via prompt augmented by chatgpt,” arXiv preprint\\narXiv:2304.11116, 2023.\\n[110] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332, 2021.\\n[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural\\nquestions: a benchmark for question answering research,” Transactions\\nof the Association for Computational Linguistics, vol. 7, pp. 453–466,\\n2019.\\n[112] Y. Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y. Zhou,\\n“Exploring the integration strategies of retriever and large language\\nmodels,” arXiv preprint arXiv:2308.12574, 2023.\\n[113] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehen-\\nsion,” arXiv preprint arXiv:1705.03551, 2017.\\n[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\\nquestions\\nfor\\nmachine\\ncomprehension\\nof\\ntext,”\\narXiv\\npreprint\\narXiv:1606.05250, 2016.\\n[115] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on\\nfreebase from question-answer pairs,” in Proceedings of the 2013\\nconference on empirical methods in natural language processing, 2013,\\npp. 1533–1544.\\n[116] A. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\\n“When not to trust language models: Investigating effectiveness and\\nlimitations of parametric and non-parametric memories,” arXiv preprint\\narXiv:2212.10511, 2022.\\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\\nand L. Deng, “Ms marco: A human-generated machine reading com-\\nprehension dataset,” 2016.\\n[118] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdi-\\nnov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explain-\\nable multi-hop question answering,” arXiv preprint arXiv:1809.09600,\\n2018.\\n[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, “Constructing a\\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps,”\\narXiv preprint arXiv:2011.01060, 2020.\\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique:\\nMultihop questions via single-hop question composition,” Transactions\\nof the Association for Computational Linguistics, vol. 10, pp. 539–554,\\n2022.\\n[121] A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:\\nLong form question answering,” arXiv preprint arXiv:1907.09190,\\n2019.\\n[122] T. Koˇ\\ncisk`\\ny, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\\nand E. Grefenstette, “The narrativeqa reading comprehension chal-\\nlenge,” Transactions of the Association for Computational Linguistics,\\nvol. 6, pp. 317–328, 2018.\\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, “A human-\\ninspired reading agent with gist memory of very long contexts,” arXiv\\npreprint arXiv:2402.09727, 2024.\\n[124] I. Stelmakh, Y. Luan, B. Dhingra, and M.-W. Chang, “Asqa: Factoid\\nquestions meet long-form answers,” arXiv preprint arXiv:2204.06092,\\n2022.\\n[125] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H.\\nAwadallah, A. Celikyilmaz, Y. Liu, X. Qiu et al., “Qmsum: A new\\nbenchmark for query-based multi-domain meeting summarization,”\\narXiv preprint arXiv:2104.05938, 2021.\\n[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,\\n“A dataset of information-seeking questions and answers anchored in\\nresearch papers,” arXiv preprint arXiv:2105.03011, 2021.\\n[127] T. M¨\\noller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A\\nquestion answering dataset for covid-19,” in ACL 2020 Workshop on\\nNatural Language Processing for COVID-19 (NLP-COVID), 2020.\\n[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,\\nJ. Li, X. Wan, B. Wang et al., “Cmb: A comprehensive medical\\nbenchmark in chinese,” arXiv preprint arXiv:2308.08833, 2023.\\n[129] H. Zeng, “Measuring massive multitask chinese understanding,” arXiv\\npreprint arXiv:2304.12986, 2023.\\n[130] R. Y. Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V. Pad-\\nmakumar, J. Ma, J. Thompson, H. He et al., “Quality: Question an-\\nswering with long input texts, yes!” arXiv preprint arXiv:2112.08608,\\n2021.\\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457, 2018.\\n[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\\nA question answering challenge targeting commonsense knowledge,”\\narXiv preprint arXiv:1811.00937, 2018.\\n[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\\n“Wizard of wikipedia: Knowledge-powered conversational agents,”\\narXiv preprint arXiv:1811.01241, 2018.\\n[134] H. Wang, M. Hu, Y. Deng, R. Wang, F. Mi, W. Wang, Y. Wang, W.-\\nC. Kwan, I. King, and K.-F. Wong, “Large language models as source\\n20\\nplanner for personalized knowledge-grounded dialogue,” arXiv preprint\\narXiv:2310.08840, 2023.\\n[135] ——, “Large language models as source planner for personal-\\nized knowledge-grounded dialogue,” arXiv preprint arXiv:2310.08840,\\n2023.\\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, and S. Wang,\\n“Long time no see! open-domain conversation with long-term persona\\nmemory,” arXiv preprint arXiv:2203.05797, 2022.\\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\\nSu, S. Ultes, D. Vandyke, and S. Young, “Conditional generation\\nand snapshot learning in neural dialogue systems,” arXiv preprint\\narXiv:1606.03352, 2016.\\n[138] R. He and J. McAuley, “Ups and downs: Modeling the visual evolution\\nof fashion trends with one-class collaborative filtering,” in proceedings\\nof the 25th international conference on world wide web, 2016, pp.\\n507–517.\\n[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction\\nby conditional generation,” arXiv preprint arXiv:2104.05919, 2021.\\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, “Multi-\\nsentence argument linking,” arXiv preprint arXiv:1911.03766, 2019.\\n[141] H. Elsahar, P. Vougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest,\\nand E. Simperl, “T-rex: A large scale alignment of natural language\\nwith knowledge base triples,” in Proceedings of the Eleventh Inter-\\nnational Conference on Language Resources and Evaluation (LREC\\n2018), 2018.\\n[142] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation ex-\\ntraction via reading comprehension,” arXiv preprint arXiv:1706.04115,\\n2017.\\n[143] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830, 2019.\\n[144] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,\\n“The cot collection: Improving zero-shot and few-shot learning of\\nlanguage models via chain-of-thought fine-tuning,” arXiv preprint\\narXiv:2305.14045, 2023.\\n[145] A. Saha, V. Pahuja, M. Khapra, K. Sankaranarayanan, and S. Chandar,\\n“Complex sequential question answering: Towards learning to converse\\nover linked question answer pairs with a knowledge graph,” in Proceed-\\nings of the AAAI conference on artificial intelligence, vol. 32, no. 1,\\n2018.\\n[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300, 2020.\\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\\nmixture models,” arXiv preprint arXiv:1609.07843, 2016.\\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\\n“Did aristotle use a laptop? a question answering benchmark with\\nimplicit reasoning strategies,” Transactions of the Association for\\nComputational Linguistics, vol. 9, pp. 346–361, 2021.\\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a\\nlarge-scale dataset for fact extraction and verification,” arXiv preprint\\narXiv:1803.05355, 2018.\\n[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for\\npublic health claims,” arXiv preprint arXiv:2010.09926, 2020.\\n[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from\\nstructured data with application to the biography domain,” arXiv\\npreprint arXiv:1603.07771, 2016.\\n[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,\\nand G. Neubig, “Wikiasp: A dataset for multi-domain aspect-based\\nsummarization,” Transactions of the Association for Computational\\nLinguistics, vol. 9, pp. 211–225, 2021.\\n[153] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details,\\njust the summary! topic-aware convolutional neural networks for ex-\\ntreme summarization,” arXiv preprint arXiv:1808.08745, 2018.\\n[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,\\nS. I. Ahmed, N. Mohammed, and M. R. Amin, “Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms\\nof communal violence and its evaluation,” in Proceedings of the First\\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72–\\n84.\\n[155] X. Li and D. Roth, “Learning question classifiers,” in COLING 2002:\\nThe 19th International Conference on Computational Linguistics, 2002.\\n[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng,\\nand C. Potts, “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proceedings of the 2013 conference on\\nempirical methods in natural language processing, 2013, pp. 1631–\\n1642.\\n[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\\n“Codesearchnet challenge: Evaluating the state of semantic code\\nsearch,” arXiv preprint arXiv:1909.09436, 2019.\\n[158] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.\\n[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,\\nand D. Varga, “The jrc-acquis: A multilingual aligned parallel corpus\\nwith 20+ languages,” arXiv preprint cs/0609058, 2006.\\n[160] Y. Hoshi, D. Miyashita, Y. Ng, K. Tatsuno, Y. Morioka, O. Torii,\\nand J. Deguchi, “Ralle: A framework for developing and eval-\\nuating retrieval-augmented large language models,” arXiv preprint\\narXiv:2308.10633, 2023.\\n[161] J. Liu, “Building production-ready rag applications,” https://www.ai.\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n[162] I. Nguyen, “Evaluating rag part i: How to evaluate document retrieval,”\\nhttps://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au-\\ntomated evaluation of retrieval augmented generation,” arXiv preprint\\narXiv:2309.15217, 2023.\\n[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “Ares: An\\nautomated evaluation framework for retrieval-augmented generation\\nsystems,” arXiv preprint arXiv:2311.09476, 2023.\\n[166] C.\\nJarvis\\nand\\nJ.\\nAllard,\\n“A\\nsurvey\\nof\\ntechniques\\nfor\\nmaximizing\\nllm\\nperformance,”\\nhttps://community.openai.\\ncom/t/openai-dev-day-2023-breakout-sessions/505213#\\na-survey-of-techniques-for-maximizing-llm-performance-2, 2023.\\n[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-\\nguage models in retrieval-augmented generation,” arXiv preprint\\narXiv:2309.01431, 2023.\\n[168] Y. Liu, L. Huang, S. Li, S. Chen, H. Zhou, F. Meng, J. Zhou, and\\nX. Sun, “Recall: A benchmark for llms robustness against external\\ncounterfactual knowledge,” arXiv preprint arXiv:2311.08147, 2023.\\n[169] Y. Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025,\\n2023.\\n[171] C. Packer, V. Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\\nzalez, “Memgpt: Towards llms as operating systems,” arXiv preprint\\narXiv:2310.08560, 2023.\\n[172] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, “Efficient\\nstreaming language models with attention sinks,” arXiv preprint\\narXiv:2309.17453, 2023.\\n[173] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.\\nGonzalez, “Raft: Adapting language model to domain specific rag,”\\narXiv preprint arXiv:2403.10131, 2024.\\n[174] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361, 2020.\\n[175] U. Alon, F. Xu, J. He, S. Sengupta, D. Roth, and G. Neubig, “Neuro-\\nsymbolic language modeling with automaton-augmented retrieval,” in\\nInternational Conference on Machine Learning.\\nPMLR, 2022, pp.\\n468–485.\\n[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,\\nM. Lewis, L. Zettlemoyer, and W.-t. Yih, “Retrieval-augmented multi-\\nmodal language modeling,” arXiv preprint arXiv:2211.12561, 2022.\\n[177] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-\\nimage pre-training with frozen image encoders and large language\\nmodels,” arXiv preprint arXiv:2301.12597, 2023.\\n[178] W. Zhu, A. Yan, Y. Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y.\\nWang, “Visualize before you write: Imagination-guided open-ended\\ntext generation,” arXiv preprint arXiv:2210.03765, 2022.\\n[179] J. Zhao, G. Haffar, and E. Shareghi, “Generating synthetic speech from\\nspokenvocab for speech translation,” arXiv preprint arXiv:2210.08174,\\n2022.\\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, “Using external\\noff-policy speech-to-text mappings in contextual end-to-end automated\\nspeech recognition,” arXiv preprint arXiv:2301.02736, 2023.\\n21\\n[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,\\nJ. Sivic, and C. Schmid, “Vid2seq: Large-scale pretraining of a visual\\nlanguage model for dense video captioning,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2023, pp. 10 714–10 726.\\n[182] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt\\nselection for code-related few-shot learning,” in 2023 IEEE/ACM 45th\\nInternational Conference on Software Engineering (ICSE), 2023, pp.\\n2450–2462.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of usage\n",
    "example_extracted_text = extract_text_from_pdf(\"../data/02_article/Retrieval-Augmented-Generation-for-LLM.pdf\")\n",
    "example_extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Intelligence sample\n",
    "### Reference\n",
    "- https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/unlocking-advanced-document-insights-with-azure-ai-document/ba-p/4109675\n",
    "- https://github.com/Azure-Samples/document-intelligence-code-samples/blob/main/Python(v4.0)/Retrieval_Augmented_Generation_(RAG)_samples/sample_figure_understanding.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (1.34.0)\n",
      "Requirement already satisfied: azure-ai-documentintelligence in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (1.0.0b3)\n",
      "Requirement already satisfied: azure-identity in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (1.16.1)\n",
      "Collecting pillow\n",
      "  Using cached pillow-10.3.0-cp312-cp312-win_amd64.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (1.24.5)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from openai) (2.7.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from azure-ai-documentintelligence) (0.6.1)\n",
      "Requirement already satisfied: azure-core>=1.30.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from azure-ai-documentintelligence) (1.30.2)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from azure-identity) (42.0.8)\n",
      "Requirement already satisfied: msal>=1.24.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from azure-identity) (1.28.1)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from azure-identity) (1.1.0)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.3 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from PyMuPDF) (1.24.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from azure-core>=1.30.0->azure-ai-documentintelligence) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from azure-core>=1.30.0->azure-ai-documentintelligence) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from cryptography>=2.5->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity) (2.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from msal-extensions>=0.3.0->azure-identity) (24.1)\n",
      "Requirement already satisfied: portalocker<3,>=1.6 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from msal-extensions>=0.3.0->azure-identity) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.22)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from portalocker<3,>=1.6->msal-extensions>=0.3.0->azure-identity) (306)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hishida\\repo\\rag-knowledge\\.venv\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (2.2.1)\n",
      "Using cached pillow-10.3.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "Installing collected packages: pillow\n",
      "Successfully installed pillow-10.3.0\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv openai azure-ai-documentintelligence azure-identity pillow PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "\n",
    "aoai_api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key= os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_deployment_name = 'gpt-4o' # your model deployment name for GPT-4V\n",
    "aoai_api_version = '2024-02-01' # this might change in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Unify the format of headings in markdown text\n",
    "def convert_markdown_headings(markdown_text):\n",
    "    # Convert \"===\" headers to \"#\"\n",
    "    markdown_text = re.sub(r'^(.*?)\\n={3,}$', r'# \\1', markdown_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Convert \"---\" headers to \"##\"\n",
    "    markdown_text = re.sub(r'^(.*?)\\n-{3,}$', r'## \\1', markdown_text, flags=re.MULTILINE)\n",
    "    \n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layout(input_file_path, output_folder):\n",
    "    \"\"\"\n",
    "    Analyzes the layout of a document and extracts figures along with their descriptions, then update the markdown output with the new description.\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): The path to the input document file.\n",
    "        output_folder (str): The path to the output folder where the cropped images will be saved.\n",
    "\n",
    "    Returns:\n",
    "        str: The updated Markdown content with figure descriptions.\n",
    "\n",
    "    \"\"\"\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=doc_intelligence_endpoint, \n",
    "        credential=AzureKeyCredential(doc_intelligence_key),\n",
    "        headers={\"x-ms-useragent\":\"sample-code-figure-understanding/1.0.0\"},\n",
    "    )\n",
    "\n",
    "    with open(input_file_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", output_content_format=ContentFormat.MARKDOWN \n",
    "        )\n",
    "\n",
    "    result = poller.result()\n",
    "    md_content = convert_markdown_headings(result.content)\n",
    "            \n",
    "    with open(f\"{output_folder}/{os.path.splitext(os.path.basename(input_file_path))[0]}.md\", 'w', encoding='utf-8') as f:\n",
    "        f.write(md_content)\n",
    "    \n",
    "    return md_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Incomplete download: ('Connection broken: IncompleteRead(21061632 bytes read, 1430334 more expected)', IncompleteRead(21061632 bytes read, 1430334 more expected))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Azure Al Search の検索インデックス\\n\\n[アーティクル]·2024/02/16\\n\\nAzure Al Search の “検索インデックス\"は検索可能なコンテンツであり、検索エンジン でインデックス作成、全文検索、ベクトル検索、ハイブリッド検索、フィルターされた クエリに使用できます。インデックスは、スキーマによって定義され、検索サービス に保存されます。2 番目のステップとしてデータのインポートが続きます。このコン テンツは検索サービス内に存在します。これは、最新の検索アプリケーションで想定さ れるミリ秒単位の応答時間に必要な、プライマリ データ ストアとは別のものです。イ ンデクサー主導のインデックス作成シナリオを除き、検索サービスがソース データに 接続したり、クエリを実行したりすることはありません。\\n\\n検索インデックスを作成して管理する場合、この記事は次の点を理解するのに役立ちま す。\\n\\n● コンテンツ(ドキュメントおよびスキーマ)\\n\\n● 物理データ構造\\n\\n● 基本操作\\n\\n今すぐに使いたいですか?代わりに、検索インデックスの作成に関する記事を参照して ください。\\n\\n\\n## 検索インデックスのスキーマ\\n\\nAzure Al Search のインデックスには 検索ドキュメントが格納されます。概念的に、ド キュメントはインデックス内で検索可能なデータの1 つの単位です。たとえば、小売 業者に製品ごとのドキュメントがあり、ニュース組織に記事ごとのドキュメントがある 場合、旅行サイトにはホテルと目的地ごとのドキュメントがある場合があります。こ れらの概念をなじみのあるデータベースの同等のものに対応させるなら、検索インデッ クスはテーブルと同じで、ドキュメントはテーブルにおける行とほぼ同じです。\\n\\n次の例に示すように、ドキュメントの構造は“インデックス スキーマ\"によって決まり ます。\"フィールド\"コレクションは通常、インデックスの最大の部分であり、各フィ ールドには、名前、データ型の割り当て、および使用方法を決定する許容される動作を 示す属性が設定されます。\\n\\nJSON\\n\\n{ \"name\": \"name\\\\_of\\\\_index, unique across the service\",\\n\\n\"fields\": [\\n\\n<!-- PageNumber=\"{\" -->\\n\\n\"name\": \"name\\\\_of\\\\_field\",\\n\\n\"type\": \"Edm. String | Collection(Edm. String) Collection(Edm. Single) | Edm. Int32 | Edm. Int64 |Edm. Double | Edm. Boolean | Edm. DateTimeOffset Edm. GeographyPoint\",\\n\\n\"searchable\": true (default where applicable) | false (only Edm.String and Collection (Edm. String) fields can be searchable),\\n\\n\"filterable\": true (default) | false,\\n\\n\"sortable\": true (default where applicable) | false (Collection(Edm.String) fields cannot be sortable), \"facetable\": true (default where applicable) | false (Edm. GeographyPoint fields cannot be facetable),\\n\\n\"key\": true | false (default, only Edm. String fields can be keys), \"retrievable\": true (default) | false,\\n\\n\"analyzer\": \"name\\\\_of\\\\_analyzer\\\\_for\\\\_search\\\\_and\\\\_indexing\", (only if \\'searchAnalyzer\\' and \\'indexAnalyzer\\' are not set)\\n\\n\"searchAnalyzer\": \"name\\\\_of\\\\_search\\\\_analyzer\", (only if \\'indexAnalyzer\\' is set and \\'analyzer\\' is not set)\\n\\n\"indexAnalyzer\": \"name\\\\_of\\\\_indexing\\\\_analyzer\", (only if\\n\\n\\'searchAnalyzer\\' is set and \\'analyzer\\' is not set)\\n\\n\"normalizer\": \"name\\\\_of\\\\_normalizer\", (applies to fields that are filterable)\\n\\n\"synonymMaps\": \"name\\\\_of\\\\_synonym\\\\_map\", (optional, only one synonym map per field is currently supported)\\n\\n\"dimensions\": \"number of dimensions used by an emedding models\",\\n\\n(applies to vector fields only, of type Collection(Edm.Single))\\n\\n\"vectorSearchProfile\": \"name\\\\_of\\\\_vector\\\\_profile\" (indexes can have many configurations, a field can use just one) }\\n\\n1,\\n\\n\"suggesters\": [ ],\\n\\n\"scoringProfiles\": [ ],\\n\\n\"analyzers\": (optional)[ ... ],\\n\\n\"charFilters\": (optional)[ ... ],\\n\\n\"tokenizers\": (optional)[ ... ],\\n\\n\"tokenFilters\": (optional)[ ... ],\\n\\n\"defaultScoringProfile\": (optional) \" ... \",\\n\\n\"corsOptions\": (optional) { },\\n\\n\"encryptionKey\": (optional) { },\\n\\n\"semantic\": (optional){ },\\n\\n\"vectorSearch\" : (optional) { } }\\n\\n簡潔にするために他の要素は折りたたまれていますが、次のリンク先で詳細を確認でき ます。\\n\\n● suggesters は、オートコンプリートのような先行入力クエリをサポートします。\\n\\n● scoringProfiles は関連性のチューニングに使われます。\\n\\n● analyzers は、アナライザーがサポートする言語規則やその他の特性に従って文字 列をトークンに処理するために使われます。\\n\\n● corsOptions、つまりクロスオリジン リモート スクリプト(CORS) は、さまざまな ドメインから要求を発行するアプリに使われます。\\n:unselected: :unselected: :unselected: :unselected: :unselected: :unselected:\\n● encryptionKey は、インデックス内の機密コンテンツの二重暗号化を構成します。\\n\\n● semantic は、全文検索とハイブリッド検索でのセマンティック再ランク付けを構 成します。\\n\\n● vectorSearch は、ベクトル フィールドとクエリを構成します。\\n\\n\\n## フィールド定義\\n\\n検索ドキュメントは、インデックス要求の作成に関する記事の本文の “フィールド\"コ :unselected: レクションによって定義されます。ドキュメントの識別のためのフィールド(キー)、 検索可能なテキストの格納、フィルター、ファセット、並べ替えをサポートするための フィールドが必要になります。ユーザーに表示しないデータのフィールドが必要にな る場合もあります。たとえば、検索スコアを上げるためにスコアリング プロファイル で使用できる、利益率やマーケティング プロモーションのフィールドが必要になるこ とがあります。\\n\\n受信データが階層化された性質を持つ場合は、入れ子構造に使われる複合型として、イ ンデックス内でそれを表すことができます。あらかじめ登録されているサンプルデー タ セットである Hotels は、各ホテルとの一対一のリレーションシップを持つ Address (複数のサブフィールドを含む) と、各ホテルに複数の部屋が関連付けられている複合型 コレクションの Rooms を使用した複合型を示しています。\\n\\n\\n### フィールド属性\\n\\nフィールド属性は、フィールドがどのように使用されるか(フルテキスト検索、ファセ ットナビゲーション、並べ替えなどの操作で使用されるかどうか) を決定します。\\n\\n文字列フィールドは多くの場合、\"検索可能\"および“取得可能\"としてマークされま す。検索結果を絞り込むために使用されるフィールドには、“並べ替え可能\"、\"フィル ター可能\"、および\"ファセット可能\" が含まれます。\\n\\nに テーブルを展開する\\n\\n|||\\n| - | - |\\n| 属性 | 説明 |\\n| \"検 索可 能\" | 全文またはベクトル検索可能。テキスト フィールドは、インデックス作成時に単語分割 などの字句解析の対象になります。検索可能フィールドを \"sunny day\" などの値に設定 した場合、その値は内部的に個別のトークン\"sunny\"と\"day\"に分割されます。詳細に ついては、「フルテキスト検索のしくみ」を参照してください。 |\\n\\n\"フ $filter クエリで参照されます。型 Edm.String または Collection(Edm.String) のフィル\\n\\nィル ター可能フィールドは単語分割されないため、比較は完全に一致するかどうかだけにな\\n\\n可\\n\\nター ります。たとえば、このようなフィールドを \"sunny day\"に設定した場合、 $filter=f 能\"\\n\\n|||\\n| - | - |\\n| 属性 eq \\'sunny\\' では一致が見つかりませんが、$filter=f eq \\'sunny day\\' では見つかりま す。 ||\\n| ||\\n| \"並 べ替 え可 能\" | 既定では、システムは結果をスコアで並べ替えますが、ドキュメント内のフィールドに 基づいて並べ替えを構成できます。型 Collection(Edm.String) のフィールドを“並べ替 え可能\"にすることはできません。 |\\n| \"フ アセ ット 可 能\" | 通常、カテゴリ(たとえば、特定の市にあるホテル) ごとのヒット カウントを含む検索結 果のプレゼンテーションで使用されます。このオプションは、型 Edm . GeographyPoint の フィールドでは使用できません。\"フィルター可能\"、\"並べ替え可能\"、または\"ファセッ 卜可能\" である型 Edm.String のフィールドの長さは、最大 32 キロバイトです。詳細に ついては、「Create Index (REST API) (インデックスの作成(REST API)」を参照してくだ さい。 |\\n| “キ ㅡ\" | インデックス内のドキュメントの一意識別子。キー フィールドとして正確に1 つのフィ ールドを選択する必要があり、それは型 Edm.String である必要があります。 |\\n| “取 得可 能\" | 検索結果でこのフィールドを返すことができるかどうかを決定します。これは、あるフ ィールド(\"利幅\"など)をフィルター、並べ替え、またはスコア付けのメカニズムとして 使用するが、このフィールドをエンド ユーザーには表示したくない場合に役立ちます。 true for key である必要があります。 |\\n\\nいつでも新しいフィールドを追加できますが、既存のフィールド定義はインデックスの 有効期間の間ロックされます。このため、開発者は通常、単純なインデックスを作成 :unselected: したり、アイデアをテストしたり、ポータル ページを使用して設定を検索したりする ためのポータルを使用します。インデックスを容易に再構築できるようにコード ベー スのアプローチに従う場合は、インデックス設計を頻繁に反復する方がより効率的で :unselected: す。\\n\\n\\n#### 1 注意\\n\\nインデックスの作成に使用する API には、さまざまな既定の動作があります。 REST API の場合、ほとんどの属性は既定で有効であり(たとえば、文字列フィー ルドの \"searchable\"および \"retrievable\" は true です)、無効にする場合は、単にそ れらを設定するだけです。 . NET SDK の場合は、逆のことが言えます。明示的に 設定していないプロパティの場合、既定では、特に有効にしない限り、対応する 検索動作は無効にされています。\\n\\n\\n## 物理的な構造とサイズ\\n\\nAzure Al Search におけるインデックスの物理的な構造は、主に内部実装です。そのス キーマにアクセスし、そのコンテンツにクエリを実行し、そのサイズを監視し、容量を\\n:unselected: :unselected:\\n管理することができますが、クラスター自体(インデックス、シャード、その他のファ イルとフォルダー)は、Microsoft が内部で管理します。\\n\\nインデックス サイズを監視するには、Azure portal の [インデックス] タブを使用する か、検索サービスに対して GET INDEX 要求を発行します。サービス統計情報要求を発 行し、ストレージ サイズの値を確認することもできます。\\n\\nインデックスのサイズは、次の条件によって決まります。\\n\\n● ドキュメントの数量と構成\\n\\n● 個々のフィールドの属性\\n\\n● インデックスの構成(具体的には、suggester を含めるかどうか)\\n\\nドキュメントの構成と数量は、インポートに選択した内容によって決まります。検索 インデックスには検索可能なコンテンツのみを含める必要があります。ソース データ にバイナリ フィールドが含まれている場合は、AI エンリッチメントを使用してコンテ ンツを解読して分析し、テキスト検索可能な情報を作成する場合を除き、それらのフィ ールドは除外してください。\\n\\nフィールドの属性によって動作が決まります。これらの動作をサポートするために、 インデックス作成プロセスによって、必要なデータ構造が作成されます。たとえば、 :unselected: Edm.String 型のフィールドの場合、“検索可能\" によって、トークン化された用語の転 置インデックスをスキャンする全文検索が呼び出されます。これに対して、\"フィルタ 一可能\" または\"並べ替え可能\" の属性では、未変更の文字列に対する反復処理がサポー トされます。次のセクションの例では、選択した属性に基づくインデックス サイズの バリエーションを示しています。\\n\\nsuggester は、先行入力またはオートコンプリートのクエリをサポートするコンストラ クトです。そのため、suggester を含めると、インデックス作成プロセスによって、逐 :unselected: 語的な文字の照合に必要なデータ構造が作成されます。 suggester はフィールド レベ ルで実装されるため、先行入力にふさわしいフィールドのみを選択してください。\\n\\n\\n### 属性と suggester がストレージに与える影響を示す例\\n\\n次のスクリーンショットは、属性のさまざまな組み合わせの結果であるインデックス格 納パターンを示しています。このインデックスは不動産サンプルインデックスに基づ いています。これは、データのインポート ウィザードと組み込みのサンプル データを 使用して簡単に作成できます。インデックスのスキーマは表示されませんが、インデ ックス名に基づいて属性を推測できます。たとえば、realestate-searchable インデック スでは \"searchable\"属性が選択されていて他には何もなく、realestate-retrievable イン デックスでは\"retrievable\" 属性が選択されていて他には何もなく、以下同様です。\\n\\n| NAME | DOCUMENT COUNT | STORAGE SIZE |\\n| - | - | - |\\n| realestate-all-attributes-no-suggester | 4,959 | 26.55 MiB |\\n| realestate-all-attributes-plus-suggester | 4,959 | 49.4 MIB |\\n| realestate-filterable-facetable-sortable | 4,959 | 20.89 MiB |\\n| realestate-no-attributes | 4,959 | 4.99 MİB |\\n| realestate-retrievable | 4,959 | 5.04 MIB |\\n| realestate-searchable | 4,959 | 9.95 MiB |\\n\\nこれらのインデックスのバリエーションはやや人為的なものですが、属性がストレージ に与える影響の広範な比較のために参照できます。\\n\\n● “取得可能\"は、インデックスのサイズに影響しません。\\n\\n● “フィルター可能\"、\"並べ替え可能\"、\"ファセット可能\"は、より多くのストレー ジを消費します。\\n\\n● suggester では、インデックス サイズが大きくなる可能性が大いにありますが、 スクリーンショットで示すほどではありません。(suggester 対応になる可能性の あるすべてのフィールドが選択されていますが、ほとんどのインデックスではこ のようなシナリオになる可能性はありません)。\\n\\nまた、上記の表に反映されていない事柄に、アナライザーの影響があります。\\n\\nedgeNgram トークナイザーを使って逐語的な文字シーケンス(a, ab, abc, abcd) を格 納した場合、インデックスは、標準アナライザーを使用した場合よりも大きくなりま す。\\n\\n\\n## 基本的な操作と相互作用\\n\\nインデックスの概要を理解したので、このセクションでは、1 つのインデックスに接続 してセキュリティを保護するなどの、インデックスの実行時操作について説明します。\\n\\n\\n### 4 注意\\n\\nインデックスを管理する際、インデックスの移動やコピーに関して、ポータルや API のサポートはないことに注意してください。代わりに、ユーザーは通常、ア プリケーション デプロイ ソリューションを別の検索サービスでポイントする(同 じインデックス名を使用している場合) か、名前を変更して現在の検索サービスに コピーを作成してからビルドします。\\n:unselected: :unselected:\\n## インデックスの分離性\\n\\nAzure Al Search で操作の対象となるインデックスは一度に1 つです。インデックスに 関連したすべての操作は、単一のインデックスが対象となります。関連するインデッ クスや、インデックス作成またはクエリのための独立したインデックスの結合の概念は ありません。\\n\\n\\n## 継続的に使用可能\\n\\nインデックスは、最初のドキュメントのインデックスが作成されるとすぐにクエリで使 用できますが、すべてのドキュメントのインデックスが作成されるまでは完全には機能 しません。内部的には、検索インデックスは パーティション間で分散され、レプリカ 上で実行されます。物理インデックスは内部で管理されます。論理インデックスはユ ーザーが管理します。\\n\\nインデックスは継続的に使用可能であり、一時停止したり、オフラインにしたりするこ とはできません。 継続的な操作のために設計されているので、コンテンツの更新やイ ンデックス自体への追加はリアルタイムで行われます。その結果、要求がドキュメン トの更新と一致する場合、クエリは一時的に不完全な結果を返す可能性があります。\\n\\nドキュメント操作(更新または削除)や、現在のインデックスの既存の構造と整合性に 影響しない変更(新しいフィールドの追加など) に対しては、クエリの継続性が存在し ます。構造上の更新(既存のフィールドの変更)を行う必要がある場合は、通常、開発 環境での削除と再構築のワークフローを使用して、または運用サービスでインデックス :unselected: の新しいバージョンを作成することによってそれらが管理されます。\\n\\nインデックスの再構築を避けるため、小規模な変更を行っている一部のお客様は、以前 のバージョンと共存する新しいものを作成することによって、フィールドの“バージョ ン管理\" を選択しています。これは、時間の経過と共に、特にレプリケートに負荷のか かる運用環境のインデックスで、古いフィールドまたは古いカスタム アナライザー定 義の形式の、孤立したコンテンツになります。インデックス ライフサイクル管理の一 部として、インデックスの計画更新に関する問題に対処することができます。\\n\\n\\n### エンドポイントの接続とセキュリティ\\n\\nすべてのインデックス作成とクエリの要求は、インデックスを対象とします。エンド ポイントは、通常、次のいずれかになります。\\n\\n<!-- PageFooter=\"[] テーブルを展開する\" -->\\n\\n| エンドポイント | 接続とアクセスの制御 |\\n| - | - |\\n| <your-service>. search.windows.net/indexes | インデックスのコレクションを対象とします。 インデックスを作成、一覧表示、または削除す るときに使用します。これらの操作には管理 者権限が必要です。管理者 API キーまたは Search 共同作成者ロールを通じて使用できま :unselected: す。 |\\n| <your - | 1 つのインデックスのドキュメント コレクショ |\\n| service>. search.windows. net/indexes/<your- | ンを対象とします。インデックスまたはデー 夕更新に対してクエリを実行するときに使用し ます。クエリには、読み取り権限で十分であ り、クエリ API キーまたはデータ閲覧者ロール :unselected: を通じて使用できます。データ更新の場合 は、管理者権限が必要です。 |\\n| index>/docs ||\\n\\n検索サブスクライバー(つまり検索サービスの作成者) は、Azure portal で検索サービス を管理できます。サービスを作成または削除するには、Azure サブスクリプションに 共同作成者以上のアクセス許可が必要です。検索サービスに直接接続するには、Azure portal にサインイン☑します。\\n\\n他のクライアントの場合は、接続手順のクイックスタートを確認することをお勧めしま す。\\n\\n● クイックスタート:REST\\n\\n● クイックスタート:Azure SDK\\n\\n\\n## 次のステップ\\n\\nAzure Al Search のほぼすべてのサンプルまたはチュートリアルを使用して、インデッ クスを作成する実践的な体験ができます。まず、目次から任意のクイックスタートを 選択できます。\\n\\nただし、データを使用してインデックスを読み込む方法についても理解しておく必要が あります。インデックスの定義とデータのインポートの方法は、連携して定義されま す。次の記事では、インデックスの作成および読み込みの詳細について説明します。\\n\\n● 検索インデックスの作成\\n\\n● ベクトル ストアを作成する\\n\\n· インデックスの別名を作成する\\n\\n● データ インポートの概要\\n\\n## ● インデックスを読み込む\\n\\n# Azure Al Search でのベクターストレー ジ\\n\\n[アーティクル]·2024/03/04\\n\\nAzure Al 検索には、ベクトル検索とハイブリッド検索用のベクトル ストレージと構成 が用意されています。サポートはフィールド レベルで実装されます。つまり、同じ検 索コーパスでベクター フィールドと非ベクター フィールドを組み合わせることができ ます。\\n\\nベクターは検索インデックスに格納されます。Create Index REST API または同等の Azure SDK メソッドを使用して、ベクター ストアを作成します。\\n\\nベクター ストレージの主な考慮事項は以下のとおりです。\\n\\n● 目的とするベクトル取得パターンに基づいて、ユース ケースに合うスキーマを設 計します。\\n\\n● インデックス サイズを見積もり、検索サービスの容量を確認します。\\n\\n● ベクトル ストアを管理する\\n\\n● ベクトル ストアをセキュリティで保護する\\n\\n\\n## ベクター取得パターン\\n\\nAzure Al Search には、検索結果を操作するための2 つのパターンがあります。\\n\\n● 生成検索。 言語モデルは、Azure Al Search のデータを使用して、ユーザーのクエ リに対する応答を作成します。このパターンには、プロンプトを調整し、コンテ :unselected: キストを維持するためのオーケストレーション レイヤーが含まれます。このパタ ーンでは、検索結果はプロンプト フローにフィードされ、GPT や Text-Davinci な どのチャット モデルが受け取ります。このアプローチは、検索インデックスによ :unselected: ってグラウンド データが提供される取得拡張生成(RAG) アーキテクチャに基づい ています。\\n\\n● 検索バー、クエリ入力文字列、レンダリングされた結果を使用した従来の検索。 検索エンジンによって、ベクトル クエリが受け入れられて実行され、応答が作成 されます。ユーザーはそれらの結果をクライアント アプリにレンダリングしま す。 Azure Al Search では、結果はフラット化された行セットで返され、検索結果 を含めるフィールドを選ぶことができます。チャット モデルがないため、応答で 人間が判読できる非ベクトル コンテンツをベクトル ストア(検索インデックス) に設定することが期待されます。検索エンジンはベクトルに一致しますが、非べ クトル値を使用して検索結果を設定する必要があります。ベクトル クエリとハイ\\n\\nブリッド クエリは、従来の検索シナリオ用に作成できるクエリ要求の型に対応し ます。\\n\\nインデックス スキーマには、主なユース ケースが反映されている必要があります。次 のセクションでは、生成 AI と従来の検索用に構築されるソリューションのフィールド 構成の違いに着目します。\\n\\n\\n## ベクトル ストアのスキーマ\\n\\nベクトル ストアのインデックス スキーマには、名前、キー フィールド(文字列)、1つ 以上のベクトル フィールド、およびベクトル構成が必要です。非ベクター フィールド は、ハイブリッド クエリ、または言語モデルを通過する必要のない、人間が読み取り 可能な逐語的なコンテンツを返す場合に推奨されます。ベクター構成の手順について は、「ベクター ストアを作成する」を参照してください。\\n\\n\\n### 基本的なベクター フィールドの構成\\n\\nベクトル フィールドは、データ型とベクトル固有のプロパティによって区別されま :unselected: す。フィールド コレクション内のベクトル フィールドの外観を以下に示します。\\n\\n<figure>\\n\\n![](figures/0)\\n\\n<!-- FigureContent=\"JSON { \"name\" : \"content\\\\_vector\", \"type\": \"Collection(Edm.Single)\", \"searchable\": true, \"retrievable\": true, \"dimensions\": 1536, \"vectorSearchProfile\": \"my-vector-profile\" }\" -->\\n\\n</figure>\\n\\n\\nベクトル フィールドの型は Collection(Edm.Single) です。\\n\\nベクトル フィールドは検索可能で取得可能である必要がありますが、フィルター可 能、ファセット可能、並べ替え可能にすることはできません。また、アナライザー、ノ ーマライザー、シノニム マップの割り当てを持つことはできません。\\n\\nベクトル フィールドでは、埋め込みモデルによって生成される埋め込みの数に dimensions を設定する必要があります。たとえば、text-embedding-ada-002 では、テ キストのチャンクごとに1,536 個の埋め込みが生成されます。\\n\\nベクトル フィールドには、\"ベクトル検索プロファイル\"によって示されるアルゴリズ :unselected: ムを使用してインデックスが作成されます。このプロファイルはインデックス内の他の\\n\\n場所で定義されているため、例では示されていません。詳細については、ベクトル検 索の構成に関するページを参照してください。\\n\\n# 基本的なベクトル ワークロードのフィールド コレクショ ン\\n\\nベクトル ストアでは、ベクトル フィールド以外にもさらにフィールドが必要です。た とえば、キー フィールド(この例では \"id\") はインデックス要件です。\\n\\nJSON\\n\\n\"name\": \"example-basic-vector-idx\",\\n\\n\"fields\": [\\n\\n{ \"name\": \"id\", \"type\": \"Edm. String\", \"searchable\": false, \"filterable\": true, \"retrievable\": true, \"key\": true },\\n\\n{ \"name\": \"content\\\\_vector\", \"type\": \"Collection(Edm.Single)\",\\n\\n\"searchable\": true, \"retrievable\": true, \"dimensions\": 1536,\\n\\n\"vectorSearchProfile\": null },\\n\\n{ \"name\": \"content\", \"type\": \"Edm. String\", \"searchable\": true, \"retrievable\": true, \"analyzer\": null },\\n\\n{ \"name\": \"metadata\", \"type\": \"Edm. String\", \"searchable\": true,\\n\\n\"filterable\": true, \"retrievable\": true, \"sortable\": true, \"facetable\": true }\\n\\n]\\n\\n\"content\" フィールドなどの他のフィールドでは、人間が判読できる \"content vector\" フィールドと同等のものが提供されます。応答の作成専用の言語モデルを使用してい る場合は、非ベクトル コンテンツ フィールドを省略できますが、クライアント アプリ に検索結果を直接プッシュするソリューションには非ベクトル コンテンツが必要で す。\\n\\nメタデータ フィールドは、特にメタデータにソース ドキュメントに関する配信元情報 が含まれている場合に、フィルターに役立ちます。ベクトル フィールド上で直接フィ ルター処理を行うことはできませんが、ベクトル クエリの実行前または実行後にフィ ルター処理を行うプリフィルター モードまたはポストフィルター モードを設定するこ とができます。\\n\\n\\n## データのインポートとベクター化ウィザードによって生成 されるスキーマ\\n\\n評価と概念実証のテストには、データのインポートとベクター化ウィザードをお勧めし ます。ウィザードによって、このセクションのスキーマ例が生成されます。\\n\\nこのスキーマの偏りは、検索ドキュメントがデータ チャンクを中心に構築されている ことです。RAG アプリで一般的なように、言語モデルが応答を作成する場合は、デー タ チャンクを中心に設計されたスキーマが必要です。\\n\\nデータ チャンクは、言語モデルの入力制限内を維持するために必要ですが、複数の親 ドキュメントからプルされたコンテンツの小さなチャンクに対してクエリを照合できる 場合の類似性検索の精度も向上します。最後に、セマンティック ランク付けを使用し ている場合、セマンティック ランカーにはトークン制限もあります。これは、データ チャンクがアプローチの一部である場合に、より簡単に満たされます。\\n :unselected:\\n次の例では、検索ドキュメントごとに1つのチャンク ID、チャンク、タイトル、ベク ター フィールドがあります。 blob メタデータ(パス) の base 64 エンコードを使用し て、chunkID と親 ID がウィザードによって設定されます。チャンクとタイトルは、 BLOB コンテンツと BLOB 名から派生します。ベクター フィールドのみが完全に生成 されます。これは、ベクトル化されたバージョンのチャンク フィールドです。埋め込 みは、指定した Azure OpenAl 埋め込みモデルを呼び出すことによって生成されます。\\n\\n\\n### JSON\\n\\n\"name\": \"example-index-from-import-wizard\",\\n\\n\"fields\": [\\n\\n{\"name\": \"chunk\\\\_id\", \"type\": \"Edm. String\", \"key\": true, \"searchable\" : true, \"filterable\": true, \"retrievable\": true, \"sortable\": true, \"facetable\": true, \"analyzer\": \"keyword\"},\\n\\n{ \"name\": \"parent\\\\_id\", \"type\": \"Edm. String\", \"searchable\": true, \"filterable\": true, \"retrievable\": true, \"sortable\": true},\\n\\n{ \"name\": \"chunk\", \"type\": \"Edm. String\", \"searchable\": true, \"filterable\": false, \"retrievable\": true, \"sortable\": false},\\n\\n{ \"name\": \"title\", \"type\": \"Edm. String\", \"searchable\": true, \"filterable\": true, \"retrievable\": true, \"sortable\": false},\\n\\n{ \"name\": \"vector\", \"type\": \"Collection (Edm. Single)\", \"searchable\": true, \"retrievable\": true, \"dimensions\": 1536, \"vectorSearchProfile\": \"vector- 1707768500058-profile\"} ]\\n\\n\\n## RAG アプリとチャットスタイル アプリのスキーマ\\n\\n生成検索用のストレージを設計する場合は、インデックスを作成してベクトル化した静 的コンテンツに対して個別のインデックスを作成し、プロンプト フローで使用できる :unselected: 会話用に2 つ目のインデックスを作成できます。以下のインデックスは、chat-with- your-data-solution-accelerator& アクセラレータから作成されます。\\n\\n## Home > contosochat-search\\n\\n= contosochat-search | Indexes \\\\*\\n\\nSearch service\\n\\nP Search\\n\\n« + Add index V :unselected: Refresh Delete\\n\\nSearch management\\n\\nFilter by name ...\\n\\nIndexes\\n\\nIndexers\\n\\nData sources\\n :selected:\\nAliases\\n\\n| Name | Document Count | Storage Size |\\n| - | - | - |\\n| conversations | 14 | 434.61 KB |\\n| chat-index | 191 | 5.42 MB |\\n\\n生成検索エクスペリエンスをサポートするチャット インデックスのフィールド:\\n\\n\\n## JSON\\n\\n\"name\": \"example-index-from-accelerator\",\\n\\n\"fields\": [\\n\\n{ \"name\": \"id\", \"type\": \"Edm. String\", \"searchable\": false, \"filterable\" : true, \"retrievable\": true },\\n\\n{ \"name\": \"content\", \"type\": \"Edm. String\", \"searchable\": true,\\n\\n\"filterable\": false, \"retrievable\": true },\\n\\n{ \"name\": \"content\\\\_vector\", \"type\": \"Collection (Edm. Single)\",\\n\\n\"searchable\": true, \"retrievable\": true, \"dimensions\": 1536, \"vectorSearchProfile\": \"my-vector-profile\"},\\n\\n{ \"name\": \"metadata\", \"type\": \"Edm. String\", \"searchable\": true, \"filterable\": false, \"retrievable\": true },\\n\\n{ \"name\": \"title\", \"type\": \"Edm. String\", \"searchable\": true, \"filterable\": true, \"retrievable\": true, \"facetable\": true },\\n\\n{ \"name\" : \"source\", \"type\": \"Edm. String\", \"searchable\": true, \"filterable\": true, \"retrievable\": true },\\n\\n{ \"name\": \"chunk\", \"type\": \"Edm. Int32\", \"searchable\": false, \"filterable\": true, \"retrievable\": true },\\n\\n{ \"name\": \"offset\", \"type\": \"Edm. Int32\", \"searchable\": false, \"filterable\": true, \"retrievable\": true } ]\\n\\n\\n### オーケストレーションとチャット履歴をサポートする会話インデックスのフィールド:\\n\\nJSON\\n\\n\"fields\": [\\n\\n{ \"name\": \"id\", \"type\": \"Edm. String\", \"key\": true, \"searchable\": false, \"filterable\": true, \"retrievable\": true, \"sortable\": false, \"facetable\": false },\\n\\n{ \"name\": \"conversation\\\\_id\", \"type\": \"Edm. String\", \"searchable\": false, \"filterable\": true, \"retrievable\": true, \"sortable\": false, \"facetable\": true },\\n\\n{ \"name\": \"content\", \"type\": \"Edm. String\", \"searchable\": true,\\n\\n\"filterable\": false, \"retrievable\": true },\\n\\n{ \"name\": \"content\\\\_vector\", \"type\": \"Collection(Edm. Single)\",\\n\\n\"searchable\": true, \"retrievable\": true, \"dimensions\": 1536,\\n:selected:\\n\"vectorSearchProfile\": \"default-profile\" },\\n\\n{ \"name\": \"metadata\", \"type\": \"Edm. String\", \"searchable\": true, \"filterable\": false, \"retrievable\": true },\\n\\n{ \"name\": \"type\", \"type\": \"Edm.String\", \"searchable\": false,\\n\\n\"filterable\": true, \"retrievable\": true, \"sortable\": false, \"facetable\" : true },\\n\\n{ \"name\": \"user\\\\_id\", \"type\": \"Edm. String\", \"searchable\": false, \"filterable\": true, \"retrievable\": true, \"sortable\": false, \"facetable\": true },\\n\\n{ \"name\": \"sources\", \"type\": \"Collection(Edm.String)\", \"searchable\" : false, \"filterable\": true, \"retrievable\": true, \"sortable\": false, \"facetable\": true },\\n\\n{ \"name\": \"created\\\\_at\", \"type\": \"Edm. DateTimeOffset\", \"searchable\" : false, \"filterable\": true, \"retrievable\": true },\\n\\n{ \"name\": \"updated\\\\_at\", \"type\": \"Edm. DateTimeOffset\", \"searchable\" : false, \"filterable\": true, \"retrievable\": true } ]\\n\\n次に示すのは、Search Explorer における会話インデックスの検索結果のスクリーンシ ョットです。検索に条件がないため、検索スコアは 1.00 となっています。オーケスト レーションとプロンプト フローをサポートするために存在するフィールドに注目して ください。 会話 ID は、特定のチャットを特定します。 \"type\" は、コンテンツがユー ザーとアシスタントのどちらからのものであるかを示します。日付は、古さによって 履歴からチャットを削除するために使用されます。\\n\\n<figure>\\n\\n![](figures/1)\\n\\n</figure>\\n\\n\\n| Results |||||\\n| 18 | | { | | |\\n| - | - | - | - | - |\\n| 19 | | \"@search.score\": 1, | | |\\n| 20 | | \"id\": \"NDImODY4MjgtOWU2ZC00YTY3LTgWNTItNmI20WUyYzllZjRj\", | | |\\n| 21 | | \"conversation\\\\_id\": \"01db26eb-f781-462b-8da3-0ec10e551a35\", | | |\\n| 22 | || \"content\": \"The Gulf Stream carries a lot of heat from the Equator toward the far North Atlantic, [  | |\\n| 23 | | \"metadata\": \"{\\\\\\\\\"conversation\\\\_id\\\\\\\\\": \\\\\\\\\"01db26eb-f781-462b-8da3-0ec10e551a35\\\\\\\\\", \\\\\\\\\"sources\\\\\\\\\": [\\\\\\\\\"doc\\\\_2€ | | |\\n| 24 | | \"type\": \"assistant\", | | |\\n| 25 | | \"user\\\\_id\": null, | | |\\n| 26 | | \"sources\": [ | | |\\n| 27 | | \"doc\\\\_2005da260b463009d5e230b09a55acedf51fbcd7\", | | |\\n| 28 | | \"doc\\\\_541c34b9a54b97ac888034a21c73fc6910243741\" | | |\\n| 29 | | ], | | |\\n| 30 | | \"created\\\\_at\": \"2024-01-15T05:19:52Z\", | | |\\n| 31 | | \"updated\\\\_at\": \"2024-01-15T05:19:52Z\" | | |\\n| 32 | | }, | | |\\n| 33 | | { | | |\\n| 34 | | \"@search.score\": 1, | | |\\n| 35 | | \"id\": \"NDMINWMOMZMtZGZmZi00ZmQ4LTk3YTgtY2MyMGU3YmRhNTVm\", | | |\\n| 36 | | \"conversation\\\\_id\": \"01db26eb-f781-462b-8da3-0ec10e551a35\", | | |\\n| 37 | | \"content\": \"what can you tell me about winds\", | | |\\n| 38 | | \"metadata\": \"{\\\\\\\\\"type\\\\\\\\\"; \\\\\\\\\"user\\\\\\\\\", \\\\\\\\\"conversation\\\\_id\\\\\\\\\": \\\\\\\\\"01db26eb-f781-462b-8da3-0ec10e551a35\\\\\\\\\", \\\\\\\\\\' | | |\\n| 39 | | \"type\": \"user\", | | |\\n| 40 | | \"user\\\\_id\": null, | | |\\n| 41 | | \"sources\": [], | | |\\n| 42 | | \"created\\\\_at\": \"2024-01-15T19:51:12Z\", | | |\\n| 43 | | \"updated\\\\_at\": \"2024-01-15T19:51:12Z\" | | |\\n\\n\\n## 物理的な構造とサイズ\\n\\nAzure Al Search におけるインデックスの物理的な構造は、主に内部実装です。スキー マへのアクセス、コンテンツの読み込みとクエリ実行、サイズの監視、容量の管理を行 うことはできますが、クラスター自体 (逆インデックスとベクトル インデックス、シャ\\n\\nード、その他のファイルとフォルダー) は、Microsoft によって内部的に管理されま す。\\n\\nインデックスのサイズと内容は、以下によって決まります。\\n\\n● ドキュメントの数量と構成\\n\\n● 個々のフィールドの属性。たとえば、フィルター処理可能なフィールドにはより 多くのストレージが必要です。\\n\\n● 類似性検索に HNSW と網羅的 KNN のどちらを選択するかに基づくインデックス 構成(内部的なナビゲーション構造がどのように作成されるかを指定するベクトル 構成を含む)。\\n\\nAzure Al 検索はベクトル ストレージに制限を課しており、これは、すべてのワークロ :unselected: ードにとってバランスのとれた安定したシステムを維持するのに役立ちます。利用者 が制限の範囲内に留まるのを手助けするために、ベクトルの使用状況は Azure portal に おいてと、サービスとインデックスの統計情報を使用したプログラム的な方法とで個別 に追跡され報告されます。\\n\\n次に示すのは、1 つのパーティションと1つのレプリカで構成された S1 サービスのス クリーンショットです。この特定のサービスには、平均して1 つのベクトル フィール ドを含む 24 個の小さなインデックスがあり、各フィールドは 1536 個の埋め込みで構 成されています。2 番目のタイルが示しているのは、ベクトル インデックスのクォー タと使用状況です。ベクトル インデックスは、各ベクトル フィールドに対して作成さ れた内部的なデータ構造です。そのため、ベクトル インデックスのストレージは常 に、インデックス全体によって使用されるストレージの一部となります。残りの部分 は、その他の非ベクトル フィールドとデータ構造によって使用されます。\\n\\n<figure>\\n\\n![](figures/2)\\n\\n<!-- FigureContent=\"Storage Vector index size Indexes 3% Current Current 93 MB Current 459.63 MB 24 Quota 25 GB Quota Quota 3 GB 50 View scale View indexes View indexes\" -->\\n\\n</figure>\\n\\n\\nベクトル インデックスの制限と見積もりについては別の記事でカバーされています が、前もって強調しておくべき 2 つの点として、ストレージの最大値はサービス レベ ルによって異なるということと、検索サービスがいつ作成されたかによっても異なると いうことがあります。同じレベルでも新しいサービスの方が、かなり多いベクトルイ ンデックスの容量を持ちます。このため、以下のアクションを行ってください。\\n\\n● 検索サービスのデプロイ日を確認する。2023 年7月1日より前に作成されてい る場合は、容量を増やすために新しい検索サービスを作成することを検討してく ださい。\\n\\n● ベクトル ストレージ要件の変動が予想される場合、スケーラブルなレベルを選択 する。Basic レベルは、1 つのパーティションに固定されています。柔軟性を高 め、パフォーマンスを向上させるには、Standard 1 (S1) 以上を検討してくださ い。\\n\\n\\n## 基本的な操作と相互作用\\n\\nこのセクションでは、1 つのインデックスへの接続やセキュリティ保護など、ベクトル の実行時操作について紹介します。\\n\\n\\n### 1 注意\\n\\nインデックスを管理する際、インデックスの移動やコピーに関して、ポータルや API のサポートはないことに注意してください。代わりに、ユーザーは通常、ア プリケーション デプロイ ソリューションを別の検索サービスでポイントする(同 じインデックス名を使用している場合)か、名前を変更して現在の検索サービスに コピーを作成してからビルドします。\\n\\n\\n## 継続的に使用可能\\n\\nインデックスは、最初のドキュメントのインデックスが作成されるとすぐにクエリで使 用できますが、すべてのドキュメントのインデックスが作成されるまでは完全には機能 しません。内部的には、インデックスは複数のパーティションにわたって分散され、 レプリカ上で実行されます。物理インデックスは内部で管理されます。 論理インデッ クスはユーザーが管理します。\\n\\nインデックスは継続的に使用可能であり、一時停止したり、オフラインにしたりするこ とはできません。継続的な操作のために設計されているので、コンテンツの更新やイ ンデックス自体への追加はリアルタイムで行われます。その結果、要求がドキュメン トの更新と一致する場合、クエリは一時的に不完全な結果を返す可能性があります。\\n:unselected: :unselected:\\nドキュメント操作(更新または削除)や、現在のインデックスの既存の構造と整合性に 影響しない変更(新しいフィールドの追加など) に対しては、クエリの継続性が存在し ます。構造上の更新(既存のフィールドの変更)を行う必要がある場合は、通常、開発 環境での削除と再構築のワークフローを使用して、または運用サービスでインデックス :unselected: の新しいバージョンを作成することによってそれらが管理されます。\\n\\nインデックスの再構築を避けるため、小規模な変更を行っている一部のお客様は、以前 のバージョンと共存する新しいものを作成することによって、フィールドの“バージョ ン管理\" を選択しています。これは、時間の経過と共に、特にレプリケートに負荷のか かる運用環境のインデックスで、古いフィールドまたは古いカスタム アナライザー定 義の形式の、孤立したコンテンツになります。インデックス ライフサイクル管理の一 部として、インデックスの計画更新に関する問題に対処することができます。\\n\\n\\n### エンドポイント接続\\n\\nベクトルのインデックス作成とクエリ要求はすべて、インデックスを対象とします。 エンドポイントは、通常、次のいずれかになります。\\n\\n〔〕 テーブルを展開する\\n\\n| エンドポイント | 接続とアクセスの制御 |\\n| - | - |\\n| <your-service>.search.windows.net/indexes | インデックスのコレクションを対象とします。 インデックスを作成、一覧表示、または削除す るときに使用します。これらの操作には管理 者権限が必要です。管理者 API キーまたは Search 共同作成者ロールを通じて使用できま :unselected: す。 |\\n| <your - | 1 つのインデックスのドキュメント コレクショ |\\n| service>. search.windows. net/indexes/<your- | ンを対象とします。インデックスまたはデー |\\n| index>/docs | 夕更新に対してクエリを実行するときに使用し ます。クエリには、読み取り権限で十分であ り、クエリ API キーまたはデータ閲覧者ロール :unselected: を通じて使用できます。データ更新の場合 は、管理者権限が必要です。 |\\n\\n\\n## Azure AI 検索への接続方法\\n\\n1\\\\. アクセス許可または API アクセス キーがあることを確認します。既存のインデッ クスに対してクエリを実行する場合を除き、検索サービスのコンテンツを管理お よび表示するには、管理者権限または共同作成者ロールの割り当てが必要です。\\n:unselected:\\n2\\\\. Azure portal から開始します。検索サービスを作成したユーザーは、[アクセス 制御(IAM)] ページを使用して他のユーザーにアクセスを許可するなど、検索サー ビスを表示および管理できます。\\n\\n3\\\\. プログラムによるアクセスのために他のクライアントに移動します。最初の手順 としては、以下のクイックスタートとサンプルをお勧めします。\\n\\n● クイックスタート: REST\\n\\n● ベクトルのサンプル☑\\n\\n\\n## ベクトル データへのアクセスをセキュリティで保護する\\n\\nAzure Al 検索では、データ暗号化、インターネットなしのシナリオ用のプライベート接 続、Microsoft Entra ID を介した安全なアクセスのためのロールの割り当てが実装され ています。エンタープライズ セキュリティ機能の全容については、「Azure AI 検索の セキュリティ」で説明されています。\\n\\n\\n## ベクトル ストアを管理する\\n\\nAzure には、診断ログとアラートを含む監視プラットフォームが用意されています。 推奨するベスト プラクティスを次に示します。\\n\\n● 診断ログを有効にする\\n :unselected:\\n● アラートを設定する\\n\\n● クエリとインデックスのパフォーマンスを分析する\\n\\n\\n## 関連項目\\n\\n● REST API を使用してベクトル ストアを作成する(クイックスタート)\\n\\n● ベクトル ストアを作成する\\n\\n● ベクトル ストアにクエリを実行する\\n\\n# Azure Al Search 内のナレッジ ストア\\n\\n[アーティクル]·2024/01/10\\n\\nナレッジ ストアは、Azure Al Search のスキルセットによって作成された AI エンリッチ コンテンツのセカンダリ ストレージです。Azure Al Search では、インデックス作成ジ ョブは常に出力を検索インデックスに送信しますが、インデクサーにスキルセットをア タッチする場合は、必要に応じて、AZURE Storage のコンテナーまたはテーブルに Al エンリッチされた出力を送信することもできます。ナレッジ ストアは、ナレッジ マイ ニングなどの検索以外のシナリオで、独立した分析またはダウンストリーム処理に使用 できます。\\n\\nインデックス作成の 2 つの出力(検索インデックスとナレッジ ストア) は、同じパイプ ラインの相互排他的な製品です。これらは同じ入力から派生し、同じデータを含みま すが、その内容は構造化され、格納され、さまざまなアプリケーションで使用されま す。\\n\\n<figure>\\n\\n![](figures/3)\\n\\n<!-- FigureContent=\"Cognitive Skill Source Data Document Cracking Search Index </> Skillset: Extensible enrichment pipeline :unselected: Enriched Documents Retrieval Analytics Projections Machine Learning Human Validation Knowledge Store\" -->\\n\\n</figure>\\n\\n\\n物理的には、ナレッジ ストアは Azure Storage です。つまり Azure Table Storage か Azure Blob Storage、またはその両方になります。Azure Storage に接続できるすべて のツールまたはプロセスは、ナレッジ ストアのコンテンツを使用できます。\\n :unselected:\\nAzure portal を使用して表示すると、ナレッジ ストアはテーブル、オブジェクト、また はファイルの他のコレクションのようになります。次のスクリーンショットは、3つ のテーブルで構成されるナレッジ ストアを示しています。プレフィックスなどの名前 付け規則を kstore 採用して、コンテンツをまとめることができます。\\n:unselected: :unselected:\\nHome > demoblobstorage\\n\\n# demoblobstorage | Storage browser (preview) Storage account\\n\\n...\\n\\n<figure>\\n\\n![](figures/4)\\n\\n<!-- FigureContent=\"P Search (Ctrl+) blobstorage < + Add table :unselected: Refresh Delete = Overview Favorites Tables Activity log > Recently viewed Authentication method: Access key (Switch to Azure 0 Tags Blob containers 0 Search tables by prefix Diagnose and solve problems File shares Showing all 9 items Access Control (IAM) Queues Data migration Name Tables Events kstoreProjectionDemoDocument Storage browser (preview) kstoreProjectionDemoEntities Data storage kstoreProjectionDemoKeyPhrases Containers MsAzSearchIndexerCacheIndex33b0d ...\" -->\\n\\n</figure>\\n\\n\\n\\n## ナレッジ ストアのメリット\\n\\nナレッジ ストアの主な利点は、コンテンツに柔軟にアクセスできることと、データを 形成する機能という 2 つの点にあります。\\n\\nAzure Al Search のクエリでのみアクセスできる検索インデックスとは異なり、ナレッ ジ ストアには、Azure Storage への接続をサポートする任意のツール、アプリ、または プロセスからアクセスできます。この柔軟性によって、エンリッチメント パイプライ ンによって生成された、分析およびエンリッチメントされたコンテンツを消費するため の新しいシナリオが開きます。\\n\\nデータをエンリッチする同じスキルセットを、データの形成にも使用できます。Power BI のようなツールは、テーブルの方が適していますが、データ サイエンス ワークロー :unselected: ドには BLOB 形式の複雑なデータ構造が必要になる場合があります。スキルセットに Shaper スキルを追加すると、データのシェイプを制御できるようになります。そし て、このシェイプをテーブルや BLOB などのプロジェクションに渡すことで、データの 使用目的に沿った物理的なデータ構造を作成することができます。\\n\\n次のビデオでは、これらの利点の両方について説明します。\\n\\nhttps://www.youtube-nocookie.com/embed/XWzLBP8iWqg?version=3 &\\n\\n\\n### ナレッジ ストアの定義\\n\\nナレッジ ストアは、スキルセット定義内で定義されており、2 つのコンポーネントが あります。\\n\\n● Azure ストレージの接続文字列\\n:selected: :unselected: :selected: :unselected: :selected: :unselected: :selected: :unselected: :unselected:\\n● ナレッジ ストアがテーブル、オブジェクト、ファイルのいずれで構成されている かを決定するプロジェクション。プロジェクション要素は配列です。1 つのナレ :unselected: ッジ ストア内に、テーブル、オブジェクト、ファイルの組み合わせを複数セット 作成することができます。\\n\\n<figure>\\n\\n![](figures/5)\\n\\n<!-- FigureContent=\"JSON \"knowledgeStore\": { \"storageConnectionString\" : \"<YOUR-AZURE-STORAGE-ACCOUNT-CONNECTION- STRING>\", \"projections\" : [ { \"tables\": [ ], \"objects\": [ ], \"files\": [ ] } ] }\" -->\\n\\n</figure>\\n\\n\\nこの構造体で指定するプロジェクションの種類は、ナレッジ ストアが使用するストレ ージの種類を決定しますが、その構造体は決定しません。テーブル、オブジェクト、 およびファイルのフィールドは、ナレッジ ストアをプログラムで作成する場合は Shaper スキルの出力によって決定され、ポータルを使用している場合はデータのイン ポート ウィザードによって決定されます。\\n\\ntables は、エンリッチメントされたコンテンツを Table Storage に投影します。 分析ツールへの入力のために表形式のレポート構造が必要な場合や、データ フレ ームとして他のデータ ストアにエクスポートする場合は、テーブル プロジェクシ :unselected: ョンを定義します。同じプロジェクション グループ内の複数の tables を指定し て、エンリッチメントされたドキュメントのサブセットまたは断面を取得するこ とができます。同じプロジェクション グループ内では、テーブルのリレーション シップが保持されるため、すべてのテーブルを操作できます。\\n\\nプロジェクションされたコンテンツは集計または正規化されません。次のスクリ ーンショットは、キー フレーズで並べ替えられたテーブルを示しており、隣接す る列に親ドキュメントが示されています。インデックス作成中のデータ インジェ ストとは対照的に、言語分析やコンテンツの集計はありません。複数形と大文字 と小文字の違いは、一意のインスタンスと見なされます。\\n:unselected: :unselected: :unselected:\\n|||\\n| - | - |\\n| Content.metadata\\\\_storage\\\\_name | Content.KeyPhrases |\\n| Cognitive Services and Content Intelligence.pptx | Computer Vision |\\n| 10-K-FY16.html | computing device |\\n| 10-K-FY16.html | computing devices |\\n| MSFT\\\\_FY17\\\\_10K.docx | computing devices |\\n| 10-K-FY16.html | Computing segment |\\n| Cognitive Services and Bots (spanish).pdf | confianza |\\n\\n● objects では、JSON ドキュメントを BLOB ストレージに投影します。 object の 物理的表現は、エンリッチメントされたドキュメントを表す階層型の JSON 構造 体です。\\n\\nfiles では、イメージ ファイルを BLOB ストレージに投影します。 file は、ド ● キュメントから抽出され、BLOB ストレージにそのまま転送されるイメージです。 \"ファイル\" という名前ですが、ファイル ストレージではなく Blob Storage に表示 されます。\\n\\n\\n### ナレッジ ストアの作成\\n\\nナレッジ ストアを作成するには、ポータルまたは API を使用します。\\n\\nAzure Storage、スキルセット、インデクサーが必要になります。インデクサーには検 索インデックスが必要なので、インデックス定義も指定する必要があります。\\n\\n完成したナレッジ ストアへの最短ルートとしては、ポータル アプローチを採用してく :unselected: ださい。または、オブジェクトがどのように定義され、関連しているかをより深く理 解するには、REST API を選択します。\\n\\nAzure Portal\\n\\nデータのインポート ウィザードを使用して、4 つの手順で最初のナレッジ ストア を作成します。\\n\\n1\\\\. エンリッチするデータを含むデータ ソースを定義します。\\n\\n2\\\\. スキルセットを定義します。スキルセットにより、エンリッチメント ステッ プとナレッジ ストアが指定されます。\\n\\n3\\\\. インデックス スキーマを定義します。これは必要ない場合もありますが、イ ンデクサーでは必要です。このウィザードではインデックスを推測できま す。\\n\\n4\\\\. ウィザードの完了。この最後のステップで、抽出、エンリッチメント、ナレ ッジ ストアの作成が行われます。\\n\\nこのウィザードを使用すると、いくつかのタスクを自動化できます。具体的に は、整形とプロジェクションの両方(Azure Storage 内の物理データ構造の定義) が :unselected: 作成されます。\\n\\n\\n## アプリに接続する\\n\\nエンリッチされたコンテンツがストレージに存在するようになると、Azure Blob に接 続する任意のツールまたはテクノロジを使用して、コンテンツを探索、分析、または使 :unselected: 用できます。次の一覧が開始点です。\\n\\n● エンリッチされたドキュメント構造とコンテンツを表示するための Azure portal の Storage Explorer またはストレージブラウザー(プレビュー)。これは、ナレッ ジ ストアのコンテンツを表示するためのベースライン ツールと考えてください。\\n\\n● レポートと分析のための Power BI。\\n\\n● さらに操作するための Azure Data Factory。\\n\\n\\n### コンテンツのライフサイクル\\n\\nインデクサーとスキルセットを実行するたび、スキルセットまたは基になるソース デ ータが変更された場合、ナレッジ ストアが更新されます。インデクサーによって取得 された変更は、エンリッチメント プロセスを通じてナレッジ ストア内のプロジェクシ :unselected: ョンに反映され、投影されたデータが元のデータ ソース内のコンテンツの現在の表現 になります。\\n\\n\\n#### 4 注意\\n\\nプロジェクション内のデータを編集することができますが、ソース データ内のド キュメントが更新された場合、次のパイプライン呼び出しですべての編集が上書 きされます。\\n\\n\\n## ソース データの変更\\n\\n変更の追跡をサポートするデータ ソースの場合、インデクサーは新規および変更され たドキュメントを処理し、既に処理されている既存のドキュメントをバイパスします。 タイムスタンプ情報はデータ ソースによって異なりますが、BLOB コンテナーでは、イ\\n\\nンデクサーによって lastmodified の日付が確認され、取り込む必要がある BLOB が特 定されます。\\n\\n\\n## スキルセットの変更\\n\\nスキルセットに変更を加える場合は、エンリッチメントされたドキュメントのキャッシ ュを有効にして、可能な限り既存のエンリッチメントを再利用する必要があります。\\n\\n増分キャッシュを使用しない場合、インデクサーは常に高いウォーター マークの順に 逆戻りせずドキュメントを処理します。BLOB の場合、インデクサーは、インデクサー の設定やスキルセットに対する変更に関係なく、lastModified で並べ替えた BLOB を 処理します。スキルセットを変更した場合、以前に処理されたドキュメントは、新し いスキルセットを反映するように更新されません。スキルセットの変更後に処理され たドキュメントでは新しいスキルセットが使用され、その結果、インデックス ドキュ メントには古いスキルセットと新しいスキルセットが混在します。\\n\\n増分キャッシュを使用する場合、スキルセットの更新後に、インデクサーはスキルセッ トの変更の影響を受けないエンリッチメントを再利用します。アップストリーム エン リッチメントは、変更されたスキルから独立して分離されたエンリッチメントと同様 に、キャッシュからプルされます。\\n\\n\\n## 削除\\n\\nインデクサーは、Azure Storage 内の構造とコンテンツを作成および更新しますが、そ れらを削除しません。インデクサーまたはスキルセットが削除された場合でも、プロ :unselected: ジェクションは引き続き存在します。ストレージ アカウントの所有者は、不要になっ たプロジェクションを削除する必要があります。\\n :unselected:\\n\\n## 次のステップ\\n\\nナレッジ ストアは、エンリッチメントされたドキュメントを永続化する手段として、 スキルセットを設計する際に役立つほか、Azure Storage アカウントにアクセスする機 能を備えた、あらゆるクライアント アプリケーションから利用する新しい構造やコン テンツを作成する際にも役立てることができます。\\n\\nエンリッチメントされたドキュメントを作成する最も簡単なアプローチは、ポータルを :unselected: 使用することですが、Postman と REST API を使用する方法もあります。プログラムで :unselected: オブジェクトがどのように作成され、参照されるのかについて分析情報が必要な場合に は、後者の方が便利です。\\n\\n\\n## Postman と REST を使用してナレッジ ストアを作成する\\n:selected: :unselected: :unselected:\\n# Azure Al Search でのデータインポート\\n\\n[アーティクル]·2024/01/19\\n\\nAzure Al Search では、クエリは、検索インデックスに読み込まれたユーザー所有のコ :unselected: ンテンツに対して実行されます。この記事では、インデックスを作成する 2 つの基本 的なワークフローについて説明します。プログラムでデータをインデックスにプッシュ :unselected: するワークフローと、検索インデクサーを使用してデータをプルするワークフローで す。\\n\\nどちらの方法でも、外部データ ソースからドキュメントが読み込まれます。空のイン デックスを作成することもできますが、コンテンツを追加するまでクエリは実行できま :unselected: せん。\\n\\n\\n## 1 注意\\n\\nAI エンリッチメントがソリューションの要件である場合は、プル モデル(インデ クサー) を使用してインデックスを読み込む必要があります。スキルセットはイン デクサーにアタッチされ、独立して実行されません。\\n\\n\\n### インデックスにデータをプッシュする\\n\\nプッシュ モデルは、API を使用して既存の検索インデックスにドキュメントをアップ ロードするアプローチです。ドキュメントは、1 バッチあたり最大1,000 個、または :unselected: バッチあたり 16 MB (メガバイト)(どちらか早い方) に個別に、またはバッチでアップ ロードできます。\\n :unselected:\\n主な利点:\\n\\n● データ ソースの種類に制限はありません。ペイロードは、インデックス スキー マにマップされる JSON ドキュメントで構成されている必要がありますが、デー タはどこからでもソース化できます。\\n\\n● 実行頻度に制限はありません。インデックスには、必要に応じて何度でも変更を プッシュすることができます。待機時間の要件が低いアプリケーションの場合 (たとえば、インデックスを製品在庫の変動と同期させる必要がある場合など)、プ ッシュ モデルが唯一のオプションです。\\n\\n● ドキュメントの接続性と安全な取得は、完全に制御下にあります。これに対し、 インデクサー接続は、Azure Al Search で提供されるセキュリティフィーチャーを 使用して認証されます。\\n:unselected: :unselected: :unselected: :unselected:\\n# Azure Al Search インデックスにデータをプッシュする方 法\\n\\n1 つまたは複数のドキュメントをインデックスに読み込むには、次の API を使用しま す。\\n\\n● ドキュメントの追加、更新、削除(REST API)\\n\\n● IndexDocumentsAsync (Azure SDK for .NET) または SearchIndexing BufferedSender\\n\\n● IndexDocumentsBatch (Azure SDK for Python) または SearchIndexingBufferedSender\\n\\n● IndexDocumentsBatch (Azure SDK for Java) または SearchIndexingBufferedSender\\n\\n● IndexDocumentsBatch (Azure SDK for JavaScript または SearchIndexingBufferedSender\\n\\nAzure portal を使用したデータのプッシュはサポートされていません。\\n\\nプッシュ API の概要については、以下を参照してください。\\n\\n● クイックスタート: Azure SDK を使用したフル テキスト検索\\n\\n● C# チュートリアル: プッシュ API を使用してインデックス作成を最適化する\\n\\n● REST クイック スタート: PowerShellを使用して Azure Al Search インデックスを作 成する\\n\\n\\n## インデックス作成アクション:upload、merge、 mergeOrUpload. delete\\n\\nインデックス作成アクションの種類をドキュメントごとに制御できます。つまり、ドキ ユメントを全部アップロードするか、既存のドキュメント コンテンツとマージする :unselected: か、または削除するかを指定できます。\\n\\nREST API と Azure SDK のどちらを使用する場合でも、データのインポートでは次のド キュメント操作がサポートされます。\\n\\n· ドキュメントが新しい場合は挿入され、存在する場合は更新または置き換えられ る “upsert\" と同様にアップロードします。インデックスに必要な値がドキュメン トにない場合、ドキュメント フィールドの値は null に設定されます。\\n\\n● マージでは、既に存在するドキュメントが更新され、見つからないドキュメント が失敗します。マージは既存の値を置き換えます。そのため、 Collection(Edm.String)型のフィールドなど、複数の値を含むコレクション フィ ールドは必ず確認してください。たとえば、 tags フィールドの値が [\"budget\"] で始まり、値 [\"economy\",\"pool\"] でマージを実行した場合、tags フィールドの\\n\\n最終値は [\"economy\",\"pool\"] になります。 [\"budget\", \"economy\", \"pool\"\\'] には なりません。\\n\\n● mergeOrUpload。ドキュメントが存在する場合は merge と同様な動作をし、ド キュメントが新しい場合は upload の動作をします。\\n\\n● delete。インデックスから指定したドキュメントを削除します。個々のフィール ドを削除する場合は、代わりに merge を使い、問題のフィールドを null に設定し ます。\\n\\n\\n## インデックスへのデータのプル\\n\\nプル モデルでは、サポートされているデータ ソースに接続するインデクサーが使用さ れ、データがインデックスに自動的にアップロードされます。 Microsoft のインデクサ :unselected: ーは、次のプラットフォームで利用できます。\\n\\n· Azure BLOB Storage\\n\\n· Azure Table Storage\\n\\n· Azure Data Lake Storage Gen2\\n\\n● Azure Files (プレビュー)\\n\\n· Azure Cosmos DB\\n\\n· Azure SQL Database, SQL Managed Instance, &JU Azure VM LO) SQL Server\\n\\n● Microsoft 365 での SharePoint (プレビュー)\\n\\nMicrosoft パートナーによって開発およびメインされたサード パーティ製コネクタを使 用できます。詳細とリンクについては、「データ ソースギャラリー」を参照してくだ さい。\\n\\nインデクサーは、インデックスをデータ ソース(通常はテーブル、ビュー、または同等 の構造体) に接続し、ソース フィールドをインデックスの同等のフィールドにマップし ます。実行中、行セットが自動的に JSON に変換され、指定したインデックスに読み 込まれます。すべてのインデクサーはスケジュールをサポートしているため、データ の更新頻度を指定できます。ほとんどのインデクサーは、変更の追跡を提供します(デ ータ ソースでサポートされている場合)。インデクサーは、新しいドキュメントを認識 するだけでなく、既存のドキュメントの変更と削除を追跡するため、インデックス内の データをアクティブに管理する必要がありません。\\n\\n\\n### Azure Al Search インデックスにデータをプルする方法\\n\\nインデクサーベースのインデックス作成には、次のツールと API を使用します。\\n\\n● Azure portal のデータのインポート ウィザード\\n\\n● REST API: インデクサーの作成(REST)、データソースの作成 (REST)、インデック スの作成(REST)\\n\\n. Azure SDK for .NET: SearchIndexer, SearchIndexerDataSourceConnection. SearchIndex,\\n\\n. Azure SDK for Python: SearchIndexer, SearchIndexerDataSourceConnection, SearchIndex,\\n\\n. Azure SDK for Java: SearchIndexer, SearchIndexerDataSourceConnection, SearchIndex.\\n\\n. Azure SDK for JavaScript: SearchIndexer, SearchIndexerDataSourceConnection, SearchIndex.\\n\\nインデクサー機能は、[Azure portal]、REST API、および .NET SDK で公開されていま す。\\n\\nポータルを使用する利点は、Azure Al Search では通常、ソース データセットのメタデ ータを読み取ることでデフォルトのインデックス スキーマを生成できることです。\\n\\n\\n### Search エクスプローラーを使用してデータのイ ンポートを検証する\\n\\nドキュメントのアップロード時に事前チェックを実行する簡単な方法は、ポータルで\\n :unselected:\\nSearch エクスプローラーを使用することです。\\n\\n<figure>\\n\\n![](figures/6)\\n\\n<!-- FigureContent=\"\\\\+ Add index V Import data & Import and vectorize data Search explorer Refresh Delete V Move\" -->\\n\\n</figure>\\n\\n\\nエクスプローラーを使用すると、コードを記述することなくインデックスを照会できま :unselected: す。検索エクスペリエンスは、既定の設定(単純構文、既定の searchMode クエリ パ ラメーターなど) に基づきます。結果は JSON で返されるため、ドキュメント全体を確 認できます。\\n\\nJSON ビューの検索エクスプローラーで実行できるクエリの例を次に示します。 \"Hotelld\"は hotels-sample-index のドキュメント キーです。フィルターには特定のド キュメントのドキュメント ID を指定します。\\n\\nJSON\\n\\n\"search\" :\\n\\n\"\\\\*\"\\n\\n9\\n\\n\"filter\": \"HotelId eq \\'50\\' \" }\\n\\n<!-- PageFooter=\"REST を使っている場合は、この参照クエリで同じ目的を達成できます。\" -->\\n:selected: :unselected: :unselected:\\n# 関連項目\\n\\n● インデクサーの概要\\n\\n● ポータルのクイックスタート: インデックスの作成、読み込み、クエリ\\n\\n# Azure Al Search のインデクサー\\n\\n[アーティクル]·2023/12/18\\n\\nAzure Al Search のインデクサーは、クラウド データ ソースからテキスト データを抽出 し、ソース データと検索インデックスの間のフィールド間マッピングを使用して検索 インデックスを設定するクローラーです。この方法は、インデックスにデータを追加 するコードを記述することなく、検索サービスがデータをプルするため、「プル モデ ル」と呼ばれることもあります。\\n\\nインデクサーはスキルセットの実行と AI エンリッチメントも推進 します。このエンリ ッチメントでは、インデックスにルーティングされるコンテンツの追加処理を統合する ようにスキルを構成できます。画像ファイルの OCR、データ チャンクのテキスト分割 スキル、複数の言語のテキスト翻訳など、いくつかの例があります。\\n\\nインデクサーは、サポートされているデータ ソースを対象とします。インデクサー構 成では、データ ソース(配信元) と検索インデックス(宛先) を指定します。Azure Blob Storage など、いくつかのソースには、そのコンテンツの種類に固有の追加の構成プロ :unselected: パティがあります。\\n\\nインデクサーは、オンデマンドで実行することも、5 分ごとに実行される定期的なデー 夕更新スケジュールで実行することもできます。より頻繁に更新するには、Azure Al Search と外部データ ソースの両方のデータを同時に更新する\"プッシュ モデル\"が必 要です。\\n\\n検索サービスは、検索ユニットごとに1 つのインデクサー ジョブを実行します。同時 処理が必要な場合は、十分なレプリカがあることを確認してください。インデクサー はバックグラウンドで実行されないため、サービスに負荷がかかっている場合は、通常 よりも多くのクエリ調整が検出される可能性があります。\\n\\n\\n## インデクサーのシナリオとユース ケース\\n\\nインデクサーは、データ インジェストの唯一の手段として、または他の手法と組み合 わせて使用できます。次の表に主なシナリオをまとめています。\\n\\n〔〕 テーブルを展開する\\n\\n\\n## シナ 戦略 リオ\\n\\n単一 このパターンは最も単純です。1 つのデータ ソースが検索インデックス用の唯一のコン のデ テンツ プロバイダーです。サポートされているほとんどのデータソースで何らかの形式 :unselected: ータ\\n\\n|| シナ 戦略 | リオ \\n| - | - |\\n|| ソー の変更が検出されるため、ソースでコンテンツが追加または更新されたときに、後続の ス  インデクサーの実行によって差分が取得されます。 |\\n| 複数 のデ ータ ソー ス | インデクサーの指定で使用できるデータソースは 1 つだけですが、検索インデックス自 体は複数のソースからのコンテンツを受け入れることができます。この場合、各インデ クサーの実行によって、別のデータ プロバイダーから新しいコンテンツが取り込まれま す。各ソースは、完全なドキュメントの共有を投稿したり、各ドキュメントで選択した フィールドを設定したりできます。このシナリオの詳細については、複数のデータ ソー スからのインデックスのチュートリアルを参照してください。 |\\n| 複数 のイ ンデ クサ | 実行時のパラメーター、スケジュール、またはフィールド マッピングを変更する必要が ある場合、複数のデータ ソースは通常、複数のインデクサーとペアになります。 |\\n|| Azure Al Search のクロスリージョンのスケールアウトがもう 1 つのシナリオです。同じ 検索インデックスのコピーが異なるリージョンに存在する場合があります。検索インデ ックスのコンテンツを同期するには、同じデータ ソースからプルする複数のインデクサ ーを作成できます。この場合、各インデクサーのターゲットはリージョンごとに異なる 検索インデックスです。非常に大きなデータセットの |\\n|| 並列インデックスでも、各インデクサーがデータのサブセットをターゲットとするマル チインデクサー戦略が必要です。 |\\n| コン テン ツの 変換 | インデクサーは、スキルセットの実行と AI エンリッチメントを推進 します。コンテン :unselected: ツの変換は、インデクサーにアタッチするスキルセットで定義されます。スキルを使用 して、データ チャンクとベクター化を組み込むことができます。 |\\n\\nターゲット インデックスとデータ ソースの組み合わせごとにインデクサーを 1 つ作成 するように設計する必要があります。複数のインデクサーが同じインデックスに書き 込みできます。複数のインデクサーに同じデータ ソースを再利用できます。ただし、 インデクサーが 1回に利用できるデータ ソースは 1 つだけです。そして、書き込める インデックスは 1 つだけです。次の図に示すように、1 つのデータ ソースが1 つのイ ンデクサーに入力を提供し、1 つのインデックスを設定します:\\n\\n<figure>\\n\\n![](figures/7)\\n\\n<!-- FigureContent=\"DATA SOURCE INDEXER TARGET INDEX\" -->\\n\\n</figure>\\n\\n\\n一度に使用できるインデクサーは1 つだけですが、リソースはさまざまな組み合わせ で使用できます。次の図の主なポイントは、データ ソースを複数のインデクサーと組 み合わせることができ、複数のインデクサーが同じインデックスに書き込むことができ る点です。\\n<figure>\\n\\n![](figures/8)\\n\\n<!-- FigureContent=\"DATA SOURCE 1 INDEXER DS1 A INDEX.A DATA SOURCE 2 INDEXER\\\\_DS2\\\\_A INDEXER DS2 B INDEX B\" -->\\n\\n</figure>\\n\\n\\n# サポートされるデータ ソース\\n\\nインデクサーは、Azure および Azure 外部でデータ ストアをクロールします。\\n :unselected:\\n· Azure Blob Storage\\n\\n· Azure Cosmos DB\\n\\n· Azure Data Lake Storage Gen2\\n\\n· Azure SQL Database\\n\\n· Azure Table Storage\\n\\n· Azure SQL Managed Instance\\n\\n· Azure Virtual Machines (+3 SQL Server\\n\\n● Azure Files (プレビュー段階)\\n\\n● Azure MySQL (プレビュー段階)\\n\\n● Microsoft 365 での SharePoint (プレビュー段階)\\n\\n● Azure Cosmos DB for MongoDB (プレビュー段階)\\n\\n● Azure Cosmos DB for Apache Gremlin (プレビュー段階)\\n\\nAzure Cosmos DB for Cassandra はサポートされていません。\\n\\nインデクサーは、テーブルやビューなどのフラット化された行セット、またはコンテナ ーまたはフォルダー内の項目を受け入れます。ほとんどの場合、行、レコード、また は項目ごとに1 つの検索ドキュメントが作成されます。\\n\\n共有プライベート リンクを使用する場合、リモート データ ソースへのインデクサー接 続は、標準のインターネット接続(パブリック) または暗号化されたプライベート接続 を使用して行うことができます。また、マネージド ID を使用して認証を行うように、 接続を設定することもできます。セキュリティで保護された接続の詳細については、 Azure ネットワーク セキュリティ機能によって保護されたコンテンツへのインデクサ ーのアクセスと、マネージド ID を使用したデータ ソースへの接続に関するページを参 照してください。\\n\\n# インデックス作成のステージ\\n\\n最初の実行時に、インデックスが空の場合、テーブルまたはコンテナーで提供されるす べてのデータがインデクサーによって読み取られます。その後の実行では、通常、変 更されたデータのみがインデクサーによって検出され取得されます。BLOB データの場 合、変更の検出は自動で行われます。Azure SQL や Azure Cosmos DB などの他のデー タ ソースで、変更の検出を有効にする必要があります。\\n\\n受信したドキュメントごとに、インデクサーによって、ドキュメントの取得からインデ ックス付けのための最終的な検索エンジンの“ハンドオフ\"までの、複数のステップが 実装または調整されます。また、インデクサーを使用すると、スキルセットが定義さ れている場合に、スキルセットの実行と出力も促進されます。\\n\\n<figure>\\n\\n![](figures/9)\\n\\n<!-- FigureContent=\"Document Cracking Field Mappings Skillset Execution Output Field Mappings Push into Index\" -->\\n\\n</figure>\\n\\n\\n\\n## ステージ 1: ドキュメント解析\\n\\nドキュメント解析は、ファイルを開いてコンテンツを抽出するプロセスです。テキス :unselected: トベースのコンテンツは、サービスのファイル、テーブルの行、またはコンテナーや コレクションの項目から抽出できます。スキルセットと画像スキルを追加した場合、 ドキュメント解析で画像を抽出し、画像処理のためにキューに登録することもできま す。\\n\\nデータ ソースに応じて、インデックス付けが可能なコンテンツを抽出するために、イ ンデクサーによってさまざまな操作が試行されます。\\n\\n● ドキュメントが PDF などの画像が埋め込まれたファイルである場合、インデクサ ーはテキスト、画像、メタデータを抽出します。インデクサーは、Azure Blob Storage、Azure Data Lake Storage Gen2、SharePoint からファイルを開くことが できます。\\n\\n● ドキュメントが Azure SQL のレコードの場合は、インデクサーによって各レコー ドの各フィールドからバイナリ以外のコンテンツが抽出されます。\\n\\n● ドキュメントが Azure Cosmos DB 内のレコードの場合は、インデクサーによって Azure Cosmos DB ドキュメントのフィールドとサブフィールドからバイナリ以外 のコンテンツが抽出されます。\\n\\nステージ 2: フィールド マッピング\\n\\nインデクサーによって、ソース フィールドからテキストが抽出され、インデックスま たはナレッジ ストアの送信先フィールドにそれが送信されます。フィールド名とデー 夕型が一致すると、パスは明確になります。ただし、出力には異なる名前または型が 必要な場合があります。その場合は、フィールドをマップする方法をインデクサーに指 示する必要があります。\\n\\nフィールド マッピングを指定するには、インデクサー定義に、ソース フィールドと宛 先フィールドを入力します。\\n\\nフィールド マッピングは、ドキュメント解析の後、変換前に、インデクサーがソース ドキュメントから読み取るときに行われます。フィールド マッピングを定義するとき に、ソース フィールドの値は変更されずにそのまま送信先フィールドに送信されま す。\\n\\n\\n## ステージ 3: スキルセットの実行\\n\\nスキルセットの実行は、組み込みまたはカスタムの AI 処理を呼び出す省略可能なステ ップです。スキルセットでは、コンテンツがバイナリの場合、光学式文字認識 (OCR) またはその他の形式の画像分析を追加できます。スキルセットで自然言語処理を追加 することもできます。たとえば、テキスト翻訳やキー フレーズ抽出を追加できます。\\n\\n変換が何であれ、スキルセットの実行は、エンリッチメントが発生する場所です。イ ンデクサーがパイプラインの場合、スキルセットを“パイプライン内のパイプライン\" として考えることができます。\\n\\n\\n### ステージ 4: 出カフィールドマッピング\\n\\nスキルセットを含める場合は、インデクサー定義で出力フィールド マッピングを指定 する必要があります。スキルセットの出力は、エンリッチされたドキュメントと呼ば れるツリー構造として内部的に示されます。出力フィールド マッピングを使用する と、このツリーの部分を選択してインデックス内のフィールドにマップすることができ ます。\\n\\n名前が類似しているにもかかわらず、出力フィールド マッピングとフィールド マッピ ングは、異なるソースから関連付けを構築します。フィールド マッピングでは、ソー ス フィールドの内容を検索インデックスの宛先フィールドに関連付けます。出力フィ ールド マッピングでは、内部のエンリッチされたドキュメント(スキル出力) の内容を インデックス内の宛先フィールドに関連付けます。省略可能と見なされるフィールド マッピングとは異なり、インデックスに存在する必要がある変換されたすべてのコンテ ンツには、出力フィールド マッピングが必要になります。\\n\\n次の図は、インデクサーのステージ(ドキュメント解析、フィールド マッピング、スキ ルセットの実行、出力フィールド マッピング) のサンプル インデクサーのデバッグ セ ッション表現を示しています。\\n\\n<figure>\\n\\n![](figures/10)\\n\\n<!-- FigureContent=\"document Field Mappings Image Analysis #5 BrandDetector 第4 L 2.44s VISION VISION Merga #4 Language Detection #3 TEXT Entity Recognition Key Phrase Extraction #1 #2 D:90ms TENT TEXT Output Field Mappings\" -->\\n\\n</figure>\\n\\n\\n\\\\+\\n\\n\\n## の基本的なワークフロー\\n\\nインデクサーで実行できる機能は、データ ソースごとに異なります。そのためインデ クサーやデータ ソースの構成には、インデクサーの種類ごとに異なる点があります。 しかし基本的な成り立ちと要件は、すべてのインデクサーに共通です。以降、すべて のインデクサーに共通の手順について取り上げます。\\n\\n# 手順1:データ ソースを作成する\\n\\nインデクサーには、接続文字列と必要に応じて資格情報を提供する、\"データ ソース\" オブジェクトが必要です。データ ソースは独立したオブジェクトです。複数のインデ クサーは、同じデータ ソース オブジェクトを使用して、一度に複数のインデックスを 読み込むことができます。\\n\\n次のいずれかの方法を使用してデータ ソースを作成できます。\\n\\n● Azure portal を使用し、検索サービス ページの[データ ソース]タブで[データ ソ ースの追加] を選択して、データ ソースの定義を指定します。\\n\\n● Azure portal を使用して、[データのインポート]ウィザードでデータ ソースを出 力します。\\n\\n● REST API を使用して、[データ ソースの作成] を呼び出します。\\n\\n● Azure SDK for .NET を使用して、SearchIndexerDataSourceConnection クラスを呼 び出します\\n\\n\\n## 手順 2:インデックスを作成する\\n\\nインデクサーは、データ インジェストに関連したいくつかのタスクを自動化しますが 通常、そこにはインデックスの作成は含まれていません。前提条件として、お使いの 外部データ ソース内のすべてのソース フィールドの対応するターゲット フィールドが 含まれた定義済みインデックスが必要になります。各フィールドでは、名前とデータ 型が一致する必要があります。そうでない場合は、フィールド マッピングを定義して 関連付けを確立できます。\\n\\n詳細については、「インデックスの作成」を参照してください。\\n\\n\\n## 手順 3: インデクサーを作成して実行(またはスケジュー ル)する\\n\\nインデクサーの定義は、インデクサーを一意に識別するプロパティ、使用するデータ ソースとインデックスを指定するプロパティ、実行時の動作に影響する他の構成オプシ :unselected: ョンを提供するプロパティ(インデクサーをオンデマンドで実行するか、スケジュール に基づき実行するかなど) で構成されます。\\n\\nデータ アクセスまたはスキルセットの検証に関するエラーまたは警告は、インデクサ ーの実行中に発生します。インデクサーの実行が開始されるまで、データ ソース、イ ンデックス、スキルセットなどの依存オブジェクトは、検索サービスでパッシブになり ます。\\n\\n詳細については、「インデクサーの作成」を参照してください\\n\\nインデクサーの初回実行の後、オンデマンドで再実行することや、スケジュールを設定 することができます。\\n\\nインデクサーの状態は、ポータルか Get Indexer Status API を使用して監視できます。 また、インデックスのクエリを実行し、期待した結果が得られるかどうかを確認する必 要もあります。\\n\\nインデクサーには専用の処理リソースがありません。これに基づき、インデクサーの 状態が(キュー内の他のジョブに応じて) 実行される前にアイドル状態として表示さ れ、実行時間が予測できない場合があります。インデクサーのパフォーマンスは、ド キュメント サイズ、ドキュメントの複雑さ、画像分析などのその他の要因によっても 定義されます。\\n\\n\\n## 次のステップ\\n\\nインデクサーの概要がわかったので、次のステップは、インデクサーのプロパティとパ :unselected: ラメーター、スケジュール、およびインデクサーの監視について確認することです。 また、サポートされているデータ ソースの一覧に戻り、特定のソースの詳細を確認す ることもできます。\\n\\n· インデクサーを作成する\\n\\n● インデクサーのリセットと実行\\n\\n● インデクサーをスケジュールする\\n\\n● フィールド マッピングを定義する\\n\\n· インデクサーの状態を監視する\\n\\n# Azure Al Search での AI エンリッチメン\\n\\n# ト\\n\\n[アーティクル]·2024/01/30\\n\\nAzure Al Search では、AI エンリッチメントとは、未加工の形式で検索できないコンテ ンツを処理するための Azure Al サービスとの統合を指します。エンリッチメントによ り、分析と推論を使用して、以前は存在しなかった検索可能なコンテンツや構造を作成 します。\\n\\nAzure Al Search はテキストおよびベクター検索ソリューションであるため、AI エンリ ッチメントの目的は、検索関連のシナリオでコンテンツの有用性を向上することです。 ソース コンテンツはテキスト形式である必要がありますが(ベクターをエンリッチする ことはできません)、エンリッチメント パイプラインによって作成されたコンテンツ は、チャンク用の Text Split スキルやエンコード用の AzureOpenAiEmbedding スキルな どのスキルを使用してベクター ストアでベクター化およびインデックス付けできま す。\\n\\n組み込みのスキルは、生のコンテンツに次の変換と処理を適用します。\\n\\n● 多言語検索の場合の翻訳と言語検出\\n\\n● テキストの大きなチャンクからユーザー名、場所、およびその他のエンティティ を抽出するエンティティ認識\\n\\n● 重要な用語を識別して出力するためのキー フレーズ抽出\\n\\n● バイナリ ファイル内の印刷されたテキストと手書きのテキストを認識する光学式 文字認識(OCR)\\n\\n● 画像の内容を説明し、説明を検索可能なテキスト フィールドとして出力する画像 分析\\n\\nAI エンリッチメントは、Azure データ ソースに接続するインデクサー パイプラインの 拡張機能です。エンリッチメント パイプラインには、インデクサー パイプラインのす べてのコンポーネント(インデクサー、データ ソース、インデックス) に加えて、アト ミック エンリッチメント ステップを指定するスキルセットが含まれます。\\n\\n次の図に、AI エンリッチメントの進行を示します。\\n<figure>\\n\\n![](figures/11)\\n\\n<!-- FigureContent=\"Field mappings Output field mappings Search index Source data Document cracking Skillset (built-in or custom skills) Enriched document Projections Knowledge store Enrichment cache Import Enrich & Index Explore\" -->\\n\\n</figure>\\n\\n\\nインポートは最初の手順です。ここで、インデクサーがデータ ソースに接続し、コン テンツ(ドキュメント)を検索サービスにプルします。AI エンリッチメント シナリオで 使用される最も一般的なリソースは Azure Blob Storage ですが、サポートされている 任意のデータ ソースがコンテンツを提供できます。\\n\\nエンリッチ&インデックス は、ほとんどの AI エンリッチメント パイプラインを対象と します。\\n\\n● エンリッチメントは、インデクサーが“ドキュメントを解読\"し、画像とテキスト を抽出したときに開始されます。次に発生する処理の種類は、データと、スキル セットに追加したスキルによって異なります。画像がある場合は、画像処理を実 行するスキルに転送できます。テキスト コンテンツは、テキストと自然言語の処 理のためにキューに入れられます。内部的には、スキルによって、変換が発生し たときにそれを収集する“エンリッチされたドキュメント\"が作成されます。\\n\\n● エンリッチ済みコンテンツは、スキルセットの実行の間に生成され、ユーザーが 保存しない限り一時的なものです。後でスキルセットを実行するときに再利用す るため、エンリッチメントキャッシュで解読されたドキュメントとスキル出力を 保持できます。\\n\\n● 検索インデックスにコンテンツを取り込むため、インデクサーはエンリッチされ たコンテンツをターゲット フィールドに送信するためのマッピング情報を保持し ている必要があります。フィールド マッピング(明示的または暗黙的) は、ソー ス データから検索インデックスへのデータ パスを設定します。出力フィールド マッピングは、エンリッチされたドキュメントからインデックスへのデータ パス を設定します。\\n\\n● インデックス作成は、生のコンテンツとエンリッチされたコンテンツが検索イン デックス(そのファイルとフォルダー)の物理データ構造に取り込まれるプロセス :unselected: です。このステップで、字句解析とトークン化が行われます。\\n\\n探索は最後の手順です。 出力は常に、クライアント アプリからクエリを実行できる検 索インデックスです。必要に応じて出力を、データ探索ツールまたはダウンストリー\\n:unselected:\\nム プロセスを介してアクセスされる Azure Storage 内の BLOB とテーブルで構成される ナレッジ ストアにすることができます。ナレッジ ストアを作成している場合は、エン リッチされたコンテンツのデータ パスがプロジェクションによって決定されます。イ ンデックスとナレッジ ストアの両方に、同じエンリッチされたコンテンツを含めるこ とができます。\\n\\n\\n## どのような場合に AI エンリッチメントを使用す るか\\n\\n生コンテンツが、非構造化テキスト、画像コンテンツ、または言語検出と翻訳を必要と するコンテンツの場合は、エンリッチメントが有用です。“組み込みのコグニティブ ス キル\"を通じて AI を適用すると、フルテキスト検索とデータ サイエンス アプリケーシ ョンに対してこのコンテンツのロックが解除されます。\\n :unselected:\\nカスタム スキルを作成して、外部処理を提供することもできます。オープンソース、 サードパーティ、またはファーストパーティのコードは、カスタム スキルとしてパイ プラインに統合できます。さまざまなドキュメントの種類の顕著な特徴を識別する分 類モデルはこのカテゴリに分類されますが、コンテンツの価値を高める任意の外部パッ :unselected: ケージを使用できます。\\n\\n\\n### 組み込みスキルのユース ケース\\n\\n組み込みのスキルは、Azure Al Computer Vision と Language Service などのAzure AI サ ービス API に基づいています。コンテンツの入力が少ない場合を除いて、より大きな ワークロードを実行するために、課金対象の Azure AI サービス リソースをアタッチす る必要があります。\\n\\n組み込みのスキルを使用して作られたスキルセットは、次のアプリケーション シナリ オに適しています。\\n\\n● 画像処理スキルには、光学式文字認識 (OCR) と、視覚的特徴の識別が含まれま す。後者は、顔検出、画像の解釈、画像の認識(有名な人物やランドマーク)、画 像の向きのような属性などの識別です。これらのスキルにより、Azure Al Search でフルテキスト検索用の画像コンテンツのテキスト表現が作成されます。\\n\\n● 機械翻訳 は、多くの場合、多言語ソリューション用の 言語検出 と組み合わせ て、テキスト翻訳 スキルによって提供されます。\\n\\n● 自然言語処理では、テキストのチャンクが分析されます。このカテゴリのスキル には、エンティティ認識、センチメント検出(オピニオン マイニングを含む)、個 人を特定できる情報の検出などがあります。これらのスキルによって、構造化さ\\n:unselected:\\n<!-- PageHeader=\"れていないテキストが、インデックスで検索可能およびフィルター可能なフィー ルドとしてマップされます。\" -->\\n\\n\\n#### カスタム スキルのユース ケース\\n\\nカスタム スキルは、指定した外部コードを実行し、カスタム スキル Web インターフ エイスでラップします。カスタム スキルのいくつかの例は、azure-search-power- skills ☑ の GitHub リポジトリにあります。\\n\\nカスタム スキルは常に複雑とは限りません。たとえば、パターン マッチングまたはド キュメント分類モデルを提供する既存のパッケージがある場合は、カスタム スキルで 完成させることができます。\\n\\n\\n#### 出力を格納する\\n\\nAzure Al Search では、インデクサーによって作成された出力が保存されます。1回の インデクサー実行で、エンリッチされてインデックス付けされた出力を含む最大3つ のデータ構造を作成できます。\\n\\n〔〕 テーブルを展開する\\n\\n|| データ 必 ストア 須  | 場所 | 説明 |\\n| - | - | - | - |\\n| 検索可 | 必 | 検索サ | フルテキスト検索やその他のクエリ フォームに使用されます。イン |\\n| 能なイ ンデッ クス | 須 | ービス | デックスの指定はインデクサーの要件です。インデックスの内容は、 スキルの出力に加えて、インデックス内のフィールドに直接マップさ れるすべてのソース フィールドから設定されます。 |\\n| ナレッ | オ | Azure | ナレッジ マイニングやデータ サイエンスなどのダウンストリーム ア |\\n| ジス | プ | Storage | プリに使用されます。ナレッジ ストアは、スキルセット内で定義さ |\\n| トア |\\n||\\n|| シ ョン   | | れています。その定義により、エンリッチされたドキュメントが || | Azure Storage でテーブルとオブジェクト(ファイルと BLOB) のどちら || | として投影されるかが決まります。 |\\n| エンリ | オ | Azure | 後続のスキルセットの実行で再利用するためのエンリッチメントのキ |\\n| ッチメ | プ | Storage | ャッシュに使用されます。キャッシュには、インポートされた未処理 |\\n| ント | シ | | のコンテンツ(解読されたドキュメント) が格納されます。スキルセ |\\n| キャッ | ヨ | | ットの実行中に作成されたエンリッチされたドキュメントも格納され |\\n| シュ: | ン | | ます。キャッシュは、画像解析または OCR を使用していて、画像フ アイルの再処理にかかる時間と費用を回避したい場合に役立ちます。 |\\n\\nインデックスとナレッジ ストアは相互に完全に独立しています。インデクサーの要件 を満たすにはインデックスをアタッチする必要がありますが、ナレッジ ストアだけが 目的の場合は、作成された後でインデックスを無視してかまいません。\\n\\n## コンテンツを探索する\\n\\n検索インデックスまたはナレッジ ストアを定義して読み込んだら、そのデータを探索 できます。\\n\\n\\n## 検索インデックスに対してクエリを実行する\\n\\nクエリを実行して、パイプラインによって生成されたエンリッチされたコンテンツにア クセスします。インデックスは、Azure Al Search 用に作成する他のインデックスと同 様です。カスタム アナライザーを使用してテキスト解析を補完したり、あいまい検索 クエリを呼び出したり、フィルターを追加したり、スコアリング プロファイルを実験 して検索の関連性を調整したりできます。\\n\\n\\n### ナレッジ ストアに対してデータ探索ツールを使用する\\n\\nこの Azure Storage ナレッジ ストア では、JSON ドキュメントの BLOB コンテナー、イ メージ オブジェクトの BLOB コンテナー、または Table Storage のテーブルの形式を想 :unselected: 定できます。 Storage Explorer、Power BI、または Azure Storage に接続する任意のア プリを使用して、コンテンツにアクセスできます。\\n\\n● BLOB コンテナーは、エンリッチされたドキュメント全体をキャプチャします。こ れは、他のプロセスにフィードを作成する場合に便利です。\\n :unselected:\\n● テーブルは、エンリッチされたドキュメントのスライスが必要な場合、または出 力の特定の部分を含めるか除外する場合に便利です。Power BI での分析には、表 がデータの探索や可視化に推奨されるデータソースです。\\n\\n\\n## 可用性と料金\\n\\nエンリッチメントは、Azure AI サービスが利用できるリージョンで利用できます。工 ンリッチメントをご利用いただけるかどうかは、「リージョン別の利用可能な Azure 製品☑」ページでご確認いただけます。\\n\\n課金は、従量課金制モデルに従います。スキルセットで複数リージョンの Azure Al サ ービス キーが指定されている場合、組み込みスキルを使用するためのコストが転嫁さ れます。Azure Al Search によって測定される画像抽出に関連するコストもあります。 ただし、テキスト抽出とユーティリティのスキルは請求されません。詳細について は、「 Azure Al Searchの課金方法」を参照してください。\\n\\n\\n## 一般的なワークフローのチェックリスト\\n\\nエンリッチメント パイプラインは、\"スキルセット\"を含む\"インデクサー\"で構成され ます。インデックス作成の後で、インデックスのクエリを実行して結果を検証できま す。\\n\\nサポートされるデータ ソース内のデータのサブセットから始めます。インデクサーと スキルセットの設計は、反復的なプロセスです。代表的なデータ セットが小さいと作 :unselected: 業が速くなります。\\n\\n1\\\\. データへの接続が指定されているデータ ソースを作成します。\\n\\n2\\\\. スキルセットを作成します。プロジェクトが小さい場合を除き、Azure Al マルチ サービス リソースをアタッチする必要があります。ナレッジ ストアを作成する 場合は、スキルセット内でそれを定義します。\\n\\n3\\\\. 検索インデックスを定義するインデックス スキーマを作成します。\\n\\n4\\\\. インデクサーを作成して実行し、上記のすべてのコンポーネントをまとめます。 この手順では、データの取得、スキルセットの実行、インデックスの読み込みを 行います。\\n\\nインデクサーでは、検索インデックスへのデータ パスを設定するフィールド マッ ピングと出力フィールド マッピングも指定します。\\n\\n必要に応じて、インデクサーの構成でエンリッチメント キャッシュを有効にしま す。この手順により、後で既存のエンリッチメントを再利用できるようになりま す。\\n\\n5\\\\. クエリを実行して結果を評価するか、デバッグ セッションを開始してスキルセッ トの問題を解決します。\\n\\n上記の手順を繰り返す場合は、インデクサーを実行する前に リセット します。また は、実行ごとにオブジェクトを削除して再作成します(Free レベルを使用している場合 は推奨)。インデクサーのキャッシュを有効にした場合、ソースでデータが変更されて いない場合、およびパイプラインに対する編集によってキャッシュが無効にされない場 合は、インデクサーがキャッシュからプルされます。\\n\\n\\n## 次のステップ\\n\\n● クイックスタート: AI エンリッチメントのスキルセットを作成する\\n\\n● チュートリアル: AI エンリッチメント REST API について学習する\\n\\n● スキルセットの概念\\n\\n● ナレッジ ストアの概念\\n\\n● スキルセットを作成する\\n\\n● ナレッジ ストアを作成する\\n:unselected:\\n# Azure Al Search での増分 エンリッチメ ントとキャッシュ\\n\\n[アーティクル]·2024/02/18\\n\\n\\n## 1 重要\\n\\nこの機能はパブリックプレビュー段階にあり、追加使用条件での下で提供されま す。この機能は、プレビュー REST API でサポートされます。\\n\\n\"インクリメンタル エンリッチメント”とは、スキルセットの実行中にキャッシュされ たエンリッチメントを使うことを指します。こうすることで、新規および変更されたス キルとドキュメントにのみ、Azure Al サービスへの API 呼び出しの従量課金処理料金 が発生します。キャッシュには、ドキュメント解析の出力に加え、ドキュメントごと の各スキルの出力が格納されます。キャッシュは課金対象(Azure Storage を使用)で す。ただし、ストレージのコストは画像抽出や AI 処理よりも低いため、強化全体コス トは削減されます。\\n\\nキャッシュを有効にすると、インデクサーによって更新内容が評価され、既にあるエン リッチメントをキャッシュから取得できるかどうかが判断されます。ドキュメント解 析フェーズから出力される画像やテキスト コンテンツに加え、編集の上流のスキル出 カや、編集に対して直交するスキル出力は再利用できる可能性が高くなります。\\n\\nスキルセットの処理が完了した後、更新された結果は、キャッシュだけでなく、検索イ ンデックスまたはナレッジ ストアにも書き戻されます。\\n\\n\\n## 制限事項\\n :selected:\\n× 注意事項\\n\\nSharePoint Online インデクサー(プレビュー) を使用している場合は、増分エンリ ッチメントを避ける必要があります。特定の状況では、キャッシュが無効にな り、キャッシュを再ロードする場合は、インデクサーをリセットして実行する必 :unselected: 要があります。\\n\\nキャッシュの構成\\n\\n物理的には、ご使用の Azure ストレージ アカウントの BLOB コンテナーに、インデク :unselected: サーごとに1つキャッシュが格納されます。それぞれのインデクサーには、使ってい るコンテナーに対応する一意で不変のキャッシュ識別子が割り当てられます。\\n\\nキャッシュは、\"cache\"プロパティを指定してインデクサーを実行すると作成されま :unselected: す。 キャッシュできるのは、エンリッチされたコンテンツのみです。インデクサーに スキルセットがアタッチされていない場合、キャッシュは適用されません。\\n\\n次の例は、キャッシュが有効になっているインデクサーを示しています。詳細な手順 については、エンリッチメントキャッシュの有効化に関するページを参照してくださ い。 cache プロパティを追加するときの要求では、プレビュー API バージョン (2020- :unselected: 06-30-Preview 以降) を使うことに注意してください。\\n\\n<figure>\\n\\n![](figures/12)\\n\\n<!-- FigureContent=\"JSON POST https://[search service name].search.windows.net/indexers?api- version=2020-06-30-Preview { \"name\": \"myIndexerName\", \"targetIndexName\": \"myIndex\", \"dataSourceName\" : \"myDatasource\", \"skillsetName\": \"mySkillset\", \"cache\" : { \"storageConnectionString\" : \"<Your storage account connection string>\", \"enableReprocessing\": true }, \"fieldMappings\" : [], \"outputFieldMappings\": [], \"parameters\": [] }\" -->\\n\\n</figure>\\n\\n\\n\\n## キャッシュ管理\\n\\nキャッシュのライフサイクルは、インデクサーによって管理されます。インデクサー が削除されると、そのキャッシュも削除されます。インデクサーの cache プロパティ :unselected: が null に設定されるか、接続文字列が変更された場合、既存のキャッシュはインデク サーの次回の実行時に削除されます。\\n\\nインクリメンタル エンリッチメントはユーザーの介入なしに変更を検出して対応する ように設計されています。その一方で、特定の動作を呼び出すために使用できるパラメ ーターが用意されています。\\n\\n● 新しいドキュメントを優先する\\n\\n● スキルセット チェックをバイパスする\\n\\n● データ ソース チェックをバイパスする\\n\\n● スキルセットの評価を強制的に実行する\\n\\n\\n## 新しいドキュメントを優先する\\n\\ncache プロパティには、enableReprocessing パラメータが含まれます。キャッシュ内 :unselected: で既に表現されている受信ドキュメントの処理を制御する目的で使用されます。 true (既定値) の場合、インデクサーを再実行すると、キャッシュ内に既にあるドキュメント が再処理されます。これは、スキルの更新によってそのドキュメントが影響を受けると 仮定されるためです。\\n\\nfalse の場合、既存のドキュメントは再処理されず、実質的に新しい受信コンテンツが 既存のコンテンツより優先されます。 enableReprocessing を false に設定するのは、あ くまで一時的な措置としてください。ほとんどの場合は enableReprocessing を true に 設定しておくと、新規と既存両方のすべてのドキュメントが、最新のスキルセット定義 に従って確実に有効になります。\\n\\n\\n## スキルセットの評価をバイパスする\\n\\n通常、スキルの変更とそのスキルの再処理は連動します。ただし、スキルを変更して も再処理が行われてはならない場合がいくつかあります(たとえば、新しい場所に、ま たは新しいアクセス キーを使って、カスタム スキルをデプロイする場合)。ほとんど :unselected: の場合、これらは、スキル出力の実体そのものに実際の影響を与えない末梢的な変更で す。\\n\\nスキルに対する変更が実際は表面的なものであることがわかっている場合は、\\n\\ndisableCacheReprocessingChangeDetection パラメータを true に設定して、スキルの評 価をオーバーライドする必要があります。\\n\\n1\\\\. スキルセットの更新を呼び出し、スキルセット定義を変更します。\\n\\n2\\\\. 要求に \"disableCacheReprocessingChangeDetection=true\" パラメーターを追加し ます。\\n\\n3\\\\. 変更を送信します。\\n\\nこのパラメーターを設定すると、スキルセット定義に対する更新だけがコミットされ、 変更が既存のキャッシュに及ぼす影響は評価されません。プレビューの API バージョ ンである 2020-06-30-Preview 以降を使用してください。\\n\\nHTTP\\n\\nPUT https://[servicename].search.windows.net/skillsets/[skillset\\\\_name]?api- version=2020-06-30-Preview&disableCacheReprocessingChangeDetection\\n\\n# データ ソースの検証チェックをバイパスする\\n\\nほとんどの場合、データ ソース定義を変更すると、キャッシュが無効になります。た だし、接続文字列の変更やストレージ アカウントのキーのローテーションなど、変更 :unselected: によってキャッシュが無効になってはならないことがわかっているシナリオでは、デー タ ソースの更新で ignoreResetRequirement パラメータを追加します。このパラメータ ーを true に設定すると、リセット条件(結果的にすべてのオブジェクトが最初から再構 築されて設定される条件) をトリガーすることなく、コミットを実行できます。\\n\\nHTTP\\n\\nPUT https://[search service].search.windows.net/datasources/[data source name]?api-version=2020-06-30-Preview&ignoreResetRequirement\\n\\n\\n## スキルセットの評価を強制的に実行する\\n\\nキャッシュの目的は不必要な処理を回避することにあります。ここで、インデクサーに よって検出されない変更(たとえば、カスタム スキルなどの外部コード内の変更) をス キルに加えるケースを考えてみましょう。\\n\\nこの場合は、スキルのリセットを使用して、特定のスキル(そのスキルの出力に依存す るダウンストリームのスキルも含まれます) を強制的に再処理することができます。こ の API は、無効にして再処理用にマークする必要があるスキルのリストが含まれた POST 要求を受け取ります。スキルのリセット後、インデクサー実行要求を行ってパイ プライン処理を呼び出します。\\n\\n\\n## 特定のドキュメントを再キャッシュする\\n\\nインデクサーのリセットを使用すると、検索コーパス内のすべてのドキュメントが再処 理されます。再処理を必要とするドキュメントがごく少数のシナリオでは、ドキュメ ントのリセット(プレビュー)を使用して、特定のドキュメントを強制的に再処理しま す。ドキュメントがリセットされると、インデクサーによってそのドキュメントのキ ャッシュが無効にされ、ドキュメントはその後データ ソースから読み取ることによっ て再処理されます。詳細については、インデクサー、スキル、ドキュメントの実行ま たはリセットに関するページを参照してください。\\n\\n特定のドキュメントをリセットする場合は、検索インデックスから読み取るドキュメン トキーのリストを要求で指定します。キーが外部データソースのフィールドにマップ\\n\\nされている場合、指定する値は検索インデックスで使用されているものであることが必 要です。\\n\\nAPI の呼び出し方法に応じて、要求ではキー リストを追加、上書き、またはキューに 登録します。\\n\\n● 異なるキーを使用して API を複数回呼び出すと、ドキュメント キーのリセットの 一覧に新しいキーが追加されます。\\n\\n● \"overwrite\"クエリ文字列パラメーターを true に設定して API を呼び出すと、リセ ットされるドキュメント キーの現在のリストが要求のペイロードで上書きされま :unselected: す。\\n\\n● API を呼び出しても、ドキュメント キーはインデクサーによって実行される処理 のキューに追加されるだけです。スケジュールに従ってまたはオンデマンドでイ ンデクサーが次に呼び出されると、データ ソースからの他の変更よりも、リセッ トされたドキュメント キーの処理が優先されます。\\n\\n次の例は、ドキュメントのリセット要求を示しています。\\n\\n<figure>\\n\\n![](figures/13)\\n\\n<!-- FigureContent=\"HTTP POST https://[search service name].search.windows.net/indexers/[indexer name]/resetdocs?api-version=2020-06-30-Preview { \"documentKeys\" : [ \"key1\", \"key2\", \"key3\" ] }\" -->\\n\\n</figure>\\n\\n\\n\\n## キャッシュが無効になる変更\\n\\nキャッシュを有効にすると、インデクサーによってパイプライン構成の変更が評価さ れ、再利用できるコンテンツと再処理が必要なコンテンツが特定されます。このセク ションでは、キャッシュが完全に無効になる変更を示した後、増分処理がトリガーされ る変更を示します。\\n\\n無効化につながる変更とは、キャッシュ全体の有効性が失われる変更をいいます。た とえばデータ ソースの更新は、無効化につながる変更です。次に示すのは、インデク サー パイプラインのいずれかの部分に対する変更で、キャッシュが無効になるすべて のものの一覧です。\\n\\n● データ ソースの種類の変更\\n\\n● データ ソース コンテナーの変更\\n\\n· データ ソースの資格情報の変更\\n\\n· データ ソースの変更検出ポリシーの変更\\n\\n● データ ソースの削除検出ポリシーの変更\\n\\n● インデクサーのフィールドのマッピングの変更\\n\\n● インデクサーのパラメーターの変更:\\n\\n○ 解析モード\\n :unselected:\\n○ ファイル名拡張子を除外\\n\\n○ ファイル名拡張子のインデックスを作成\\n :unselected:\\n○ サイズの大きいドキュメントのストレージ メタデータのみのインデックスを作 :unselected: 成\\n :unselected:\\n区切りテキストのヘッダー\\n\\n○ 区切りテキストの区切り記号\\n\\n○ ドキュメントのルート\\n :unselected:\\n○ 画像操作(画像の抽出方法に対する変更)\\n :unselected:\\n\\n## 増分処理がトリガーされる変更\\n\\n増分処理では、対象のスキルセット定義が評価された後、再実行する必要があるスキル が特定され、ドキュメント ツリーの影響を受ける部分が選択的に更新されます。結果 的にインクリメンタル エンリッチメントが発生する変更の完全な一覧を次に示しま す。\\n\\n● スキルの種類を変更する(スキルの OData 型が更新された)。\\n\\n● スキルに固有のパラメーター(URL、defaults など) が更新された。\\n\\n● スキルの出力が変更(スキルから返される出力が追加または変更) された。\\n\\n● 先祖の変更を伴うスキルの入力の変更があった(スキルのチェーンが変更され た)。\\n\\n● アップストリームのスキルが無効化された(このスキルへの入力となっているスキ ルが更新された場合)。\\n\\n● ドキュメントの再プロジェクションを伴う更新がナレッジ ストアのプロジェクシ ヨン場所に生じた。\\n\\n● ドキュメントの再プロジェクションを伴う変更がナレッジ ストアのプロジェクシ :unselected: ョンに生じた。\\n\\n● インデックスに対するドキュメントの再プロジェクションを伴う変更が、インデ :unselected: クサーの出力フィールドのマッピングに生じた。\\n\\nキャッシュに使用される API\\n:unselected: :unselected: :unselected:\\nREST API バージョン 2020-06-30-Preview 以降では、インデクサーの追加のプロパティ :unselected: を使用して、インクリメンタル エンリッチメントが提供されます。スキルセットとデ ータ ソースは、一般公開されているバージョンを使用できます。操作の順序について 詳しくは、リファレンス ドキュメントに加えて、インクリメンタル エンリッチメント 用のキャッシュの構成に関するページを参照してください。\\n\\n● インデクサーの作成または更新(api-version=2020-06-30-Preview)\\n\\n● スキルセットの更新(api-version=2020-06-30) (要求での新しい URI パラメーター)\\n\\n● スキルのリセット(api-version=2020-06-30)\\n\\n· データ ソースの更新: プレビュー API バージョンで呼び出されるとき\\n\\nは、\"ignoreResetRequirement\" という名前の新しいパラメータを指定し、更新ア クションによってキャッシュが無効になってはならないときはそれを true に設定 する必要があります。\"ignoreResetRequirement\" は、簡単に検出できない不整合 が意図せずデータに発生する可能性があるため、慎重に使ってください。\\n\\n\\n## 次のステップ\\n\\nインクリメンタル エンリッチメントは、変更の追跡をスキルセットと AI エンリッチメ ントに拡張する強力な機能です。インクリメンタル エンリッチメントを使用すると、 スキルセットの設計を反復処理する際に、既存の処理済みコンテンツを再利用できま す。次のステップで、インデクサーのキャッシュを有効にします。\\n\\n\\n## インクリメンタル エンリッチメントのキャッシュを有効にする\\n\\n# Azure Cognitive Search のスキルセット の概念\\n\\n[アーティクル]·2023/08/08\\n\\nこの記事は、スキルセットの概念と構成について理解を深める必要がある開発者を対象 としています。ここでは、AI エンリッチメントの高度な概念に精通していることを前 提としています。\\n\\nスキルセットは、インデクサーにアタッチされている Azure Cognitive Search の再利用 可能なリソースです。これには、外部のデータ ソースから取得したドキュメントに対 して、組み込みの AI や外部カスタム処理を呼び出す1 つ以上のスキルが含まれていま す。\\n\\n次の図は、スキルセット実行の基本的なデータ フローを示しています。\\n :unselected:\\n<figure>\\n\\n![](figures/14)\\n\\n<!-- FigureContent=\"Skill inputs Source data Document cracking Enriched document (/document) Skillset (built-in or custom skills) Skill outputs Field mappings Output field mappings Search index\" -->\\n\\n</figure>\\n\\n\\nスキルによって行われるのは、スキルセット処理の開始から終了まで、“エンリッチさ れたドキュメント\"に対する読み取りと書き込みです。最初は、エンリッチされたドキ ユメントは、データ ソースから抽出された生コンテンツだけです(\"/document\" ルート ノードとして明確に表現されています)。スキルが実行されるたびに、スキルによって その出力がグラフのノードとして書き込まれるため、エンリッチされたドキュメント は、構造と実質的な内容を取得していきます。\\n\\nスキルセットの実行が完了すると、エンリッチされたドキュメントの出力は、\"出力フ ィールド マッピング\"を介してインデックスに組み込まれます。ソースからインデッ\\n\\nクスにそのまま転送する生コンテンツは、\"フィールド マッピング\"によって定義され ます。\\n\\nエンリッチメントを構成するには、スキルセットとインデクサーで設定を指定します。\\n\\n\\n## スキルセットの定義\\n\\nスキルセットとは、画像ファイル上のテキストや OCR の翻訳など、エンリッチメント を実行する 1つ以上の“スキル\"の配列です。スキルは、Microsoft の組み込みスキ ル、または外部でホストするロジックを処理するためのカスタム スキルのいずれかに なります。スキルセットにより、インデックスを付けるときに使用される、またはナ レッジ ストアにプロジェクションされるエンリッチされたドキュメントが生成されま す。\\n\\nスキルには、コンテキスト、入力、出力があります。\\n :unselected:\\n<figure>\\n\\n![](figures/15)\\n\\n<!-- FigureContent=\"Input \"context\": \"/document/normalized\\\\_images/\\\\*\" \"name\": \"image\", \"source\": \"/document/normalized\\\\_images/\" Source data Cracked document Enriched document (/document) Output. Skillset \"name\": \"text\" Skillset JSON \"context\": \"/document/normalized\\\\_images/\\\\*\" \"textExtractionAlgorithm\": null, \"lineEnding\": \"Space\", \"defaultLanguageCode\": \"en\", \"detectOrientation\": true, \"inputs\": \"name\": \"image\", \"source\": \"/document/normalized\\\\_images/ \"sourceContext\": null, \"inputs\": [] } 1, \"outputs\": \"name\": \"text\", \"targetName\": \"text\"\" -->\\n\\n</figure>\\n\\n\\n· コンテキストは、ドキュメントごとに1回、またはコレクション内のアイテムご とに1回の操作のスコープを指します。\\n\\n● 入力はエンリッチされたドキュメント内のノードから発信されます。ここ で、\"source\" と\"name\"は特定のノードを識別します。\\n\\n● 出力は、エンリッチされたドキュメントに新しいノードとして返送されます。値 は、ノードの\"name\"とノードの内容です。ノード名が重複している場合は、あ いまいさを解消するためにターゲット名を設定できます。\\n:selected: :unselected:\\n# スキル コンテキスト\\n\\n各スキルにはコンテキストがあります。コンテキストはドキュメント全体(/document) :unselected: であったり、ツリー内の下位のノード(/document/countries/\\\\*) であったりします。 :unselected: ンテキストによって次のことが決まります。\\n\\n● 1つの値に対してスキルが実行される回数(フィールドごと、ドキュメントごとに 1回)。型コレクションのコンテキスト値の場合、/\\\\* を追加すると、コレクショ ン内のインスタンスごとにスキルが 1回呼び出されます。\\n\\n● 出力の宣言。スキルの出力が追加される強化ツリー内の場所。出力は、常にコン テキスト ノードの子としてツリーに追加されます。\\n\\n● 入力の形状。 複数レベルのコレクションの場合、コンテキストを親コレクション に設定すると、スキル入力の形状に影響します。たとえば、国または地域のリス トが含まれる強化ツリーがあり、それぞれが郵便番号のリストを含む州のリスト でエンリッチメント処理されている場合、コンテキストをどのように設定するか によって、入力がどのように解釈されるかが決まります。\\n\\n|||||\\n| - | - | - | - |\\n| Context | 入力 |\\n| | |\\n| | |\\n| | |\\n| | | 入 力 の 形状     |||||\\n| | | |\\n| | | |\\n| | | | スキル の呼び出し | | | | | | | |\\n| /document/countries/\\\\* | /document/countries/\\\\*/states/\\\\*/zipcodes/\\\\* |\\n| | |\\n| | |\\n||||\\n| ||\\n| |\\n| | |\\n| | |\\n| | |\\n| | |\\n| | |\\n| | || |\\n| | || |\\n| | || |\\n| | || |\\n| | || |\\n| | | 国または地域のすべての郵便番号のリ                 | |\\n||||||||||| 国または 地域ごとに 1 回 | | | | | | | | | | | :unselected:\\n|||||\\n| - | - | - | - |\\n| Context | 入力 |\\n| | |\\n| | |\\n| | |\\n| | | 入 カ の 形 状     |||||\\n| | | |\\n| | | |\\n| | | | スキルの呼び出し | | | | | | | |\\n| | || |\\n| | | スト  | |\\n| /document/countries/\\\\*/states/\\\\* | /document/countries/\\\\*/states/\\\\*/zipcodes/\\\\* |\\n| | |\\n| | |\\n| | |\\n| | |\\n| | |\\n| | |\\n| | |\\n| | |\\n| | |\\n| | | 州内の郵便番号のリスト           |||||||||||\\n| | | |\\n| | | |\\n| | | |\\n| | | |\\n| | | |\\n| | | |\\n| | | |\\n| | | | 国または地域と州の組み合わせごとに1回 | | | | | | | | | | | | | | | | | | |\\n\\n\\n## スキルの依存関係\\n\\nスキルは、独立して並列に実行することも、あるスキルの出力を別のスキルにフィード する場合は順番に実行することもできます。次の例は、順番に実行される 2 つの組み 込みスキルを示しています。\\n\\n● スキル #1 は、\"reviews\\\\_text\" ソース フィールドのコンテンツを入力として受け取 り、そのコンテンツを出力として 5,000 文字の \"pages\"に分割するテキスト分割 スキルです。大きなテキストを小さなチャンクに分割すると、センチメント検出 などのスキルの結果が向上する可能性があります。\\n\\n● スキル#2 は、\"pages\" を入力として受け入れ、センチメント分析の結果が含まれ る \"Sentiment\" という名前の新しいフィールドを作成するセンチメント検出スキ ルです。\\n\\n最初のスキル(\"pages\") の出力が感情分析でどのように使用されているかに注目してく ださい。ここで、\"/document/reviews\\\\_text/pages/\\\\*\" はコンテキストと入力の両方で す。パスの構文の詳細については、スキルセットで注釈を参照する方法に関するペー ジを参照してください。\\n\\nJSON\\n\\n{\\n\\n\"skills\": [ {\\n\\n\"@odata. type\": \"#Microsoft. Skills. Text. SplitSkill\", \"name\": \"#1\", \"description\": null, \"context\": \"/document/reviews\\\\_text\",\\n\\n\"defaultLanguageCode\": \"en\",\\n\\n\"textSplitMode\": \"pages\", \"maximumPageLength\": 5000, \"inputs\": [ { \"name\": \"text\", \"source\": \"/document/reviews\\\\_text\" }\\n\\n], \"outputs\": [ {\\n\\n\"name\": \"textItems\", \"targetName\": \"pages\" }\\n\\n]\\n\\n},\\n\\n{\\n\\n\"@odata. type\": \"#Microsoft. Skills. Text. SentimentSkill\", \"name\": \"#2\",\\n\\n\"description\": null,\\n\\n\"context\": \"/document/reviews\\\\_text/pages/\\\\*\",\\n\\n\"defaultLanguageCode\": \"en\",\\n\\n\"inputs\": [ { \"name\": \"text\",\\n\\n\"source\": \"/document/reviews\\\\_text/pages/\\\\*\", }\\n\\n],\\n\\n\"outputs\": [ {\\n\\n\"name\": \"sentiment\", \"targetName\": \"sentiment\" }, {\\n\\n\"name\": \"confidenceScores\", \"targetName\": \"confidenceScores\" }, {\\n\\n\"name\": \"sentences\",\\n\\n<!-- PageHeader=\"\"targetName\": \"sentences\"\" -->\\n\\n}\\n\\n]\\n\\n}\\n\\n]\\n\\n}\\n\\n# 強化ツリー\\n\\nエンリッチされたドキュメントは、スキルセットの実行中に作成された一時的なツリー 状のデータ構造で、スキルを通じて行われたすべての変更を収集します。集合的に、 エンリッチメントはアドレス指定可能なノードの階層として表されます。ノードに は、外部データ ソースから逐語的に渡されたエンリッチされていないフィールドも含 まれます。\\n\\nエンリッチされたドキュメントが存在するのは、スキルセットの実行中ですが、キャッ シュしたり、ナレッジ ストアに送信したりできます。\\n\\nエンリッチされたドキュメントは、最初は、\"ドキュメント解析\"中にデータ ソースか ら抽出されたコンテンツに過ぎません。ここでテキストやイメージがソースから抽出さ れ、言語解析やイメージ解析に利用できるようになります。\\n\\n最初のコンテンツはメタデータと\"ルート ノード\"(document/content) です。ルート ノ ードは、通常、ドキュメント全体であるか、ドキュメント解析中にデータ ソースから 抽出された正規化されたイメージです。強化ツリーでの表現方法は、データ ソースの 種類によって異なります。次の表は、サポートされているいくつかのデータ ソースに ついて、エンリッチメント パイプラインに入ったドキュメントの状態を示していま す。\\n\\n| データ ソース/解析モード | Default | JSON、JSON 行、および CSV |\\n| - | - | - |\\n| Blob Storage | /document/content | /document/{key1} /document/{key2} |\\n|| /document/normalized\\\\_images/\\\\* … ||\\n| Azure SQL | /document/{column1} /document/{column2} … | 該当なし |\\n| Azure Cosmos DB | /document/{key1} /document/{key2} ... | 該当なし |\\n\\nスキルが実行されると、出力が新しいノードとしてエンリッチメント ツリーに追加さ れます。スキルの実行がドキュメント全体に対して行われる場合、ノードはルートの 下の最初のレベルに追加されます。\\n\\nノードは、ダウンストリーム スキルの入力として使用できます。たとえば、翻訳され た文字列などのコンテンツを作成するスキルは、エンティティを認識したり、キー フ レーズを抽出したりするスキルの入力になる可能性があります。\\n\\n<figure>\\n\\n![](figures/16)\\n\\n<!-- FigureContent=\"Enriched Document Tree Skillset definition Skill #1 - Text Translation document/content 1 Input 2 + translated\\\\_text I Output 3 Skill #2 - Entity Recognition Input + locations 4 Output\" -->\\n\\n</figure>\\n\\n\\nデバッグ セッションのビジュアル エディターを使用してエンリッチメント ツリーを視 覚化して操作できますが、ほとんどは内部構造です。\\n\\nエンリッチメントは変更できません。ノードは一度作成されたら編集できません。ス キルセットや強化ツリーの複雑さが増しても、強化ツリー内のすべてのノードをインデ ックスやナレッジ ストアにする必要はありません。\\n\\nエンリッチメント出力のサブセットのみを選択的に保持して、使用する情報のみを保持 することができます。インデクサー定義の出力フィールド マッピングによって、検索 インデックスに実際に取り込まれるコンテンツが決まります。同様に、ナレッジ スト アを作成する場合は、プロジェクションに割り当てられているシェイプに、出力をマッ プできます。\\n\\n\\n## 1 注意\\n\\n強化ツリー形式により、エンリッチメント パイプラインでは、メタデータをプリ ミティブ データ型にもアタッチできます。メタデータは有効な JSON オブジェク トになりませんが、ナレッジ ストア内のプロジェクション定義で有効な JSON 形 :unselected: 式にプロジェクションできます。詳細については、Shaper スキルに関するページ :unselected: をご覧ください。\\n\\n# インデクサーの定義\\n\\nインデクサーには、インデクサーの実行を構成するために使用されるプロパティとパラ メーターがあります。これらのプロパティの中には、検索インデックス内のフィール ドにデータ パスを設定するマッピングがあります。\\n\\n<figure>\\n\\n![](figures/17)\\n\\n<!-- FigureContent=\"Input \"context\": \"/document/normalized\\\\_images/\\\\*\" \"name\": \"image\", Source data Cracked document Enriched document (/document) \\'source\": \"/document/normalized\\\\_images/\" Output Skillset JSON \"name\": \"text\" Indexer JSON \"fieldMappings\": [ \"fieldMappings\": [], \"outputFieldMappings\" \"sourceFieldName\": \"metadata\\\\_storage\\\\_path\" \"targetFieldName\": \"metadata\\\\_storage\\\\_path\" \"mappingFunction\": { \"sourceFieldName\": \"/document/normalized\\\\_images/\\\\*/text\". \"name\": \"base64Encode\", \"targetFieldName\": \"ocr\\\\_content\", \"parameters\": null mappingFunction\": null } Index JSON \"fields\": \"fields\" [ Search index \"name\": \"metadata\\\\_storage\\\\_path\" \"name\": \"ocr\\\\_content\". \"type\": \"Edm. String\", + \"type\": \"Edm. String\", \"searchable\": false, \"searchable\": true,\" -->\\n\\n</figure>\\n\\n\\nマッピングには次の2 つのセットがあります。\\n\\n● \"fieldMappings\" は、ソース フィールドを検索フィールドにマップします。\\n\\n● \"outputFieldMappings\"は、エンリッチされたドキュメント内のノードを検索フィ ールドにマップします。\\n\\n\"sourceFieldName\"プロパティは、データ ソース内のフィールドまたはエンリッチメン :unselected: ト ツリー内のノードのいずれかを指定します。\"targetFieldName\" プロパティは、コン :unselected: テンツを受け取るインデックス内の検索フィールドを指定します。\\n\\n\\n## エンリッチメントの例\\n\\nこの例では、ホテル レビュー スキルセットロを参照ポイントとして使用し、スキルの 実行によって強化ツリーがどのように発展するかを概念図を使って説明します。\\n\\nまた、この例では以下もわかります。\\n\\n● スキルの実行回数の決定に対して、スキルのコンテキストと入力がどのように働 くか\\n\\n· コンテキストに基づく入力の形状\\n:unselected: :unselected:\\nこの例では、CSV ファイルのソース フィールドに、ホテルに関する顧客レビュー (\"reviews\\\\_text\") と評価(\"reviews\\\\_rating\") が含まれています。インデクサーによって、 BLOB ストレージからメタデータ フィールドが追加され、スキルによって、翻訳された テキスト、センチメント スコア、キーフレーズ検出が追加されます。\\n\\nホテル レビューの例で、エンリッチメント プロセス内の“ドキュメント\"は1つのホテ ルレビューを表します。\\n\\n\\n### ? ヒント\\n\\nこのデータの検索インデックスやナレッジ ストアを作成するには、Azure portal、Postman、または REST API を使用します。デバッグ セッションを使用し て、スキルセットの構成、依存関係、強化ツリーへの影響の分析情報を確認する こともできます。この記事のイメージは、デバッグ セッションからプルされま す。\\n\\n概念的には、最初の強化ツリーは次のようになります。\\n :unselected:\\n<figure>\\n\\n![](figures/18)\\n\\n<!-- FigureContent=\"● reviews\\\\_text /document :unselected: reviews\\\\_rating :unselected: other columns\" -->\\n\\n</figure>\\n\\n\\nすべての強化のルート ノードは \"/document\" です。BLOB インデクサーを使用する と、 \"/document\" ノードに子ノード\"/document/normalized\\\\_images\" と\\n\\n\"/ document/content\" ができます。この例のように、データが CSV の場合、列名は \" / document\" の下のノードにマップされます。\\n\\n\\n## スキル #1: 分割スキル\\n\\nソース コンテンツが大量のテキストで構成されている場合は、言語、センチメント、 キーフレーズ検出の精度を高めるために、より小さいコンポーネントに分割すると便利\\n:selected:\\nです。使用できるグレインは、ページと文の 2 つです。ページは約 5,000 文字で構成 されます。\\n\\n通常、テキスト分割スキルは、スキルセットにおいて最初に選ばれます。\\n\\nJSON\\n\\n\"@odata. type\": \"#Microsoft. Skills. Text. SplitSkill\",\\n\\n\"name\": \"#1\",\\n\\n\"description\": null,\\n\\n\"context\": \"/document/reviews\\\\_text\",\\n\\n\"defaultLanguageCode\": \"en\", \"textSplitMode\": \"pages\",\\n\\n\"maximumPageLength\": 5000,\\n\\n\"inputs\": [ { \"name\": \"text\",\\n\\n\"source\": \"/document/reviews\\\\_text\" }\\n\\n\"outputs\": [ {\\n\\n\"name\": \"textItems\",\\n\\n\"targetName\": \"pages\" }\\n\\n\"/document/reviews\\\\_text\"のスキル コンテキストでは、この分割スキルは\\n\\nreviews\\\\_text に対して1回実行されます。スキルの出力はリストであり、\\n\\nreviews\\\\_text が 5000 文字のセグメントに分割されています。分割スキルからの出力 は、 pages という名前が付けられ、エンリッチメント ツリーに追加されます。\\n\\ntargetName 機能を使用することで、強化ツリーに追加される前に、スキル出力の名前 を変更できます。\\n\\nこれで、強化ツリーのスキルのコンテキストの下に、新しいノードが配置されました。 このノードは、任意のスキル、プロジェクション、または出力フィールド マッピング :unselected: で使用できます。\\n\\n:unselected:<figure>\\n\\n![](figures/19)\\n\\n<!-- FigureContent=\"Page\\\\_0 reviews\\\\_text :unselected: Pages /document :unselected: Page\\\\_1 :unselected: reviews\\\\_rating\" -->\\n\\n</figure>\\n\\n\\nスキルによってノードに追加されたいずれかの強化にアクセスするには、強化の完全な パスが必要です。たとえば、 pages ノードのテキストを別のスキルへの入力として使 用する場合は、 \"/document/reviews\\\\_text/pages/\\\\*\" として指定する必要があります。パ スの詳細については、注釈の参照に関するページをご覧ください。\\n\\n\\n### スキル #2: 言語検出\\n\\nホテル レビュー ドキュメントには、複数の言語で表される顧客フィードバックが含ま れています。言語検出スキルによって、どの言語が使用されているか判断されます。 結果は、キー フレーズ抽出とセンチメント検出に渡され(非表示)、センチメントと語 句を検出するときに言語が考慮されます。\\n\\n言語検出スキルは、スキルセットで定義されている3 番目のスキル(スキル#3) です が、次に実行されるスキルです。入力は必要ないので、前のスキルと並行して実行さ れます。先行する分割スキルと同様に、言語検出スキルもドキュメントごとに1回呼 び出されます。強化ツリーには言語用の新しいノードが追加されています。\\n:selected: :unselected:\\n:unselected:<figure>\\n\\n![](figures/20)\\n\\n<!-- FigureContent=\"Language reviews\\\\_text :unselected: Page\\\\_0 /document Pages :unselected: reviews\\\\_rating Page\\\\_1\" -->\\n\\n</figure>\\n\\n :unselected:\\n# スキル #3 と #4 (感情分析とキー フレーズ検出)\\n\\nお客様からのフィードバックには、ポジティブなものからネガティブなものまで、さま ざまなエクスペリエンスが反映されています。フィードバックは、感情分析スキルに よって分析され、負数から正数の範囲内で一連のスコアが割り当てられます。センチメ ントがはっきりしない場合は、中立が割り当てられます。感情分析と同時に、結果的 に重要であると思われる単語と短いフレーズが、キー フレーズ検出によって特定さ れ、抽出されます。\\n\\nコンテキストとして /document/reviews\\\\_text/pages/\\\\* が指定された感情分析スキルとキ :unselected: ー フレーズ スキルは両方とも、pages コレクション内の項目ごとに1回ずつ呼び出さ れます。スキルからの出力は、関連付けられている page 要素の下のノードになりま す。\\n\\nスキルセットに含まれる残りのスキルを表示し、各スキルの実行によって強化のツリー がどのように成長し続けるかを視覚化できるようになるはずです。マージ スキルや Shaper スキルなどの一部のスキルでも新しいノードが作成されますが、既存のノード のデータのみが使用され、新しい強化は作成されません。\\n:unselected: :unselected: :selected: :unselected:\\n:unselected:<figure>\\n\\n![](figures/21)\\n\\n<!-- FigureContent=\"O Sentiment Page\\\\_0 :unselected: :unselected: Language :unselected: KeyPhrases reviews\\\\_text( /document( Pages :unselected: reviews\\\\_rating :unselected: Sentiment Page\\\\_1 :unselected: :unselected: KeyPhrases\" -->\\n\\n</figure>\\n\\n\\n上のツリーのコネクタの色は、エンリッチメントが異なるスキルで作成されたことを示 しています。ノードは個別に指定する必要があり、親ノードを選択したときに返される オブジェクトの一部にはなりません。\\n\\n\\n## スキル #5: Shaper スキル\\n\\n出力にナレッジ ストアが含まれる場合は、最後のステップとして Shaper スキルを追加 します。Shaper スキルによって、強化ツリー内のノードからデータ シェイプが作成さ れます。たとえば、複数のノードを1つのシェイプに統合することができます。その 後、このシェイプをテーブルとして射影し(ノードはテーブル内の列になります)、名前 によってシェイプをテーブル プロジェクションに渡します。\\n :unselected:\\n整形が1つのスキルにまとめられているため、Shaper スキルは簡単に操作できます。 また、個々のプロジェクション内でインライン整形を選ぶこともできます。Shaper ス キルによってエンリッチメント ツリーの加減が行われることはありません。したがっ て、これは視覚化されません。代わりに、Shaper スキルは、既にあるエンリッチメン ト ツリーを再現する手段と考えることができます。概念的には、これはデータベース 内のテーブルからビューを作成するのと似ています。\\n\\nJSON\\n\\n\"@odata.type\": \"#Microsoft.Skills.Util. ShaperSkill\",\\n\\n\"name\": \"#5\",\\n\\n\"description\": null,\\n\\n\"context\": \"/document\",\\n\\n\"inputs\": [ { \"name\": \"name\",\\n\\n\"source\": \"/document/name\"\\n\\n},\\n:unselected: :selected: :selected:\\n{\\n\\n\"name\": \"reviews\\\\_date\",\\n\\n\"source\": \"/document/reviews\\\\_date\"\\n\\n{\\n\\n\"name\": \"reviews\\\\_rating\",\\n\\n\"source\": \"/document/reviews\\\\_rating\" {\\n\\n\"name\": \"reviews\\\\_text\",\\n\\n\"source\": \"/document/reviews\\\\_text\" },\\n\\n{\\n\\n\"source\": \"/document/reviews\\\\_title\" {\\n\\n\"name\": \"AzureSearch\\\\_DocumentKey\",\\n\\n\"source\": \"/document/AzureSearch\\\\_DocumentKey\" {\\n\\n\"name\" : \"pages\",\\n\\n\"sourceContext\": \"/document/reviews\\\\_text/pages/\\\\*\"\\n\\n\"inputs\": [ {\\n\\n\"name\": \"Sentiment\",\\n\\n\"source\": \"/document/reviews\\\\_text/pages/\\\\*/Sentiment\" {\\n\\n\"name\" : \"LanguageCode\", \"source\": \"/document/Language\" }, {\\n\\n\"name\": \"Page\", \"source\": \"/document/reviews\\\\_text/pages/\\\\*\"\\n\\n},\\n\\n{ \"name\": \"keyphrase\",\\n\\n\"sourceContext\": \"/document/reviews\\\\_text/pages/\\\\*/Keyphrases/\\\\*\", \"inputs\": [ { \"name\": \"Keyphrases\",\\n\\n\"source\": \"/document/reviews\\\\_text/pages/\\\\*/Keyphrases/\\\\*\" }\\n\\n] }\\n\\n\"name\": \"reviews\\\\_title\",\\n\\n]\\n\\n}\\n\\n1,\\n\\n\"outputs\": [ {\\n\\n\"name\": \"output\",\\n\\n\"targetName\": \"tableprojection\"\\n\\n<!-- PageNumber=\"}\" -->\\n\\n]\\n\\n<!-- PageNumber=\"}\" -->\\n\\n# 次のステップ\\n\\n概要と例を参考にしながら、組み込みスキルを使用して、最初のスキルセットを作成し てみてください。\\n\\n\\n## 最初のスキルセットを作成する\\n\\n# Azure Al Search 内の統合データのチャ ンキングと埋め込み\\n\\n[アーティクル]·2024/03/27\\n\\n\\n## 1 重要\\n\\nこの機能はパブリック プレビュー段階にあり、追加使用条件☑の下で提供されま す。この機能は、2023-10-01-Preview REST API でサポートされます。\\n\\n統合ベクター化によって、データ チャンキングとテキスト-to-ベクター埋め込みがイン デクサーベース インデックス作成のスキルに追加されます。テキスト-to-ベクター変 換もクエリに追加されます。\\n\\nこの機能は、プレビューのみ段階です。一般提供バージョンのベクトル検索と以前の プレビューバージョンでは、データのチャンキングとベクター化が外部のチャンキン グとベクターのコンポーネントに依存しており、アプリケーション コードが各手順を 操作し、調整する必要があります。このプレビュー版では、チャンキングとベクター 化がスキルおよびインデクサーを通じてインデックス作成に組み込まれています。テ キスト分割スキルを使用してデータをチャンクするスキルセットをセットアップして、 それから AzureOpenAlEmbedding スキルまたはカスタム スキルを使用して埋め込みモ デルを呼び出すことができます。インデックス作成時に使用されるあらゆるベクトル 化を、テキストをベクターに変換するクエリで呼び出すこともできます。\\n\\nインデックス作成の場合、統合ベクター化では以下が必要です。\\n\\n● サポートされるデータ ソースからデータを取得するインデクサー。\\n\\n● テキスト分割スキルを呼び出してデータをチャンクする スキルセットと、 AzureOpenAlEmbedding スキルとデータをベクター化するためのカスタム スキル のいずれか。\\n\\n● チャンクおよびベクター化した内容を受け取るための 1つ以上のインデックス。\\n\\nクエリの場合:\\n\\n● インデックス スキーマで定義され、ベクター フィールドに割り当てられて、自動 的にクエリ時に使用されてテキスト クエリをベクターに変換するベクター化。\\n\\nベクター変換は、テキスト-to-ベクターの一方向です。クエリと結果にはベクター-to- テキスト変換がありません(たとえば、ベクター結果を人間が読み取り可能な文字列に 変換することはできません)。\\n\\n# コンポーネント図\\n\\n次の図は、統合ベクター化の構成要素を示しています。\\n\\nAzure Al Search processing\\n\\n<figure>\\n\\n![](figures/22)\\n\\n<!-- FigureContent=\"2\\\\. Results after vectorizing user query Supported data store: Azure Blob Storage, Azure SQL, Data cracking Data enrichment (Chunking, index projections and others) Azure Al Search chunked index Azure Cosmos DB Azure Data Lake Gen2 etc. Vectorization during ingestion Vectorization at query time 1. Text query submission Al enrichment Azure OpenAl Service Custom Web App Embedding Models + Azure OpenAl Service GPT model\" -->\\n\\n</figure>\\n\\n\\nこちらが統合ベクター化のための構成要素のチェックリストです。\\n\\n● インデクサーベースのインデックス作成でサポートされているデータ ソース。\\n\\n● ベクター フィールドを指定するインデックスと、ベクター フィールドに割り当て られたベクター化定義。\\n\\n· データ チャンキングのためのテキスト分割スキルを提供するスキルセットと、べ クター化のスキル(AzureOpenAiEmbedding スキルと、外部埋め込みモデルをポ イントするカスタム スキルのいずれか)。\\n\\n● オプションとして、チャンクしたデータをセカンダリ インデックスにプッシュす るインデックス プロジェクション(スキルセットにも定義される)\\n :unselected:\\n● 埋め込みモデル(Azure OpenAl でデプロイされているか、HTTP エンドポイントを :unselected: 通じて提供される)。\\n\\n● プロセスをエンドツーエンドで進めるためのインデクサー。インデクサーでは、 変更検出のスケジュール、フィールド マッピング、優先度も指定されます。\\n\\nこのチェックリストは統合ベクター化に重点を置いていますが、お持ちのソリューショ ンはこのリストに限定されません。AI エンリッチメントのためのスキルを増やし、ナ レッジ ストアを作成し、セマンティック ランク付けを追加し、関連性チューニングや 他のクエリ機能を追加することができます。\\n\\n<!-- PageFooter=\"可用性と料金\" -->\\n\\n統合ベクター化の可用性は、埋め込みモデルに基づきます。Azure OpenAl を使用して いる場合は、「リージョン別の提供状況☑」を確認してください。\\n\\nカスタム スキルと Azure ホスティング メカニズム(Azure 関数アプリ、Azure Web ア プリ、Azure Kubernetes など) を使用している場合は、リージョン別の製品ページでで 機能の可用性について確認してください。\\n\\nデータ チャンキング(テキスト分割スキル) は無料で、すべての地域のすべての Azure AI サービスでご利用になれます。\\n\\n\\n## 1 注意\\n\\n2019 年1月1日より前に作成された一部の古い検索サービスは、ベクトル ワー クロードをサポートしないインフラストラクチャにデプロイされています。ベク :unselected: :unselected: トル フィールドをスキーマに追加しようとしてエラーが表示された場合、それは サービスが古いためです。このような場合は、ベクトル機能を試すために新しい 検索サービスを作成する必要があります。\\n\\n\\n## 統合ベクター化をサポートできるのはどんなシ ナリオですか?\\n\\n● 大きなドキュメントをチャンクに再分割すると、ベクターおよび非ベクターシナ リオに便利です。ベクターの場合、埋め込みモデルの入力制約に合わせるのにチ ャンクが役立ちます。非ベクター シナリオの場合、チャット スタイルの検索ア プリで GPT がインデックス作成したチャンクからの応答をアセンブルしていま す。ベクトル化(または非ベクトル化)されたチャンクをチャットスタイルの検索 に使用できます。\\n\\n● フィールドのすべてがベクター フィールドであり、ドキュメント ID(検索インデ ックスに必要) が唯一の文字列フィールドであるベクター ストアを構築します。 ベクター ストアにクエリを実行してドキュメント ID を取得し、ドキュメントの ベクター フィールドを別のモデルに送信します。\\n\\n● ベクターおよびテキスト フィールドを組み合わせて、セマンティック ランク付け を使用した(または使用しない) ハイブリッド検索にします。統合ベクター化によ ってベクター検索でサポートされるシナリオのすべてが簡略化されます。\\n\\n統合ベクター化を使用するのはどのようなとき\\n\\n<!-- PageNumber=\"か\" -->\\n\\n組み込み統合ベクター化サポートの Azure Al Studio を使用することをお勧めします。 この方法でお客様のニーズが満たされない場合は、Azure Al Search のプログラマティ :unselected: ック インターフェイスを使用して統合ベクター化を呼び出すインデクサーとスキルセ ットを作成することができます。\\n\\n# 統合ベクター化の使用方法\\n\\nクエリ専用ベクター化の場合:\\n\\n1\\\\. インデックスにベクター化を追加します。インデックスにベクターを生成するた めに使用したのと同じ埋め込みモデルになるはずです。\\n\\n2\\\\. ベクター プロファイルにベクター化を割り当て、それからベクター プロファイル :unselected: をベクター フィールドに割り当てます。\\n\\n3\\\\. ベクター化するテキスト文字列を指定するベクター クエリを作成します。\\n\\nより一般的なシナリオ- インデックス作成時のデータのチャンキングとベクター化:\\n\\n1\\\\. インデクサーベースのインデックス作成でサポートされているデータ ソースへの データ ソース接続を作成します。\\n\\n2\\\\. チャンキング用のテキスト分割スキルと、AzureOpenAlEmbeddingModel または チャンクをベクター化するカスタムスキルを呼び出すスキルセットを作成しま す。\\n\\n3\\\\. クエリ時のベクター化を指定し、それをベクター フィールドに割り当てるインデ ックスを作成します。\\n\\n4\\\\. データの取得からスキルセット実行まで、インデックス作成を通してすべてを進 めるためのインデクサーを作成します。\\n\\nオプションとして、チャンクしたコンテンツが 一方のインデックス上にあり、チャン クされていないコンテンツが別のインデックスにある高度なシナリオのためのセカンダ リ インデックスを作成します。チャンクしたインデックス(セカンダリ インデックス) は RAG アプリで役立ちます。\\n\\n\\n## ? ヒント\\n\\nAzure portal で新しい[データのインポートとベクトル化]ウィザードを試して、 コードを記述する前に統合ベクター化を探索します。\\n\\nあるいは、同じワークフローを実行するための Jupyter ノートブックをセルごとに :unselected: 構成して、各手順がどう機能するかを調べます。\\n\\n<!-- PageFooter=\"制限事項\" -->\\n:unselected:\\nAzure OpenAl の埋め込みモデルのクォータと制限について理解します。Azure Al Search には再試行ポリシーがありますが、クォータを使い果たすと、再試行が失敗し ます。\\n\\nAzure OpenAl の 1 分あたりトークンの制限は、モデルごと、サブスクリプションごと に設けられています。埋め込みモデルをクエリとインデックス作成の両ワークロード で使用している場合は、このことを覚えておいてください。可能であれば、ベストプ ラクティスに従ってください。ワークロードごとに埋め込みモデルを用意して、それ :unselected: らを別々のサブスクリプションでデプロイするようにしてください。\\n :unselected:\\nAzure Al Search では、サービスの制限がレベルおよびワークロード別にあることを忘 れないでください。\\n\\n最後に、次の機能は現在サポートされていません。\\n\\n● カスタマー マネージド暗号化キー\\n\\n● ベクター化への共有プライベート リンク接続\\n\\n● 現在は、統合型データ チャンキングおよびベクター化のためのバッチ処理があり ません\\n\\n\\n## 統合ベクター化のメリット\\n\\n統合ベクター化の重要メリットのいくつかを紹介します。\\n\\n● データ チャンキングとベクター化の分離したパイプラインがありません。コード :unselected: の書き込みと維持がより簡単です。\\n\\n● エンド ツー エンドのインデックス作成を自動化します。ソース(Azure Storage、 Azure SQL、Cosmos DB など) でデータが変更されると、インデクサーはこれらの 更新を、パイプライン全体(取得からドキュメントの解読まで)で、オプションの AI エンリッチメント、データ チャンキング、ベクター化、インデックス作成を通 じて進めることができます。\\n\\n· チャンクしたコンテンツをセカンダリ インデックスに射影します。セカンダリ インデックスは他の検索インデックス(フィールドや他のコンストラクトを持つス キーマ) のように作成されますが、インデクサーによりプライマリ インデックス と並行して作成されます。各ソース ドキュメントのコンテンツが、同じインデッ クス作成実行中に、プライマリおよびセカンダリ インデックスのフィールドへ流 れていきます。\\n\\nセカンダリ インデックスの目的は、データ チャンキングおよび取得拡張生成 (RAG) アプリです。サイズの大きな PDF をソース ドキュメントとして想定する と、プライマリ インデックスには基本情報(タイトル、日付、作成者、説明) があ\\n\\nり、セカンダリ インデックスにはコンテンツのチャンクがあります。データチ ヤンク レベルのベクター化によって、関連する情報を見つけて(各チャンクが検 索可能である) 関連する応答を返すのが、特にチャットスタイルの検索アプリでは 簡単になります。\\n\\n\\n## チャンク後のインデックス\\n\\nチャンキングとは、コンテンツをより小さな管理可能部分(チャンク) に分割すること :unselected: で、それらを別々に処理できるようにするプロセスです。チャンキングが必要になる のは最大入力サイズの埋め込みモデルや大型言語モデルでソース ドキュメントが大き すぎるけれども、それによって RAG パターンやチャットスタイル検索でインデックス 構造がよくなると考えられる場合です。\\n\\n次の図は、チャンク後インデックス作成の構成要素を示しています。\\n\\n<figure>\\n\\n<figcaption>\\n\\nIntegrated Vectorization: Data ingestion process skills view\\n\\n</figcaption>\\n\\n![](figures/23)\\n\\n<!-- FigureContent=\"Supported data store: Index Projections configuration Data cracking Azure Al Search chunked index Azure Blob Storage, Azure SQL, Azure Cosmos DB Azure Data Lake Gen2 etc. Text split skill (used for data chunking) Data enrichment (Skillset configuration)\" -->\\n\\n</figure>\\n\\n\\nVectorization during ingestion\\n\\n<figure>\\n\\n![](figures/24)\\n\\n</figure>\\n\\n\\nAl enrichment (i.e. OCR skill used to process PDF with text in\\n\\nembedded images)\\n\\n\\n## 次のステップ\\n\\n● 検索インデックスにベクター化を構成する\\n\\n· スキルセットにインデックス投影を構成する\\n\\nAzure OpenAl Service Embedding skill\\n\\nWeb API Custom skill\\n\\n\\\\+\\n\\n# Azure Cognitive Search でのフルテキス ト検索\\n\\n[アーティクル]·2023/09/27\\n\\nこの記事は、Azure Cognitive Search におけるフルテキスト検索のしくみについて理解 を深める必要がある開発者を対象としています。テキスト クエリに関して、Azure Cognitive Search はほとんどの状況で速やかに適切な結果を返します。しかし一見、間 違っているのではないか、と思うような結果が返されることも皆無ではありません。 このような状況では、Lucene による 4 段階から成るクエリ実行(クエリ解析、字句解 析、文書のマッチング、スコア付け) についての背景知識があると、具体的にどのよう な変更をクエリ パラメーターやインデックス構成に加えれば目的の結果が生成される かが特定しやすくなります。\\n\\n\\n## 1 注意\\n\\nAzure Cognitive Search では、フルテキスト検索に Apache Lucene △ が使われてい ますが、Lucene の機能がそのままの形で統合されているわけではありません。 Microsoft は、Azure Cognitive Search にとって重要なシナリオを実現する Lucene の機能を選んで公開、拡張しています。\\n\\n\\n## アーキテクチャの概要と図\\n\\nクエリの実行には次の4 つの段階があります。\\n\\n1\\\\. クエリ解析\\n\\n2\\\\. 字句解析\\n\\n3\\\\. 文書検索\\n\\n4\\\\. ポイントの計算\\n\\nフルテキスト検索クエリは、クエリ テキストを解析して検索語と演算子を抽出するこ とから始まります。2 つのパーサーがあり、速度と複雑さを選択できます。次に分析 段階では、個々の検索語がしばしば分解され、新しい形に再構築されます。この手順 では、できるだけ広い範囲から“一致と見なす候補\"が得られます。検索エンジンは、 インデックスをスキャンして、一致する語句を持つドキュメントを検索し、各一致をス コア付けします。その後、一致する個々の文書に割り当てられた関連度スコアに基づ :unselected: いて結果セットが並べ替えられます。このようにランク順に並んだリストの先頭の結 果が、呼び出し元のアプリケーションに返されます。\\n\\n以下の図は、検索要求の処理に使用されるコンポーネントを示しています。\\n<figure>\\n\\n![](figures/25)\\n\\n<!-- FigureContent=\"Query text Query Top Query Parser tree Search Engine 50 Simple | Full Query Index Analyzed terms terms Analyzer Standard Asciifolding Keyboard Pattern Simple Stop Whitespace ... Language <Custom>\" -->\\n\\n</figure>\\n\\n\\n|||\\n| - | - |\\n| 主要コンポー ネント | 機能の説明 |\\n| クエリ パーサ - | クエリ演算子から検索語を切り離し、検索エンジンに送るクエリ構造(クエリ ツリー) を作成します。 |\\n| アナライザー | 検索語に対する字句解析を実行します。このプロセスには、検索語の変換、削 除、拡大が伴う場合があります。 |\\n| インデックス | 索引付けの対象文書から抽出した検索可能な語句を効率よく体系的に格納する データ構造です。 |\\n| 検索エンジン | 転置インデックスの内容に基づいて、一致する文書を検索してスコア付けしま す。 |\\n\\n\\n## 検索要求の構造\\n\\n検索要求は、結果セットで返すべき内容を詳細に規定した仕様です。最も単純な形式 は、どのような種類の条件も含まれていない空のクエリです。しかしより現実的な例 では、パラメーターや複数の検索語を伴うのが一般的です。場合によっては、検索範囲 を特定のフィールドに限定したり、フィルター式や並べ替え規則が使われたりすること もあります。\\n\\n次の例は、REST API を使用して Azure Cognitive Search に送信できる検索要求です。\\n\\nPOST /indexes/hotels/docs/search?api-version=2020-06-30 {\\n\\n\"search\": \"Spacious, air-condition\\\\* +\\\\\\\\\"Ocean view\\\\\\\\\"\",\\n\\n\"searchFields\": \"description, title\",\\n\\n\"searchMode\": \"any\",\\n\\n\"filter\": \"price ge 60 and price lt 300\",\\n\\n\"orderby\": \"geo. distance(location, geography\\' POINT (-159.476235 22.227659)\\')\",\\n\\n\"queryType\": \"full\" }\\n\\nこの要求に対して、検索エンジンは次の操作を実行します。\\n\\n1\\\\. 価格が $60 以上 $300 未満の文書を検索します。\\n\\n2\\\\. クエリを実行します。この例では、検索クエリが \"Spacious, air-condition\\\\* +\\\\\\\\\"Ocean view\\\\\\\\\"\" という語句で構成されています(通常はユーザーが句読点を入力 することはありませんが、この例では、アナライザーによる処理を説明するため にあえて含めています)。\\n\\nこのクエリの場合、検索エンジンは、\"searchFields\"に指定された説明フィールド とタイトル フィールドをスキャンして、 \"Ocean view\" を含む文書、さらに \"spacious\" という語句、または \"air-condition\" というプレフィックスで始まる 語句を探します。明示的に必須指定(+) されていない語句に関して、マッチング 対象を任意(既定) とするか、すべてとするかが、\"searchMode\" パラメーターで 指定されています。\\n\\n3\\\\. 結果として得られた一連のホテルを特定の地理的位置に近い順に並べ替え、結果 を呼び出し元のアプリケーションに返します。\\n\\nこの記事では主に、検索クエリ: \"Spacious, air-condition\\\\* +\\\\\\\\\"Ocean view\\\\\\\\\"\" の処理に ついて取り上げています。フィルター処理と並べ替えについては取り上げません。詳 細については、Search API のリファレンス ドキュメントを参照してください。\\n\\n\\n### 第1段階: クエリ解析\\n\\n前出のとおり、検索要求の最初の行がクエリ文字列です。\\n\\n\"search\": \"Spacious, air-condition\\\\* +\\\\\\\\\"Ocean view\\\\\\\\\"\",\\n\\nクエリ パーサーは、検索語から演算子(この例の\\\\* や+) を切り離し、検索クエリを \"サブクエリ\"に分解します。次の種類のサブクエリがサポートされています。\\n\\n● \"単語検索\": 独立した語を検索(spacious など)\\n\\n● \"フレーズ検索\": 引用符で囲まれた語句を検索(ocean view など)\\n\\n● \"プレフィックス検索\": 語句+プレフィックス演算子 \\\\*を検索 (air-condition など)\\n\\nサポートされているクエリの種類すべての一覧については、Lucene のクエリ構文に関 するページを参照してください\\n\\n文書が一致していると見なされるために検索条件との一致が\"必須(must)\"であるのか \"勧告 (should be)\" であるのかは、サブクエリに関連付けられている演算子によって決 まります。たとえば、 +\"Ocean view\" は+演算子が指定されているので検索条件との 一致が“必須\"となります。\\n\\nクエリ パーサーは、サブクエリを“クエリ ツリー\"(クエリを表す内部的な構造) に再構 築して検索エンジンに渡します。クエリ解析の第1段階で、クエリ ツリーは次のよう になります。\\n\\n<figure>\\n\\n![](figures/26)\\n\\n<!-- FigureContent=\"Boolean query 1 Should Should Must Term Query Spacious, Prefix Query air-condition Phrase Query \"Ocean view\"\" -->\\n\\n</figure>\\n\\n\\n\\n### サポートされるパーサー: Simple と Full Lucene\\n\\nAzure Cognitive Search では、 simple (既定) と full の2 種類のクエリ言語が使用され ます。どちらのクエリ言語を使うかは、検索要求で queryType パラメーターの設定で 指定します。その指定に基づいて、クエリ パーサーが演算子と構文を解釈します。\\n\\n● Simple クエリ言語は直感的で安定しており、多くの場合、クライアント側の処理 を行わなくてもユーザー入力をそのまま解釈するのに適しています。Web 検索工 ンジンで多く使われているクエリ演算子がサポートされます。\\n\\n● Full Lucene クエリ言語は、queryType=full を設定することによって利用できま す。より多くの演算子やクエリの種類(ワイルドカード、あいまい一致、正規表 現、フィールド指定検索など) がサポートされることで、既定の Simple クエリ言 語が拡張されます。たとえば、Simple クエリ構文で送信された正規表現は、式と してではなくクエリ文字列と解釈されます。この記事で紹介している要求の例で は、Full Lucene クエリ言語を使用しています。\\n\\n\\n## searchMode がパーサーに及ぼす影響\\n\\n解析方法に作用する検索要求パラメーターとしては、他にも \"searchMode\" パラメータ ーがあります。これは、ブール クエリの既定の演算子、つまり any (既定) または all を制御します。\\n\\n\"searchMode=any\" とした場合(既定)、spacious と air-condition との間に区切り記号と して置かれた空白は OR (II) の働きをするため、サンプル クエリ テキストは次のクエ リ テキストに相当します。\\n\\nSpacious, | | air-condition\\\\*+\"Ocean view\"\\n\\nブール クエリの構造において、明示的な演算子(+\"Ocean view\" の+など) の意味はは っきりしています。つまり検索条件との一致は“必須(must)\" です。それに比べて、残 りの語句(spacious と air-condition) の解釈はあいまいです。検索エンジンが探すべき なのは、ocean view と spacious と air-condition の“すべて\"との一致でしょうか。そ れとも、ocean view に加えて、残りの2 つの語句のうち、\"どちらか一方\"のみが含ま れていればよいのでしょうか。\\n\\n既定(\"searchMode=any\")では、検索エンジンはより広い解釈を想定します。どちらか 一方のフィールドが一致していればよい(should) の意味、つまり \"or\" のセマンティク スで解釈されます。先ほど例に挙げた、2 つの\"should\" 演算を含んだクエリ ツリーは 既定の動作を示しています。\\n\\nでは、\"searchMode=all\" と設定したらどうなるでしょうか。この場合は、空白文字が \"and\"演算と解釈されます。残りの2つの語句が両方とも文書に存在したときに初め て一致と見なされます。最終的にサンプル クエリは次のように解釈されます。\\n\\n\\\\+Spacious, +air-condition\\\\*+\"Ocean view\"\\n\\n変更後のクエリ ツリーは次のようになり、一致する文書は 3 つすべてのサブクエリの 積集合となります。\\n\\n<figure>\\n\\n![](figures/27)\\n\\n<!-- FigureContent=\"Boolean query Must → Must Must 一 Term Query Spacious, Prefix Query air-condition Phrase Query \"Ocean view\"\" -->\\n\\n</figure>\\n\\n\\n\\n### 4 注意\\n\\n\"searchMode=all\" より \"searchMode=any\"を選ぶ場合は、代表的なクエリを実行 したうえで判断することをお勧めします。普段から演算子を指定するユーザー(ド キュメント ストアを検索するときなど) は、\"searchMode=all\"で得られるブール クエリの構造の方が直感的にわかりやすいかもしれません。\"searchMode\"と演算\\n\\n<!-- PageHeader=\"子の相互作用について詳しくは、「Simple クエリ構文」に関するページをご覧く ださい。\" -->\\n\\n\\n#### 第 2 段階:字句解析\\n\\nクエリ ツリーが構築された後、“単語検索\"と\"フレーズ検索\"のクエリがアナライザー によって加工されます。アナライザーは、パーサーから渡されたテキスト入力を受け 取ってそのテキストを加工してから、トークン化した語句をクエリ ツリーに組み入れ ます。\\n\\n最も一般的な字句解析は、特定の言語に固有の規則に従って検索語を変換する\\\\*言語分 析です。\\n\\n● 検索語を単語の原形にします。\\n\\n● ストップワード、つまり検索上の重要性がさほど高くない単語(英語であれば \"the\"や \"and\") を削除します\\n\\n● 複合語をその構成要素に分解します。\\n\\n● 単語の大文字を小文字に変換します。\\n\\n通常はこれらの操作をひととおり適用することで、ユーザーによって入力されたテキス トと、インデックスに格納されている語句との相違点が取り除かれます。こうした操 作はテキスト処理の範囲を超えており、言語そのものに対する深い知識が必要となりま す。この言語知識のレイヤーを追加するために、Azure Cognitive Search は、Lucene と Microsoft から提供されているさまざまな言語アナライザーに対応しています。\\n\\n\\n##### 1 注意\\n\\n解析要件は、実際のシナリオによって大きく異なります。ごく最低限で済む場合 もあれば、膨大な作業が必要となる場合もあります。字句解析の難易度は、あら かじめ定義されているいずれかのアナライザーを選択するか、カスタム アナライ ザーを独自に作成するかによって決まります。アナライザーは、検索可能なフィ ールドにその適用対象が限定されており、フィールド定義の一環として指定され ます。これにより、フィールドごとに多様な字句解析を行うことができます。指 定されなかった場合は、“標準\"の Lucene アナライザーが使用されます。\\n\\nこの例では、解析前の最初のクエリ ツリーに \"Spacious,\"という語句があり、大文字の \"S\" とコンマはクエリ パーサーによって検索語の一部として解釈されています(コンマ はクエリ言語の演算子とは見なされません)。\\n\\n既定のアナライザーは、この語句を加工する際、\"ocean view\" と\"spacious\" に含まれ ている大文字を小文字に変換し、さらにコンマを削除します。変更後のクエリ ツリー\\n:unselected: :selected:\\n<!-- PageHeader=\"は次のようになります。\" -->\\n\\n<figure>\\n\\n![](figures/28)\\n\\n<!-- FigureContent=\"Boolean query Should Should Must Term Query Prefix Query Phrase Query spacious air-condition \"ocean view\"\" -->\\n\\n</figure>\\n\\n\\n\\n#### アナライザーの動作テスト\\n\\nアナライザーの動作は、Analyze API を使ってテストすることができます。解析したい テキストを入力すると、指定したアナライザーからどのような語句が生成されるかを確 認できます。たとえば、標準アナライザーで \"air-condition\" というテキストがどのよ うに加工されるかを確認するには、次のように要求します。\\n\\nJSON\\n\\n{\\n\\n\"text\": \"air-condition\", \"analyzer\": \"standard\" }\\n\\n標準アナライザーは、入力テキストを次の 2 つのトークンに分解します。その際、開 始オフセットと終了オフセット(一致部分の強調表示に使用される)やその位置(フレー ズの照合に使用される) など、各種の属性を使ってそれらに注釈を付けます。\\n\\n<figure>\\n\\n![](figures/29)\\n\\n<!-- FigureContent=\"JSON { \"tokens\": [ { \"token\": \"air\", \"startOffset\": 0, \"endOffset\": 3, \"position\": 0 }, { \"token\": \"condition\", \"startOffset\": 4, \"endOffset\": 13, \"position\": 1 } ] }\" -->\\n\\n</figure>\\n\\n\\n<!-- PageFooter=\"字句解析の例外\" -->\\n\\n字句解析が適用されるのは、語句全体を必要とする種類の検索(単語検索とフレーズ検 索) だけです。語句全体を必要としない種類の検索(プレフィックス検索、ワイルドカ ード検索、正規表現検索など) やあいまい検索には適用されません。前出の例の air- condition\\\\* という語を使ったプレフィックス検索も含め、こうした種類の検索は、解 析段階を経ずに直接クエリ ツリーに追加されます。こうした種類の検索語に対して適 用される変換は、大文字から小文字への変換だけです。\\n\\n\\n##### 第3 段階:文書検索\\n\\nここでいう文書検索とは、一致する語句がインデックスに存在する文書を見つけること です。この段階は、例を使用するとよくわかります。まず、次のような単純なスキー マを使用した hotels というインデックスを考えてみましょう。\\n\\n\\n###### JSON\\n\\n{ \"name\": \"hotels\", \"fields\": [\\n\\n{ \"name\": \"id\", \"type\": \"Edm. String\", \"key\": true, \"searchable\" : false }, { \"name\": \"title\", \"type\": \"Edm. String\", \"searchable\": true }, { \"name\": \"description\", \"type\": \"Edm. String\", \"searchable\": true } ]\\n\\n}\\n\\nさらに、このインデックスには、以下の4 つの文書が追加されているとします。\\n\\nJSON\\n\\n{\\n\\n\"value\": [ { \"id\": \"1\", \"title\": \"Hotel Atman\", \"description\": \"Spacious rooms, ocean view, walking distance to the beach. \"\\n\\n}, { \"id\": \"2\", \"title\": \"Beach Resort\", \"description\": \"Located on the north shore of the island of Kaua\\'i. Ocean view.\" },\\n\\n{ \"id\": \"3\", \"title\": \"Playa Hotel\", \"description\": \"Comfortable, air-conditioned rooms with ocean view. \"\\n\\n},\\n\\n{\\n\\n\"id\": \"4\", \"title\": \"Ocean Retreat\", \"description\": \"Quiet and secluded\"\\n\\n}\\n\\n]\\n\\n}\\n\\n\\n## 語句のインデックス作成方法\\n\\n検索を理解するには、インデックス作成の基本をいくつかを把握しておくと役立ちま す。保存の単位は転置インデックスで、検索可能なフィールドごとに1つ存在しま す。転置インデックス内には、全文書から抽出されたすべての語句を並べ替えたリス トが存在します。それぞれの語句は、それが出現する一連の文書に対応付けられてい ます(以下の例を参照)。\\n\\n転置インデックスに含める語句を得るために、検索エンジンは、クエリの加工時と同様 の字句解析を文書の内容に対して実行します。\\n\\n1\\\\. \"テキスト入力\"はアナライザーに渡され、小文字への変換や句読点の削除など、 アナライザーの構成に応じた処理が行われます。\\n\\n2\\\\. \"トークン\"は字句解析の出力です。\\n\\n3\\\\. \"用語\" はインデックスに追加されます。\\n\\n検索語の体裁をインデックスに登録されている語句の体裁と合わせるために、通常は検 索操作とインデックス作成操作に同じアナライザーが使用されますが、必ずしも同じで ある必要はありません。\\n\\n\\n### 注意\\n\\nAzure Cognitive Search では、追加の indexAnalyzer および searchAnalyzer フィー ルド パラメーターを使用して、インデックス作成と検索に別々のアナライザーを 指定することができます。指定しなかった場合、 analyzer プロパティで設定され たアナライザーが、インデックス作成と検索の両方に使用されます。\\n\\n\\n## 文書サンプルの転置インデックス\\n\\nもう一度先ほどの例を見てみましょう。title フィールドの転置インデックスは、次の ようになります。\\n\\n| 任期 | ドキュメント リスト |\\n| - | - |\\n| atman | 1 |\\n\\n| 任期 | ドキュメント リスト |\\n| - | - |\\n| beach | 2 |\\n| hotel | 1、3 |\\n| ocean | 4 |\\n| playa | 3 |\\n| resort | 3 |\\n| retreat | 4 |\\n\\ntitle フィールドの場合、hotel だけが 2 つの文書(1 と3) に出現します。\\n\\ndescription フィールドのインデックスは次のようになっています。\\n\\n| 任期 | ドキュメント リスト |\\n| - | - |\\n| air | 3 |\\n| and | 4 |\\n| beach | 1 |\\n| conditioned | 3 |\\n| comfortable | 3 |\\n| distance | 1 |\\n| island | 2 |\\n| kaua\\'i | 2 |\\n| located | 2 |\\n| north | 2 |\\n| ocean | 1, 2, 3 |\\n| of | 2 |\\n| on | 2 |\\n| 通知の停止 | 4 |\\n| 会議室 | 1, 3 |\\n| secluded | 4 |\\n\\n| 任期 | ドキュメント リスト |\\n| - | - |\\n| shore | 2 |\\n| spacious | 1 |\\n| the | 1、2 |\\n| to | 1 |\\n| 表示 | 1, 2, 3 |\\n| walking | 1 |\\n| with | 3 |\\n\\n\\n### インデックスが作成された語句に対する検索語の照合\\n\\n上記の転置インデックスを踏まえてサンプル クエリに戻り、一致する文書がどのよう に検索されるかを見ていきましょう。最終的なクエリ ツリーは、次のようになってい たことを思い出してください。\\n\\n<figure>\\n\\n![](figures/30)\\n\\n<!-- FigureContent=\"Boolean query Should Should Must Term Query spacious Prefix Query Phrase Query air-condition \"ocean view\"\" -->\\n\\n</figure>\\n\\n\\nクエリの実行中、検索可能なフィールドに対して個々の検索が別々に実行されます。\\n\\n● 単語検索 \"spacious\" と一致する文書は1件です(Hotel Atman)。\\n\\n● プレフィックス検索 \"air-condition\\\\*\" と一致する文書はありません。\\n\\nこれは、場合によっては開発者の混乱を招く動作です。\"air-conditioned\" という 語句はこの文書内に存在しますが、既定のアナライザーによって2 つの単語に分 割されています。部分的な語句を含むプレフィックス クエリは、解析されないこ とに注意してください。そのため、転置インデックスで、\"air-condition\" という プレフィックスを含む語句を検索しても見つかりません。\\n\\n● フレーズ検索 “ocean view\" では、\"ocean\" と\"view\"という 2 つの語句が検索さ れ、元の文書内で両者が近いかどうかがチェックされます。文書 1、2、3の description フィールドに、この検索との一致が見つかります。文書 4の title に は ocean という語が存在しますが、一致とは見なされないことに注意してくださ い。検索対象は \"ocean view\" というフレーズであって、個々の単語ではありませ ん。\\n\\n# 1 注意\\n\\n特定のフィールドを searchFields パラメーター(検索要求の例を参照) で指定しな い限り、検索クエリは、Azure Cognitive Search インデックス内の検索可能なすべ てのフィールドに対して個別に実行されます。選択したフィールドのいずれかで 一致する文書が返されます。\\n\\n全体として、この例のクエリの場合、一致する文書は 1、2、3 です。\\n\\n\\n## 第 4 段階: スコア付け\\n\\n検索結果セット内のすべての文書には、関連度スコアが割り当てられます。関連度ス コアの機能は、ユーザーからの問い合わせ(検索クエリ)に対して最適解となる文書に 対し、相対的に高いランクを与えることです。このスコアは、一致した語句の統計学 的な特性に基づいて計算されます。スコア付けの式の核となるのは TF/IDF (Term Frequency-Inverse Document Frequency)△ です。TF/IDF は、出現頻度の低い語句と高 い語句を含んだ検索において、出現頻度の低い語句を含んだ結果に、より高いランクを 与えます。たとえば、Wikipedia の記事をすべて含んだ架空のインデックスでは、the president というクエリに一致した文書のうち、president で一致した文書の方が the で 一致した文書よりも関連性が高いと見なされます。\\n\\n\\n## スコア付けの例\\n\\n冒頭のサンプル クエリで見つかった 3 つの文書を思い出してください。\\n\\nsearch=Spacious, air-condition\\\\* +\"Ocean view\"\\n\\nJSON\\n\\n{\\n\\n\"value\": [\\n\\nY\\n\\n\"@search.score\": 0.25610128,\\n\\n\"id\": \"1\",\\n\\n\"title\": \"Hotel Atman\",\\n\\n\"description\": \"Spacious rooms, ocean view, walking distance to the beach. \"\\n\\n},\\n\\n{ \"@search.score\": 0.08951007,\\n\\n\"id\": \"3\",\\n:selected: :unselected: :unselected:\\n\"title\": \"Playa Hotel\",\\n\\n},\\n\\n\"description\": \"Comfortable, air-conditioned rooms with ocean view.\"\\n\\n{\\n\\n\"@search.score\": 0.05967338,\\n\\n\"id\": \"2\",\\n\\n\"title\": \"Ocean Resort\",\\n\\n\"description\": \"Located on a cliff on the north shore of the island of Kauai. Ocean view.\" }\\n\\n]\\n\\n}\\n\\n文書1は、クエリに対して最も高い関連度で一致しています。なぜなら、spacious と いう単語と ocean view という必須のフレーズの両方が description フィールドに出現す るためです。その他の 2 つの文書は、ocean view しか一致していません。しかし文書 2 と文書 3は、クエリに対して同じように一致しているにもかかわらず、関連度スコア が異なるのはなぜでしょうか。これは、スコア付けの式の構成要素が TF/IDF だけでは ないためです。この場合、文書 3 の方が、description が短いために、少しだけ高いス コアが割り当てられています。フィールドの長さやその他の要因が関連度スコアに与 える影響については、Lucene の実際に役立つスコア付けの式☑に関するページを参照 してください。\\n\\n一部の検索の種類(ワイルドカード、プレフィックス、正規表現)は、文書全体のスコ :unselected: アに対して常に一定のスコアをもたらします。これによって、ランクには影響を与え ずに、クエリ拡張によって見つかった一致を結果に反映することができます。\\n\\nこのことが重要である理由を例で説明します。ワイルドカード検索(プレフィックス検 索を含む) は、本質的に解釈があいまいです。入力されるのは文字列の一部であり、ま ったく異なる、膨大な数の語句と一致する可能性があるからです(\"tour\\\\*\" と入力した場 合、\"tours\"、\"tourettes\"、\"tourmaline\" などとの一致が検出されます)。こうした結果 の性質上、用語の相対的な重みを適切に推測することができません。そのため、ワイ ルドカード検索、プレフィックス検索、正規表現検索では、結果のスコア付けを行う際 に、語句の出現頻度が無視されます。部分的な語句と完全な語句とを含んだ複数の構 成要素から成る検索要求では、予期しない一致が偏重されないよう、部分的な入力から 得られた結果については、定数スコアを割り当てたうえで反映されます。\\n\\n\\n### 関連性のチューニング\\n\\nAzure Cognitive Search の関連度スコアは、次の 2 とおりの方法でチューニングできま す。\\n\\n1\\\\. スコアリング プロファイル: ランク付けされた結果リストにおいて、一連のルー ルに基づく重みを文書に与えます。このページの例では、title フィールドに一致\\n:selected: :unselected:\\nの見つかった文書の方が、description フィールドに一致の見つかった文書よりも 関連性が高いと見なすことができます。加えて、仮にホテルごとの料金フィール ドをインデックスに含めた場合、料金の低い方の文書に高い重みを与えることも 可能です。スコアリング プロファイルを検索インデックスに追加する方法の詳細 について学習します。\\n\\n2\\\\. 項目ブースト(Full Lucene クエリ構文のみで使用可能): クエリ ツリーの任意の構 成要素に適用できるブースト演算子^が用意されています。このページの例で は、air-condition\\\\* というプレフィックスで検索する代わりに、air-\\n\\ncondition^2||air-condition\\\\* のように単語検索にブーストを適用することもできま す。そうすれば、air-condition で完全一致する語句と、プレフィックスで一致す る語句との両方を検索したうえで、完全一致の語句で一致した文書の方に、より 高いランクを与えることができます。クエリでの語句ブーストの詳細について学 習します。\\n\\n\\n## 分散されたインデックスにおけるスコア付け\\n\\nAzure Cognitive Search のすべてのインデックスは自動的に複数のシャードに分割され ます。これにより、Microsoft は、サービスのスケールアップまたはスケールダウンの 間に、複数のノードにインデックスをすばやく分散することができます。検索要求は 送信されると、各シャードに対して別々に送られます。その後、各シャードから得ら れた結果がマージされ、スコア順に並べ替えられます(他に並べ替えが定義されていな い場合)。スコアリング関数は、シャード内のすべてのドキュメントで逆ドキュメント 頻度に対して検索語頻度を重み付けするものであり、すべてのシャードで重み付けする ものではないことを知ることが重要です。\\n\\nつまり、まったく同じ文書でも、それらが異なるシャードに存在していれば、関連度ス コアが異なる“可能性がある\"ということです。さいわい、インデックス内の文書数が 増えるにつれて語句の分布が平準化され、そのような差異は総じて消失します。文書 がどのシャードに配置されるかを推測することは不可能です。しかし文書は、そのキ ーが変化しなければ、常に同じシャードに割り当てられます。\\n\\n一般に、順序の不変性が重要となる場合、文書を並べ替えるための特性として、文書の スコアはあまり適していません。たとえば、まったく同じスコアの文書が 2 つあると して、同ークエリを繰り返し実行したときに、どちらの文書が上位にランク付けされる かを毎回確実に予測することができません。文書スコアは、検索結果群における特定 の文書の相対的な関連度の高さを測る、おおよその目安としてのみ使用してください。\\n\\nまとめ\\n\\n商用検索エンジンが多くの人々の支持を得たことで、プライベート データに対するフ ルテキスト検索への期待が高まってきました。ほぼすべての検索について言えること ですが、語句の綴りが間違っていたり不完全であったりしても自分の意図がきちんと反 映されるほどの利便性を、私たちは検索エンジンに求めるようになっています。指定 してもいない同義語やほぼ同等の語句に基づいた検索結果が得られることさえ、当然の ように感じてしまうほどです。\\n\\n技術的な観点でいえばフルテキスト検索はきわめて複雑で、洗練された言語分析と検索 語の加工(関連性の高い結果を得るために検索語を抽出、展開、変換する処理) への秩 序立ったアプローチとが要求されます。そうした本質的な複雑さもあって、検索結果 :unselected: はさまざまな要因によって左右されます。フルテキスト検索のメカニズムをしっかり 理解しておけば、予期しない結果に対処しようとする際に、はっきりとその効果を実感 できるでしょう。\\n\\nこの記事では、Azure Cognitive Search の観点からフルテキスト検索について詳しく見 てきました。ここで身に付けた知識が、検索時に遭遇しやすい問題の原因や解決策を 判断するうえでの一助となればさいわいです。\\n\\n\\n## 次のステップ\\n\\n● サンプル インデックスを構築し、さまざまな検索を試してその結果を確認しま す。詳しい手順については、ポータルでのインデックスの構築と照会に関するペ ージを参照してください。\\n\\n● Search Documents の例に関するセクションや単純なクエリ構文で紹介されている 他のクエリ構文をポータルの検索エクスプローラーで試します。\\n :unselected:\\n● 検索アプリケーションにおけるランク付けをチューニングする方法については、 スコアリング プロファイルに関するページを参照してください。\\n\\n● 言語に固有の字句解析器を適用する方法について書かれた記事を参照します。\\n\\n● 特定のフィールドに対して最小限の処理または特殊な処理を適用するためのカス タム アナライザーを構成します。\\n\\n\\n## 関連項目\\n\\nSearch Documents REST API\\n\\n単純なクエリ構文\\n\\nFull Lucene クエリ構文\\n\\n検索結果の処理方法\\n\\n# Azure Al Search のベクター\\n\\n[アーティクル]·2024/04/09\\n\\nベクトル検索とは、コンテンツの数値表現に対するインデックス付けとクエリの実行を サポートする情報取得のアプローチです。コンテンツはプレーンテキストではなく数 :unselected: :unselected: 値であるため、照合はクエリ ベクトルに最も類似したベクトルに基づいて行われま す。これにより、次のシナリオでの照合が可能になります。\\n\\n● 意味的または概念的な類似性 (\"dog\"と\"canine\"は概念的には似ているが言語的 には異なる)\\n\\n● 多言語コンテンツ(英語では \"dog\"、ドイツ語では \"hund\")\\n\\n● 複数のコンテンツ タイプ(プレーンテキストの \"dog\"と画像ファイル内の犬の写 真)\\n\\nこの記事では、Azure AI 検索でのベクトルの概要について説明します。また、他の Azure サービスとの統合についても説明し、ベクトル検索の開発に関連する用語と概念 についても説明します。\\n\\n最初にこの記事を読んで基礎知識を得ることをお勧めしますが、それはかまわないので すぐに使いたいという方は、これらの手順を実行してください。\\n\\n✔ インデックス用の埋め込みを提供するか、インデクサー パイプラインで埋め込み (プレビュー)を生成する\\n\\n✔ ベクトル インデックスを作成する\\n\\n✔ ベクトル クエリの実行\\n\\nベクトル クイックスタートまたは GitHub のコード サンプルマを使用して開始するこ ともできます。\\n\\n\\n## ベクトル検索をサポートできるシナリオ\\n\\nベクトル検索には次のようなシナリオがあります。\\n\\n● 類似性検索。OpenAl 埋め込みなどの埋め込みモデルや SBERT などのオープン ソ ース モデルを使用してテキストをエンコードし、やはりベクトルとしてエンコー ドされたクエリを使用してドキュメントを取得します。\\n\\n● さまざまなコンテンツ タイプ(マルチモーダル)で検索します。画像とテキスト をマルチモーダル埋め込み(たとえば、Azure OpenAl の OpenAl CLIP ☑ や GPT-4 Turbo with Vision を使用) を使用してエンコードし、両方のコンテンツ タイプの ベクトルで構成される埋め込みスペースをクエリします。\\n:selected: :selected: :selected: :unselected: :unselected: :unselected:\\n● ハイブリッド検索。Azure Al 検索のハイブリッド検索では、同じ要求でベクトル およびキーワード クエリの実行を参照します。ベクトル サポートはフィールド レベルで実装されており、ベクトル フィールドと検索可能なテキスト フィールド の両方を含むインデックスがあります。クエリは並列で実行され、結果は 1 つの 応答にマージされます。必要に応じて、セマンティック ランク付けを追加して、 Bing を動作させているのと同じ言語モデルを使用して、L2 の再ランク付けによっ て精度をさらに高めます。\\n\\n● 多言語検索。 複数の言語でトレーニングされたモデルとチャット モデルを埋め込 んで、ユーザーの母国語での検索エクスペリエンスを提供できます。翻訳をより 詳細に制御する必要がある場合は、ハイブリッド検索シナリオで Azure Al Search が非ベクトル コンテンツに対して提供する多言語機能を追加できます。\\n\\n● フィルター選択されたベクトル検索。クエリ要求にはベクトル クエリとフィルタ 一式を含めることができます。フィルターはテキスト フィールドと数値フィール ドに適用され、メタデータ フィルターに役立ち、フィルター条件に基づいて検索 結果を含めたり除外したりするのに役立ちます。ベクトル フィールド自体はフィ ルター処理できませんが、フィルター可能なテキスト フィールドまたは数値フィ ールドを設定できます。検索エンジンは、ベクトル クエリの実行前または実行後 にフィルターを処理できます。\\n\\n● ベクトル データベース。Azure Al Search には、クエリを実行するデータが格納 されます。長期メモリやナレッジ ベース、あるいは取得拡張生成(RAG) アーキテ クチャロやベクトルを使用するあらゆるアプリケーションの基礎データが必要な 場合は、純粋なベクトル ストアとして使用します。\\n\\n\\n## Azure Al Searchでのベクトル検索のしくみ\\n\\nベクトルのサポートには、検索インデックスからのベクトル埋め込みのインデックス作 成、格納、クエリが含まれます。\\n\\n次の図は、ベクトル検索のインデックス作成とクエリのワークフローを示しています。\\n :unselected:\\n<figure>\\n\\n![](figures/31)\\n\\n<!-- FigureContent=\"Vector representation Approximate nearest neighbor Vector Data sources representation App/UX Images Azure Cognitive Search [ 2, 3, 4, 5 ] Transform into embedding Audio [ -2, -1, 0, 1 ] Transform into embeddings [ 2, 3, 4, 5 ] Video [ 6, 7,8, 9 ] · . Results + Text\" -->\\n\\n</figure>\\n\\n\\nインデックス作成側では、Azure Al Search はベクトル埋め込みを受け取り、ニアレス トネイバー アルゴリズム を使用して、同様のベクトルをインデックス内の近い場所に\\n:selected: :selected: :unselected: :selected:\\n配置します。内部では、各ベクトル フィールドのベクトル インデックスが作成されま す。\\n\\nソース コンテンツから埋め込みを Azure Al Search に取り込む方法は、アプローチとプ レビュー機能が使用できるかどうかによって異なります。OpenAl、Azure OpenAl、お よび任意の数のプロバイダーのモデルを使用して、テキスト、画像、モデルでサポート されているその他のコンテンツ タイプなど、さまざまなソース コンテンツに対して埋 め込みをベクトル化または生成できます。その後、事前にベクトル化したコンテンツ をベクトル ストアのベクトル フィールドにプッシュすることができます。これが一般 公開されているアプローチです。プレビュー機能を使用できる場合、Azure Al Search はインデクサー パイプラインで統合されたデータ チャンクとベクトル化を提供しま す。リソース(エンドポイントと Azure OpenAl への接続情報) は引き続き提供されま すが、Azure Al Search はすべての呼び出しを行い、移行を処理します。\\n\\nクエリ側では、クライアント アプリケーションで、通常はプロンプト ワークフローを 使用して、ユーザーからクエリ入力を収集します。その後、入力をベクトルに変換す るエンコード手順を追加し、Azure Al Search 上のインデックスにベクトル クエリを送 信して類似性検索を行うことができます。インデックス作成と同様に、統合ベクトル 化(プレビュー)をデプロイして、質問をベクトルに変換できます。どちらの方法で :unselected: も、Azure Al Search では、要求された k ニアレスト ネイバー(kNN) を含むドキュメン トが結果に返されます。\\n\\nAzure Al Search は、ベクトル検索とキーワード検索を並行して実行するハイブリッド シナリオをサポートしており、統合された結果セットを返しますが、これはしばしば、 ベクトル検索やキーワード検索のみよりも優れた結果を提供します。ハイブリッドの 場合、ベクトル コンテンツと非ベクトル コンテンツは、並列して実行されるクエリに 対して、同じインデックスに取り込まれます。\\n\\n\\n## 可用性と料金\\n\\nベクトル検索は、すべてのリージョンのすべての Azure Al Search レベルの一部として 追加料金なしで利用できます。\\n\\n2024 年4月3日以降に作成された新しいサービスでは、ベクトル インデックスのため により高いクォータをサポートしています。\\n\\nベクトル検索は次で利用できます。\\n\\n● データのインポートとベクトル化ウィザードを使用した Azure portal\\n\\n● Azure REST API、バージョン 2023-11-01\\n\\n● .NET☑、Python☑、JavaScript△ 用の Azure SDK\\n\\n● Azure Al Studio や Azure OpenAl Studio などその他の Azure オファリング。\\n:unselected:\\n### 1 注意\\n\\n2019 年1月1日より前に作成された一部の古い検索サービスは、ベクトル ワー クロードをサポートしないインフラストラクチャにデプロイされています。ベク :unselected: :unselected: トル フィールドをスキーマに追加しようとしてエラーが表示された場合、それは サービスが古いためです。このような場合は、ベクトル機能を試すために新しい 検索サービスを作成する必要があります。\\n\\n\\n## Azure の統合と関連サービス\\n\\nAzure Al Search は、Azure AI プラットフォーム全体で深く統合されています。次の表 に、ベクトル ワークロードで役立ついくつかの要素を示します。\\n\\n〔〕 テーブルを展開する\\n\\n| Product | 統合 |\\n| - | - |\\n| Azure OpenAl Studio | データ プレイグラウンドとのチャットで、[独自のデータを追加する]は、 データの基盤と会話型検索のために Azure Al Search を使用します。これ は、データとチャットするための最も簡単かつ高速なアプローチです。 :unselected: |\\n| Azure OpenAl | Azure OpenAl には埋め込みモデルとチャットモデルが用意されていま す。デモとサンプルでは、text-embedding-ada-002 を対象とします。テ キスト用の埋め込みを生成するには、Azure OpenAl をお勧めします。 |\\n| Azure Al Services | Image Retrieval Vectorize Image API (プレビュー)では、画像コンテンツの ベクトル化がサポートされます。画像用の埋め込みを生成するには、この API をお勧めします。 |\\n| Azure データ プラ ットフォーム: Azure Blob Storage, Azure Cosmos DB | インデクサーを使用してデータ インジェストを自動化し、統合ベクトル化 (プレビュー) を使用して埋め込みを生成できます。Azure Al Search では、 Azure Blob インデクサーと Azure Cosmos DB for NoSQL インデクサーの 2 つのデータ ソースからベクトル データのインデックスを自動的に作成で きます。詳細については、「検索インデックスにベクトル フィールドを 追加する」を参照してください。 |\\n\\nこれは、LangChain△ などのオープンソース フレームワークでも一般的に使用されて います。\\n\\n\\n## ベクトル検索の概念\\n\\nベクトルを初めて使用する場合、このセクションではいくつかの主要な概念について説 明します。\\n\\n## ベクトル検索について\\n\\nベクトル検索は、ドキュメントとクエリがプレーン テキストではなくベクトルとして 表現される場合の情報取得の方法です。ベクトル検索では、機械学習モデルがソース 入力(テキスト、画像、その他のコンテンツ) のベクトル表現を生成します。コンテン :unselected: ツの数学表現を使用することによって、検索シナリオの共通基盤が提供されます。 す べてがベクトルであれば、関連する元のコンテンツがクエリとは異なるメディアや言語 であっても、クエリはベクトル空間で一致するものを見つけることができます。\\n\\n\\n## ベクトル検索を使用する理由\\n\\n検索可能なコンテンツがベクトルとして表されると、クエリは類似するコンテンツ内の 近い一致を見つけることができます。ベクトル生成に使用される埋め込みモデルは、 どの単語と概念が類似しているかを認識し、結果のベクトルを埋め込み空間内で近くに 配置します。たとえば、\"クラウド\"と\"霧\" に関するベクトル化されたソース ドキュメ ントは、意味的に類似しているため、構文上の一致ではない場合も“霧\" に関するクエ リで表示される可能性が高くなります。\\n\\n\\n## 埋め込みベクトル化\\n\\n“埋め込み\"は、テキストのセマンティックな意味や画像などの他のコンテンツの表現 を読み取る機械学習モデルによって作成された、コンテンツまたはクエリの特定の種類 のベクトル表現です。自然言語機械学習モデルは、単語間のパターンや関係を識別す るために、大量のデータでトレーニングされます。トレーニング中に、\"エンコーダ ー\"と呼ばれる中間ステップで、入力を実数のベクトルとして表現する方法を学習しま す。トレーニングが完了すると、中間ベクトル表現がモデルの出力になるように、こ れらの言語モデルを変更できます。結果として得られる埋め込みは高次元ベクトルで あり、埋め込みの概要(Azure OpenAl) に関する記事で説明されているように、同じよ うな意味を持つ単語がベクトル空間で互いに近くなります。\\n\\n関連する情報の取得におけるベクトル検索の有効性は、ドキュメントとクエリの意味を 結果のベクトルに抽出する埋め込みモデルの有効性に依存します。最適なモデルは、 それらが代表するデータの種類によって適切にトレーニングされています。Azure OpenAl text-embedding-ada-002 などの既存のモデルを評価したり、問題領域で直接 トレーニングされた独自のモデルを使用したり、汎用モデルを微調整したりできます。 Azure Al Search では、選ぶモデルに制約が課されないため、データに最適なものを選 んでください。\\n\\nベクトル検索に対して効果的な埋め込みを作成するには、入力サイズの制限を考慮する ことが重要です。埋め込みを生成する前に、データをチャンクするためのガイドライ\\n:unselected: :unselected: :selected:\\nンに従うことをお勧めします。このベストプラクティスのおかげで、埋め込みによっ て関連情報が正確に読み取られ、より効率的なベクトル検索が可能になります。\\n\\n\\n## 埋め込み空間とは\\n\\n\"埋め込み空間\"は、ベクトル クエリのコーパスです。検索インデックス内では、埋め 込み空間は、同じ埋め込みモデルからの埋め込み値が設定されているすべてのベクトル フィールドです。機械学習モデルでは、個々の単語、語句、またはドキュメント(自然 言語処理の場合)、画像、またはその他の形式のデータを、高次元空間の座標を表す実 数のベクトルで構成される表現にマッピングすることで、埋め込み空間を作成します。 この埋め込みスペースでは、類似項目は近くに配置され、異なる項目は離れた場所に配 置されます。\\n\\nたとえば、さまざまな種類の犬について説明するドキュメントは、埋め込み空間で互い に近くに集められます。猫に関するドキュメントは互いに近くに集まりますが、犬の クラスターから遠く離れており、それでも動物としては近くになります。クラウド コ :unselected: ンピューティングなどの異なる概念は、はるかに遠く離れています。実際には、これ らの埋め込み空間は抽象的で、人間が解釈できる明確に定義された意味はありません が、中核となる概念は同じです。\\n\\n\\n## ニアレストネイバー検索\\n\\nベクトル検索では、検索エンジンは埋め込みスペース内のベクトルをスキャンして、ク エリ ベクトルに最も近いベクトルを識別します。この手法は“ニアレストネイバー検 索\" と呼ばれます。△ ニアレストネイバーは、項目間の類似性を定量化するのに役立ち ます。ベクトルの類似性が高い場合は、元のデータも同様であることを示します。高 速なニアレストネイバー検索を容易にするために、検索エンジンでは最適化を実行する か、データ構造およびデータ パーティション分割を使用して検索領域を削減します。\\n\\n各ベクトル検索アルゴリズムは、最小待機時間、最大スループット、再現率、メモリを 最適化する際に、ニアレストネイバーの問題をさまざまな方法で解決します。類似性 を計算するために、類似性メトリックでは距離を計算するためのメカニズムを提供しま す。\\n\\nAzure Al Search では現在、次のアルゴリズムがサポートされています。\\n\\n● Hierarchical Navigable Small World (HNSW): HNSW は、データ分散が不明である か、頻繁に変更される可能性がある、高いリコールと待機時間の短い用途に最適 化された主要な ANN アルゴリズムです。高次元のデータ ポイントを階層グラフ 構造に整理することで、高速でスケーラブルな類似性検索を可能にしながら、検 索精度と計算コストのトレードオフを調整できます。このアルゴリズムでは、高 速ランダム アクセスのためにすべてのデータ ポイントがメモリ内に存在する必要\\n\\nがあるため、このアルゴリズムではベクトル インデックス サイズのクォータが使 用されます。\\n\\n● 完全な K ニアレストネイバー (KNN): クエリ ベクトルとすべてのデータ ポイント の間の距離を計算します。計算負荷が高いので、小規模なデータセットに最適で す。このアルゴリズムではデータ ポイントの高速ランダム アクセスが不要なた め、このアルゴリズムではベクトル インデックス サイズのクォータが使用されま せん。ただし、このアルゴリズムではニアレストネイバーのグローバル セットが :unselected: 提供されます。\\n\\nインデックス定義内で1つ以上のアルゴリズムを指定し、ベクトル フィールドごとに 使用するアルゴリズムを指定できます。\\n\\n● インデックスとフィールドにアルゴリズムを指定するベクトル ストアを作成しま す。\\n\\n● 完全な KNN の場合、2023-11-01、2023-10-01-Preview、または REST API バージ ョンを対象とする Azure SDK ベータ ライブラリを使用します。\\n\\nインデックスの作成時にインデックスの初期化に使用されるアルゴリズムパラメータ ーは不変であり、インデックスの作成後に変更することはできません。ただし、クエ リ時間の特性(efSearch) に影響を与えるパラメーターは変更することができます。\\n\\nさらに、HNSW アルゴリズムを指定するフィールドは、クエリ要求パラメーター \"exhaustive\": true を使用した完全な KNN 検索もサポートします。ただし、その逆は 当てはまりません。 exhaustiveKnn に対してフィールドがインデックス付けされている 場合、効率的な検索’を可能にする追加のデータ構造が存在しないため、クエリで HNSW を使用することはできません。\\n\\n\\n## 近似ニアレストネイバー\\n\\n近似ニアレストネイバー検索(ANN) は、ベクトル空間で一致を検索するためのアルゴ リズムの種類です。この種類のアルゴリズムでは、検索空間を大幅に削減してクエリ 処理を高速化するため、さまざまなデータ構造またはデータ パーティション分割方法 が採用されます。\\n\\nANN アルゴリズムでは、精度がいくらか犠牲になりますが、近似ニアレストネイバー をスケーラブルかつ迅速に取得できるため、最新の情報取得の用途で効率と精度のバラ ンスを取るのに最適です。アルゴリズムのパラメーターを調整して、検索用途のリコ ール、待機時間、メモリ、ディスク フットプリントの要件を微調整できます。\\n\\nAzure Al Search では、ANN アルゴリズムに HNSW が使用されます。\\n:unselected:\\n# 次のステップ\\n\\n● クイックスタートを試す\\n\\n● ベクトル インデックス作成の詳細を確認する\\n\\n● ベクトル クエリの詳細を確認する\\n\\n● Azure Cognitive Search と LangChain: 拡張ベクトル検索機能のシームレスな統合☑\\n\\n# Azure Al Search でベクトルやフルテキ ストを使用したハイブリッド検索\\n\\n[アーティクル]·2024/01/31\\n\\nハイブリッド検索は、フルテキストとベクトル クエリを組み合わせたものです。検索 可能なプレーンテキスト コンテンツと生成された埋め込み両方を含む検索インデック スに対してクエリを実行します。クエリの目的として、ハイブリッド検索とは次のよ うなものです。\\n\\n● search および vectors クエリ パラメータ両方を含む単一のクエリ要求です。\\n\\n● 並行して実行されます。\\n\\n● クエリ応答のマージされた結果の場合、Reciprocal Rank Fusion (RRF) を使用して スコアリングされます。\\n\\nこの記事では、ハイブリッド検索のコンセプト、利点、制限について説明します。こ の 埋め込みビデオ で、ハイブリッド検索が高品質のチャット スタイル アプリやコパ イロット アプリにどのように貢献するかの説明と短いデモをご覧ください。\\n :unselected:\\n\\n## ハイブリッド検索が機能するしくみ\\n\\nAzure Al Search では、埋め込みを含むベクター フィールドをテキスト フィールドと数 値フィールドと共に使用できるため、並列で実行されるハイブリッド クエリを作成で きます。ハイブリッド クエリでは、単一の検索要求で、フィルター処理やファセット 処理、並べ替え、スコアリング プロファイル、セマンティック ランク付けといった既 存の機能を利用できます。\\n\\nハイブリッド検索では、BM25 や HNSW などの異なるランク付け機能を利用し、フル テキストおよびベクトル クエリの両方の結果を組み合わせます。逆ランク 融合(RRF) アルゴリズムによって結果がマージされます。クエリ応答は、RRF を使用して各クエ リから最も関連性の高い一致を選択する結果セットを1 つだけ提供します。\\n\\n\\n## ハイブリッド クエリの構造\\n\\nハイブリッド検索は、プレーン テキストと数値、地理空間検索の地理座標、テキスト のチャンクの数学的表現のためのベクトルなど、さまざまな データ型のフィールドを 含む検索インデックスを持つことを前提とします。ベクトル クエリにより、オートコ ンプリートや検索候補などのクライアント側のインタラクションを除き、Azure Al Search のほぼすべてのクエリ機能を使用できます。\\n\\n典型的なハイブリッド クエリは、次のようになります(簡潔にするためベクトルを省略 しています)。\\n\\nHTTP\\n\\nPOST https: //{{searchServiceName} } . search.windows.net/indexes/hotels-vector- quickstart/docs/search?api-version=2023-11-01\\n\\ncontent-type: application/JSON {\\n\\n\"count\": true,\\n\\n\"search\": \"historic hotel walk to restaurants and shopping\",\\n\\n\"select\": \"HotelId, HotelName, Category, Description, Address/City, Address/StateProvince\",\\n\\n\"filter\": \"geo.distance(Location, geography\\' POINT(-77.03241 38.90166)\\') le 300\",\\n\\n\"facets\": [ \"Address/StateProvince\"],\\n\\n\"vectors\": [ {\\n\\n\"value\": [ <array of embeddings> ] \"k\": 7,\\n\\n\"fields\": \"DescriptionVector\"\\n\\n},\\n\\n{ \"value\": [ <array of embeddings> ] \"k\": 7, \"fields\": \"Description\\\\_frVector\" }\\n\\n], \"queryType\": \"semantic\", \"queryLanguage\": \"en-us\", \"semanticConfiguration\": \"my-semantic-config\" }\\n\\n\\n## 重要なポイントは次のとおりです。\\n\\n. search はフルテキスト検索クエリを指定します。\\n\\n· vectors はベクトル クエリです。複数設定して複数のベクトル フィールドを対象 とすることができます。埋め込みスペースに多言語コンテンツが含まれる場合、 言語アナライザーや翻訳を介さずに、ベクトル クエリで一致を検出できます。\\n\\n● select は結果で返すフィールドを指定します。人間が判読できるテキスト フィ ールドを指定することもできます。\\n\\n. filters は地理空間の検索、またはその他の包含条件や除外条件(駐車場を含める かどうかなど) を指定できます。この例における地理空間のクエリでは、ワシン トン D.C. から半径 300 キロ以内にあるホテルを検出します。\\n\\n● facets はハイブリッド クエリで返された結果のファセットバケットの計算に使 用できます。\\n\\n● queryType=semantic はセマンティック ランク付けを呼び出します。機械読解を適 用し、より関連性の高い検索結果を表示させます。\\n\\nフィルターとファセットは、フルテキスト検索に使用した逆インデックスおよびベクト ル検索に使用したベクトル インデックスとは異なるインデックス内のデータ構造を対 象とします。そのため、フィルターとファセット処理が実行されると、検索エンジン の応答では、ハイブリッド検索結果に操作上の結果が適用されます。\\n\\nこのクエリには orderby がないことがかわります。明示的な並べ替え順序は、関連性 でランク付けされた結果をオーバーライドするため、類似性と BM25 の関連性が必要 な場合は、クエリで並べ替えを省略します。\\n\\n上記のクエリに対する応答は、この例のようになります。\\n\\nHTTP\\n\\n{ \"@odata.count\": 3, \"@search. facets\": {\\n\\n\"Address/StateProvince\": [\\n\\n{ \"count\": 1,\\n\\n\"value\": \"NY\" }, {\\n\\n\"count\": 1, \"value\": \"VA\" }\\n\\n]\\n\\n}, \"value\": [\\n\\n{\\n\\n\"@search.score\": 0.03333333507180214, \"@search.rerankerScore\": 2.5229012966156006, \"HotelId\": \"49\",\\n\\n\"HotelName\": \"Old Carrabelle Hotel\", \"Description\": \"Spacious rooms, glamorous suites and residences, rooftop pool, walking access to shopping, dining, entertainment and the city center.\",\\n\\n\"Category\": \"Luxury\",\\n\\n\"Address\": {\\n\\n\"City\": \"Arlington\", \"StateProvince\": \"VA\" }\\n\\n}, {\\n\\n\"@search.score\": 0.032522473484277725, \"@search.rerankerScore\": 2.111117362976074, \"HotelId\": \"48\", \"HotelName\": \"Nordick\\'s Motel\",\\n\\n\"Description\": \"Only 90 miles (about 2 hours) from the nation\\'s\\n\\ncapital and nearby most everything the historic valley has to offer. Hiking? Wine Tasting? Exploring the caverns? It\\'s all nearby and we have specially priced packages to help make our B&B your home base for fun while visiting the valley.\",\\n\\n\"Category\": \"Boutique\", \"Address\": {\\n\\n\"City\": \"Washington D.C.\",\\n\\n\"StateProvince\": null }\\n\\n}\\n\\n]\\n\\n}\\n\\n\\n## ハイブリッド検索を選択する理由\\n\\nハイブリッド検索は、ベクトル検索とキーワード検索の長所を組み合わせたものです。 ベクトル検索のメリットは、逆インデックスにキーワードの一致がない場合でも、検索 クエリと概念的に似た情報が検索されることです。キーワード検索やフル テキスト検 索のメリットは、精度の高さと、最初の結果の品質を向上させるセマンティック ラン ク付けを適用できる機能です。製品コードや、極めて特殊な専門用語、日付、人の名 前に対するクエリなど、一部のシナリオでは、完全一致を識別できるため、キーワード 検索を使用すると、より良いパフォーマンスを発揮します。\\n\\n実際のデータセットとベンチマーク データセットに対するベンチマーク テストでは、 セマンティック ランク付けを使用したハイブリッド検索の場合、検索の関連性に大き な利点があることが示されています。\\n\\n次のビデオでは、有用な AI 応答を生成するための最適なグラウンディング データが、 ハイブリッド検索でどのように提供されるかについて説明します。\\n\\nhttps://www.youtube-nocookie.com/embed/Xwx1DJ00qCk z7\\n\\n\\n## 関連項目\\n\\nハイブリッド検索とランク付けでベクトル検索の性能を上回る(技術ブログ)☑\\n\\n# Azure Al Search での取得拡張生成(RAG)\\n\\n[アーティクル]·2024/04/22\\n\\n取得拡張生成(RAG) は、グラウンディング データを提供する情報取得システムを追加 することで、ChatGPT などの大規模言語モデル(LLM)の機能を拡張するアーキテクチ ヤです。 情報取得システムを追加すると、応答を作成するときに LLM によって使用さ れるグラウンディング データを制御できます。エンタープライズ ソリューションの場 合、RAG アーキテクチャは、ベクトル化されたドキュメントや画像、およびそのコン テンツの埋め込みモデルがある場合は、その他のデータ形式から取得された“エンター プライズ コンテンツ\"に生成 AI を制限できることを意味します。\\n\\nどの情報取得システムを使用するかによって LLM への入力が決定されるため、この決 定は重要です。情報取得システムは、次の情報を提供する必要があります。\\n\\n● 必要な頻度で、すべてのコンテンツに対して、大規模に読み込んで更新するイン デックス作成戦略。\\n\\n● クエリ機能と関連性のチューニング。システムは、関連する結果を、LLM 入力の トークンの長さの要件を満たすのに必要な短い形式で返す必要があります。\\n\\n· データと操作の両方のセキュリティ、グローバル展開、信頼性。\\n :unselected:\\n● インデックス作成用の埋め込みモデル、および取得のためのチャット モデルまた は言語理解モデルとの統合。\\n\\nAzure Al Search は、RAG アーキテクチャにおける 情報取得のための実証済みのソリュ ーション です。Azure クラウドのインフラストラクチャとセキュリティを備えたイン デックス作成とクエリ機能を提供します。コードやその他のコンポーネントを使用し :unselected: て、財産的価値のあるコンテンツに対する生成 AI のすべての要素を含む包括的な RAG ソリューションを設計できます。\\n\\n\\n## 4 注意\\n\\nCopilot と RAG の概念は初めてですか?「ベクトル検索と、生成 AI アプリの最新 の取得☑」をご覧ください。\\n\\nAzure Al Search を使用した RAG へのアプロー チ\\n\\nMicrosoft は、RAG ソリューションで Azure Al Search を使用するためのいくつかの組 み込み実装を用意しています。\\n\\n● Azure Al Studio。ベクトル インデックスと取得拡張を使用します。\\n\\n● Azure OpenAl Studio。ベクトルの有無にかかわらず、検索インデックスを使用し ます。\\n\\n● Azure Machine Learning。プロンプト フローでベクトル ストアとして検索インデ ックスを使用します。\\n\\nキュレーションされたアプローチを使用すると、簡単に作業を開始できますが、アーキ :unselected: テクチャをより詳細に制御するには、カスタム ソリューションが必要です。これらの テンプレートでは、以下でエンド ツー エンドのソリューションが作成されます。\\n\\n· Python &Z\\n\\n● .NET 心\\n\\n· JavaScript ZZ\\n\\n· Java &\\n\\nこの記事の残りの部分では、Azure Al Search がカスタム RAG ソリューションにどのよ うに適合するかについて説明します。\\n\\n\\n## Azure Al Search のカスタム RAG パターン\\n\\nパターンの大まかな概要は次のとおりです。\\n\\n● ユーザーの質問または要求(プロンプト)から始めます。\\n\\n● Azure Al Search に送信して、関連情報を見つけます。\\n\\n● 上位の検索結果を LLM に送信します。\\n\\n● LLM の自然言語理解と推論機能を使用して、最初のプロンプトに対する応答を生 成します。\\n\\nAzure Al Search が LLM プロンプトに入力を提供しますが、モデルのトレーニングはし ません。RAG アーキテクチャでは、追加のトレーニングはありません。LLM はパブリ ック データを使用して事前トレーニングされますが、取得コンポーネントからの情報 によって拡張された応答を生成します。\\n\\nAzure Al Search を含む RAG パターンには、次の図に示す要素があります。\\n<figure>\\n\\n![](figures/32)\\n\\n<!-- FigureContent=\"Data Sources (files, databases, etc.) Azure Al Search Query > Knowledge SQL Prompt + Knowledge > Response App UX App Server, Orchestrator Azure OpenAl + 人 (GPT/ChatGPT)\" -->\\n\\n</figure>\\n\\n\\n● ユーザー エクスペリエンスのためのアプリ UX (Web アプリ)\\n\\n● アプリ サーバーまたはオーケストレーター(統合と調整レイヤー)\\n\\n● Azure Al Search (情報取得システム)\\n\\n● Azure OpenAl (生成 AI 用の LLM)\\n\\nWeb アプリはユーザー エクスペリエンスを提供し、プレゼンテーション、コンテキス :unselected: ト、ユーザー操作を提供します。ユーザーからの質問またはプロンプトは、ここから :unselected: 始まります。入力は統合レイヤーを通過します。最初に情報を取得して検索結果を取 得しますが、さらに LLM に移動してコンテキストと意図を設定します。\\n\\nアプリ サーバーまたはオーケストレーターは、情報の取得と LLM の間のハンドオフを 調整する統合コードです。1つのオプションは、LangChain △ を使用してワークフロー を調整することです。LangChain は Azure Al Search と統合されるため、Azure Al Search を 取得コンポーネント としてワークフローに簡単に含めることができます。 セマンティック カーネルロ も別のオプションです。\\n\\n情報取得システムは、検索可能なインデックス、クエリ ロジック、ペイロード(クエリ :unselected: 応答)を提供します。検索インデックスには、ベクトルまたはベクトル以外のコンテン ツを含めることができます。ほとんどのサンプルとデモにはベクトル フィールドが含 まれていますが、必須ではありません。クエリは、キーワード(または用語) とベクト ル クエリを処理できる Azure Al Search の既存の検索エンジンを使用して実行されま す。インデックスは、定義したスキーマに基づいて事前に作成され、ファイル、デー タベース、またはストレージからソース化されたコンテンツと共に読み込まれます。\\n\\nLLM は、元のプロンプトに加えて、Azure Al Search からの結果を受け取ります。LLM は結果を分析し、応答を作成します。LLM が ChatGPT の場合、ユーザーの対話は会話 のやり取りである可能性があります。Davinci を使用している場合、プロンプトは完全 に構成された回答である可能性があります。Azure ソリューションでは Azure OpenAl が使用される可能性が最も高いですが、この特定のサービスに対するハードな依存関係 はありません。\\n:unselected:\\nAzure Al 検索では、プロンプト フローやチャットの保持のためのネイティブ LLM 統合 は提供されないため、オーケストレーションと状態を処理するコードを記述する必要が あります。完全なソリューションに必要なブループリントについては、デモ ソース (Azure-Samples/azure-search-openai-demo △) を確認できます。また、LLM と統合す る RAG ベースの Azure AI 検索ソリューションを作成するには、Azure Al Studio または Azure OpenAl Studio を使用することをお勧めします。\\n\\n\\n## Azure Al Search の検索可能なコンテンツ\\n\\nAzure Al Search では、検索可能なすべてのコンテンツは、検索サービスでホストされ ている検索インデックスに格納されます。検索インデックスは、応答時間がミリ秒レ ベルの高速なクエリを実現するために設計されているため、内部データ構造はその目標 をサポートするために存在します。そのため、検索インデックスにはインデックス付 きコンテンツが保存されます。コンテンツ ファイル全体(PDF 全体や画像など) は保存 されません。内部では、データ構造にはトークン化されたテキストロの逆インデック ス、埋め込み用のベクトル インデックス、逐語的一致が必要な場合(フィルター、あい まい検索、正規表現クエリなど) の変更されていないテキストが含まれます。\\n\\nRAG ソリューションのデータを設定するときは、Azure Al Search でインデックスを作 成して読み込む機能を使用します。インデックスには、ソース コンテンツを複製また は表すフィールドが含まれます。インデックス フィールドは単純な転送(ソース ドキ ユメントのタイトルまたは説明が検索インデックスのタイトルまたは説明になる) であ るか、画像の表現またはテキストの説明を生成するベクトル化やスキル処理などの外部 プロセスの出力を含む場合があります。\\n\\n検索するコンテンツの種類をご存知のことと思われるので、各コンテンツ タイプに適 用できるインデックス作成機能を検討します。\\n\\n〔〕 テーブルを展開する\\n\\n||||\\n| - | - | - |\\n| コンテ ンツタ イプ | 付けられ たインデ ックス | 機能 |\\n| text | トーク ン、変更 されてい ないテキ スト | インデクサーは、Azure Storage や Cosmos DB などの他の Azure リソー スからプレーンテキストをプルできます。インデックスに任意の JSON コンテンツをプッシュすることもできます。処理中のテキストを変更す るには、アナライザーとノーマライザーを使用して、インデックス作成 中に字句処理を追加します。同意語マップは、クエリで使用される可能 性のある用語がソース ドキュメントにない場合に便利です。 |\\n| text | ベクトル 1 | テキストをチャンクして外部でベクトル化し、インデックスにベクトル フィールドとしてインデックスを付けることができます。 |\\n\\n||||\\n| - | - | - |\\n| コンテ ンツタ イプ | 付けられ たインデ ックス | 機能 |\\n| image | トーク ン、変更 されてい ないテキ スト 2 | OCR と画像解析のスキルでは、テキスト認識やイメージ特性のために画 像を処理できます。画像情報は検索可能なテキストに変換され、インデ ックスに追加されます。スキルにはインデクサーの要件があります。 |\\n| image | ベクトル 1 | 画像は、画像コンテンツを数学的に表現するために外部でベクトル化 し、インデックスにベクトル フィールドとしてインデックスを付けるこ とができます。OpenAl CLIP Z などのオープン ソース モデルを使用し て、同じ埋め込み空間内のテキストと画像をベクトル化できます。 |\\n\\n1ベクトル サポートの一般提供機能では、データ チャンクとベクトル化のために他の ライブラリまたはモデルを呼び出す必要があります。しかし、垂直統合(プレビュー) にはこれらの手順が埋め込まれています。両方のアプローチを示すコード サンプルに :unselected: ついては、azure-search-vector リポジトリ☑を参照してください。\\n\\n2スキルは AI エンリッチメントの組み込みサポートです。OCR と画像分析の場合、イ ンデックス作成パイプラインは Azure Al Vision API の内部呼び出しを行います。 これ らのスキルは、抽出された画像を処理のために Azure AI に渡し、Azure Al Search によ ってインデックス付けされたテキストとして出力を 受け取ります。\\n\\nベクトルは、異なるコンテンツ(複数のファイル形式と言語) に最適な設備を提供しま す。これは、コンテンツが数学表現で汎用的に表現されるためです。また、ベクトル :unselected: では類似性検索もサポートされています。つまり、ベクトル クエリに最も似た座標で 照合します。トークン化された用語で照合するキーワード検索(または用語検索) と比 較すると、類似性検索の方が微妙です。コンテンツまたはクエリにあいまいな点や解 :unselected: 釈の要件がある場合は、より適切な選択肢です。\\n\\n\\n## Azure Al Search でのコンテンツの取得\\n\\nデータが検索インデックスに格納されたら、Azure Al Search のクエリ機能を使用して コンテンツを取得します。\\n\\nRAG 以外のパターンでは、クエリは検索クライアントからラウンド トリップを行いま す。クエリが送信され、検索エンジンで実行され、応答がクライアント アプリケーシ ョンに返されます。応答(検索結果)は、インデックス内で見つかった逐語的なコンテ ンツのみで構成されます。\\n\\nRAG パターンでは、検索エンジンと LLM の間でクエリと応答が調整されます。ユーザ ーの質問またはクエリは、検索エンジンと LLM の両方にプロンプトとして転送されま\\n:unselected:\\nす。検索結果は検索エンジンから戻り、LLM にリダイレクトされます。ユーザーに返 される応答は生成 AI で、LLM からの合計または回答のどちらかです。\\n\\nAzure Al Search には、新しい回答を構成するクエリの種類はありません(セマンティッ ク検索やベクトル検索でさえも)。LLM だけが生成 AI を提供します。クエリの作成に 使用される Azure Al Search の機能を次に示します。\\n\\n〔〕 テーブルを展開する\\n\\n| クエリ機 能 | 目的 | 使用する理由 |\\n| - | - | - |\\n| 単純また は完全な Lucene 構 文 :selected: | テキストと非ベクトル数値コ :unselected: ンテンツに対するクエリ実行 | フルテキスト検索は、類似一致ではなく、完全一 致に最適です。フルテキスト検索クエリは、 BM25 アルゴリズムを使用してランク付けされ、 スコアリング プロファイルによる関連性チューニ ングをサポートします。また、フィルターとファ セットもサポートされています。 |\\n| フィルタ ーとファ セット | テキストまたは数値(非ベク トル)フィールドにのみ適用 されます。包含条件または除 外条件に基づいて検索対象領 域を減らします。 | クエリに精度を追加します。 |\\n| セマンテ ィックラ ンク付け | セマンティック モデルを使用 して BM25 結果セットを再ラ ンク付けします。LLM 入力と して役立つ短い形式のキャプ ションと回答を生成します。 | スコアリング プロファイルよりも簡単で、コンテ ンツによっては、関連性チューニングのためのよ り信頼性の高い手法です。 :unselected: |\\n| ベクトル 検索 | クエリ文字列が 1 つ以上のべ クトルである類似性検索のべ クトル フィールドに対するク エリ実行。 | ベクトルは、あらゆる種類のコンテンツを任意の :unselected: 言語で表すことができます。 |\\n| ハイブリ ッド検索 | 上記のクエリ手法の一部また はすべてを組み合わせます。 ベクトルおよびと非ベクトル クエリは並列で実行され、統 合された結果セットで返され ます。 | ハイブリッド クエリを使用した場合、精度とリコ ールにおけるメリットが最も多くなります。 :unselected: |\\n\\n\\n## クエリ応答を構造化する\\n\\nクエリの応答は LLM に入力を提供するため、検索結果の品質は成功に不可欠です。結 果は表形式の行セットです。結果の構成や構造は次に依存します。\\n\\n● 応答に含まれるインデックスの部分を決定するフィールド。\\n\\n● インデックスにおける一致を示す行。\\n\\nフィールドは、属性が“取得可能\" である場合に検索結果に表示されます。インデック ススキーマ内のフィールド定義には属性があり、これがフィールドが応答で使用され るかどうかを決定します。“取得可能\" フィールドだけがフル テキスト クエリまたはべ クトル クエリ結果で返されます。既定では、すべての“取得可能\"フィールドが返され ますが、“選択\"を使用してサブセットを指定できます。“取得可能\"以外に、フィール ドに制限はありません。フィールドには、任意の長さまたは型を指定できます。長さ について、Azure Al Search にはフィールド長の上限はありませんが、API 要求のサイズ には制限があります。\\n\\n行ではクエリとの一致が、関連性、類似性、またはその両方でランク付けされます。 既定では、結果はフル テキスト検索の場合は上位 50 件、ベクトル検索の場合は Kニ アレスト ネイバーに制限されます。既定値を変更して制限を(最大 1000 ドキュメント まで) 増減できます。top および skip ページング パラメーターを使用して、結果を一 連のページングされた結果として取得することもできます。\\n\\n\\n## 関連性でランク付けする\\n\\n複雑なプロセスや大量のデータを処理する場合や、ミリ秒レベルの応答が期待されてい る場合、各ステップで価値を高め、最終的な結果の品質を向上させることが重要です。 情報取得側において、関連性のチューニングは、LLM に送信される結果の品質を向上 させるアクティビティです。結果には、最も関連性の高い、または最も類似した一致 するドキュメントだけを含める必要があります。\\n\\n関連性は、キーワード(非ベクトル) 検索とハイブリッド クエリ(非ベクトル フィール ドに対する) に適用されます。Azure Al Search では、類似性検索とベクトル クエリの 関連性のチューニングはありません。BM25 ランク付けは、フル テキスト検索のラン ク付けアルゴリズムです。\\n\\n関連性のチューニングは、BM25 ランク付けを強化する機能によってサポートされま す。これらのアプローチには、次が含まれます。\\n :unselected:\\n· スコアリング プロファイル。これは、特定の検索フィールドまたはその他の条件 で一致が見つかった場合に検索スコアを向上させます。\\n\\n● セマンティックランク付け。これは、Bing のセマンティック モデルを使用して結 果を並べ替え、元のクエリに合わせてセマンティックに適合するように BM25 結 果セットを再ランク付けします。\\n\\n比較テストおよびベンチマーク テストでは、テキストフィールドとベクトル フィール ドを含むハイブリッド クエリに、BM25 ランクの結果に対するセマンティック ランク 付けを補足することで、最も関連性の高い結果が生成されます。\\n\\n# RAG シナリオの Azure Al Search クエリのコード例\\n\\n次のコードは、デモ サイトからの retrievethenread.py ☑ ファイルからコピーされま す。ハイブリッド クエリの検索結果から LLM の content が生成されます。より簡単 なクエリを記述できますが、この例では、セマンティック再ランク付けとスペル チェ ックを使用したベクトル検索とキーワード検索が含まれています。デモでは、このク エリを使用して初期コンテンツを取得します。\\n\\nPython\\n\\n\\\\# Use semantic ranker if requested and if retrieval mode is text or hybrid (vectors + text)\\n\\nif overrides.get(\"semantic\\\\_ranker\") and has\\\\_text: r = await self. search\\\\_client. search(query\\\\_text, filter=filter, query\\\\_type=QueryType. SEMANTIC,\\n\\nquery\\\\_language=\"en-us\", query\\\\_speller=\"lexicon\", semantic\\\\_configuration\\\\_name=\"default\", top=top, query\\\\_caption=\"extractive|highlight-false\"\\n\\nif use\\\\_semantic\\\\_captions else None,\\n\\nvector=query\\\\_vector, top\\\\_k=50 if query\\\\_vector else None, vector\\\\_fields=\"embedding\" if query\\\\_vector\\n\\nelse None) else:\\n\\nr = await self. search\\\\_client.search(query\\\\_text,\\n\\nfilter=filter, top=top, vector=query\\\\_vector, top\\\\_k=50 if query\\\\_vector else None, vector\\\\_fields=\"embedding\" if query\\\\_vector\\n\\nelse None) if use\\\\_semantic\\\\_captions:\\n\\nresults = [doc[self. sourcepage\\\\_field] + \": \" + nonewlines(\" . \". join([c. text for c in doc[\\'@search. captions\\' ]]) ) async for doc in r] else:\\n\\nresults = [doc[self.sourcepage\\\\_field] + \": \" +\\n\\nnonewlines (doc[self.content\\\\_field]) async for doc in r] content = \"\\\\\\\\n\". join(results)\\n\\n\\n## 統合コードと LLM\\n\\nAzure Al Search を含む RAG ソリューションでは、完全なソリューションを作成するた めに、他のコンポーネントとコードが必要です。前のセクションでは、Azure Al Search を使用した情報の取得と、検索可能なコンテンツの作成とクエリに使用される\\n\\n機能について説明しましたが、このセクションでは LLM の統合と相互作用について説 明します。\\n\\nデモ リポジトリ内のノートブックは、LLM に検索結果を渡すためのパターンを示して いるため、出発点として最適です。RAG ソリューションのほとんどのコードは LLM の 呼び出しで構成されているため、この記事の範囲外であるこれらの API のしくみを理 解する必要があります。\\n\\nchat-read-retrieve-read.ipynb ☑ ノートブックの次のセル ブロックは、チャット セッシ ョンのコンテキストでの検索呼び出しを示しています。\\n\\nPython\\n\\n\\\\# Execute this cell multiple times updating user\\\\_input to accumulate chat history user\\\\_input = \"Does my plan cover annual eye exams?\"\\n\\n\\\\# Exclude category, to simulate scenarios where there\\'s a set of docs you can\\'t see exclude\\\\_category = None\\n\\nif len(history) > 0: completion = openai. Completion. create( engine=AZURE\\\\_OPENAI\\\\_GPT\\\\_DEPLOYMENT, prompt=summary\\\\_prompt\\\\_template. format (summary=\"\\\\\\\\n\". join(history), question=user\\\\_input), temperature=0.7, max\\\\_tokens=32, stop=[\"\\\\\\\\n\"]) search = completion. choices[0].text else:\\n\\nsearch = user\\\\_input\\n\\n\\\\# Alternatively simply use search\\\\_client. search(q, top=3) if not using semantic ranking print (\"Searching:\", search) print (\" \")\\n\\nfilter = \"category ne \\'{ } \\'\". format (exclude\\\\_category. replace(\"\\'\", \"\\'\\'\")) if exclude\\\\_category else None\\n\\nr = search\\\\_client. search (search, filter=filter, query\\\\_type=QueryType. SEMANTIC, query\\\\_language=\"en-us\", query\\\\_speller=\"lexicon\", semantic\\\\_configuration\\\\_name=\"default\", top=3)\\n\\nresults = [doc[KB\\\\_FIELDS\\\\_SOURCEPAGE] + : \" + doc[KB\\\\_FIELDS\\\\_CONTENT]. replace(\"\\\\\\\\n\", \") . replace(\"\\\\\\\\r\", \") for doc in r] content = \"\\\\\\\\n\". join(results)\\n\\nprompt = prompt\\\\_prefix. format (sources=content) + prompt\\\\_history + user\\\\_input + turn\\\\_suffix\\n\\ncompletion = openai. Completion.create( engine=AZURE\\\\_OPENAI\\\\_CHATGPT\\\\_DEPLOYMENT,\\n\\nprompt=prompt,\\n\\ntemperature=0.7, max\\\\_tokens=1024, stop=[\"<| im\\\\_end|>\", \"<|im\\\\_start|>\"])\\n\\nprompt\\\\_history += user\\\\_input + turn\\\\_suffix + completion. choices [0].text + \"\\\\\\\\n<| im\\\\_end|>\" + turn\\\\_prefix\\n\\nhistory. append(\"user: \" + user\\\\_input) history. append (\"assistant: \" + completion.choices[0].text)\\n\\nprint(\"\\\\\\\\n- -\\\\\\\\n\" . join (history))\\n\\nprint(\"\\\\\\\\n- \\\\\\\\nPrompt : \\\\\\\\n\" + prompt)\\n\\n\\n## ファースト ステップ\\n\\n● Azure Al Studio を使用して検索インデックスを作成します。\\n\\n● Azure OpenAl Studio と\"データ持ち込み\"を使用して、プレイグラウンドで既存の 検索インデックスに対するプロンプトを試します。この手順は、使用するモデル を決定するのに役立ち、RAG シナリオで既存のインデックスがどの程度適切に動 作するかを示します。\\n\\n● Azure Al 検索チームによって構築された“データとのチャット\"ソリューション ア クセラレータでは、独自のカスタム RAG ソリューションを作成するのに役立ちま す。\\n\\n· エンタープライズ チャット アプリ テンプレートロでは、Contoso と Northwind の架空の医療保険ドキュメントを使用して、Azure リソース、コード、サンプル のグラウンディング データをデプロイします。このエンド ツー エンド ソリュー ションを使用すると、運用チャット アプリをわずか 15 分ほどで利用できます。 これらのテンプレートのコードは、いくつかのプレゼンテーションで取り上げら れる azure-search-openai-demo です。次のリンクでは言語固有のバージョンが 提供されます。\\n\\nㅇ :unselected: .NET Z\\n\\no Python\\n :unselected:\\no JavaScript\\n :unselected: :unselected:\\nJava\\n\\n· インデックス作成の概念と戦略を確認して、データを取り込む方法と更新方法を 決定します。ベクトル検索、キーワード検索、ハイブリッド検索のどれを使用す るかを決定します。検索する必要があるコンテンツの種類と実行するクエリの種 類によって、インデックスの設計が決まります。\\n:unselected:\\n● クエリの作成について確認して、検索要求の構文と要件の詳細について確認しま す。\\n\\n\\n### 4 注意\\n\\n一部の Azure Al Search 機能は人による操作を目的としており、RAG パターンでは 役に立ちません。具体的には、オートコンプリートと候補をスキップできます。 ファセットや orderby などの他の機能は役立つ可能性がありますが、RAG シナリ オでは一般的ではありません。\\n\\n\\n## 関連項目\\n\\n● 取得拡張生成: インテリジェント自然言語処理モデルの作成の合理化☑\\n\\n● Azure Machine Learning プロンプト フローを使用した取得拡張生成\\n :unselected:\\n● Azure Cognitive Search と LangChain: 拡張ベクトル検索機能のシームレスな統合☑\\n\\n# Azure Al Search でクエリを実行する\\n\\n[アーティクル]·2023/11/15\\n\\nAzure Al Search では、フリーフォーム テキスト検索から高度に指定されたクエリ パタ ーンやベクトル検索まで、さまざまなシナリオのクエリ コンストラクトがサポートさ れます。すべてのクエリは、検索可能なコンテンツを格納する検索インデックスに対 して実行されます。\\n\\n\\n## クエリの種類\\n\\n| クエ| リフ ンテンツ オー ム  検索可能なコ :unselected: | 説明 |\\n| - | - | - |\\n| フル テキ スト 検索 | トークン化さ れた用語の逆 インデック ス。 | フルテキストクエリでは、任意の数の検索ドキュメント内で高速スキ ャンできるように構造化された転置インデックスを反復処理します。 この場合、すべてのフィールドで一致が見つかる可能性があります。 テキストは分析されてフルテキスト検索用にトークン化されます。 |\\n| ベク トル 検索 | 生成された埋 め込みのベク トルインデッ クス。 | ベクトル クエリでは、検索インデックスのベクトル フィールドが反復 処理されます。 |\\n| ハイ ブリ ッド 検索 | 1 つの検索イ ンデックスで 上記のすべ て。 | 1 つのクエリ要求でテキスト検索とベクトル検索を組み合わせます。 テキスト検索は、\"検索可能\" および\"フィルター処理可能\" なフィール ドのプレーンテキスト コンテンツに対して機能します。ベクトル検索 は、ベクトル フィールド内のコンテンツに対して機能します。 |\\n| その 他 | プレーンテキ ストと英数字 のコンテン ツ。 | ソース ドキュメントから逐語的に抽出された未加工のコンテンツは、 地理空間検索、あいまい検索、フィールド検索などのフィルターおよ びパターン マッチング クエリをサポートします。 |\\n\\nこの記事では最後のカテゴリ、つまり、フィルターやその他の特殊なクエリ フォーム に使用される、元のソースからそのまま抽出された、プレーン テキストと英数字のコ ンテンツに対して機能するクエリに焦点を当てます。\\n\\n\\n## オートコンプリートとクエリ候補\\n\\nオートコンプリートや結果候補は、 search の代替手段です。これは、部分文字列の入 カ(各文字の後) に基づいて一連のクエリ要求を入力につれて検索する方式で行いま\\n:unselected:\\nす。このチュートリアルで説明しているように、 autocomplete と suggestions パラメ ーターは一緒に使用することも、個別に使用することもできますが、 search と一緒に 使用することはできません。完成した用語とクエリ候補はどちらもインデックスの内 容から派生されます。このエンジンは、インデックスに存在しない文字列や候補を返 しません。詳細については、「オートコンプリート(REST API)」と「検索候補(REST API)」をご覧ください。\\n\\n\\n## フィルター検索\\n\\nフィルターは、Azure Al Search を元に構築したアプリで広く使用されています。アプ リケーション ページでは、フィルターは多くの場合、ユーザー向けのフィルター処理 のためにリンク ナビゲーション構造のファセットとして視覚化されます。フィルター は、インデックス付きコンテンツのスライスを公開するために、内部的にも使用されま す。たとえば、製品カテゴリに対してフィルターを使用して検索ページを初期化した り、インデックスに英語とフランス語の両方のフィールドが含まれている場合は、言語 を初期化したりすることができます。\\n\\n次の表に示すように、特殊なクエリ フォームを呼び出すフィルターが必要な場合もあ ります。指定されていない検索(search =\\\\* ) を含むフィルターを使用することも、用 語、語句、演算子、およびパターンを含むクエリ文字列を含むフィルターを使用するこ ともできます。\\n\\n|||\\n| - | - |\\n| ユーザ ーシナ リオ | 説明 |\\n| 範囲フ ィルタ | Azure Al Search では、範囲クエリは filter パラメーターを使用して作成されます。詳 細と例については、「範囲フィルターの例」をご覧ください。 |\\n| ファセ ットナ ビゲー ション | ファセット ナビゲーション ツリーでは、ユーザーがファセットを選択できます。フ ィルターでサポートされている場合、クリックするたびに検索結果が絞り込まれま す。ファセットによって指定された条件に一致しなくなったドキュメントを除外す るフィルターによって、各ファセットがサポートされます。 |\\n\\n\\n### 1 注意\\n\\nフィルター式で使用されるテキストは、クエリ処理中には分析されません。テキ スト入力は、照合に成功するか失敗するかのどちらかになる逐語的な文字パター ンで、大文字と小文字が区別されます。フィルター式は OData 構文を使用して構 築され、インデックス内のすべての“フィルター可能な\" フィールドの filter パ\\n:selected:\\nラメーターで渡されます。詳細については、「Azure Al Search のフィルター」を ご覧ください。\\n\\n# 地理空間検索\\n\\n地理空間検索では、“近くを検索\" またはマップベースの検索エクスペリエンスのため に、場所の緯度と経度の座標との一致を検索します。Azure Al Search では、次の手順 に従って地理空間検索を実装できます。\\n\\n. Edm.GeographyPoint. Collection(Edm.GeographyPoint. Edm.GeographyPolygon) のいずれかの種類のフィルター可能フィールドを定義します。\\n\\n● 受信ドキュメントに適切な座標が含まれていることを確認します。\\n\\n● インデックス作成が完了したら、フィルターと地理空間関数を使用するクエリを 作成します。\\n\\n地理空間検索では、距離にキロメートルを使用します。座標は(longitude, latitude) :unselected: の形式で指定されます。\\n\\n地理空間検索のフィルターの例を次に示します。このフィルターでは、地理的なポイ ント(この例ではワシントン D.C.) から半径 300 キロ以内に座標を持つ他の Location :unselected: フ ィールドを検索インデックスから見つけます。結果にアドレス情報を返し、場所によ る自己ナビゲーションのためにオプションの facets 句を含めます。\\n\\nHTTP\\n\\nPOST https: //{{searchServiceName} } . search.windows.net/indexes/hotels-vector- quickstart/docs/search?api-version=2023-07-01-Preview {\\n\\n\"count\": true, \"search\": \"\\\\*\"\\n\\n\"filter\": \"geo.distance(Location, geography\\' POINT(-77.03241 38.90166)\\') le 300\"\\', \"facets\": [ \"Address/StateProvince\"],\\n\\n\"select\": \"HotelId, HotelName, Address/StreetAddress, Address/City, Address/StateProvince\", \"top\": 7 }\\n\\n詳細および例については、地理空間検索の例に関するページをご覧ください。\\n\\nドキュメントの検索\\n:unselected:\\n前述のクエリ フォームとは対照的に、このフォームでは、対応するインデックス検索 またはスキャンを行わずに、ID による検索ドキュメントを1つ取得します。1 つのド キュメントのみが要求され、返されます。ユーザーが検索結果で項目を選択する場 合、ドキュメントの取得と詳細ページでのフィールドの設定が典型的な応答になり、ド キュメントの検索はこれをサポートする操作になります。\\n\\n\\n## 高度な検索:あいまい、ワイルドカード、近接、 正規表現\\n\\n高度なクエリ フォームは、特定のクエリ動作をトリガーする完全な Lucene パーサーと 演算子に依存します。\\n\\n| クエリの種類 | 使用法 | 例/詳細情報 |\\n| - | - | - |\\n| フィールド検索 | search パラメーター、 | 1 つのフィールドを対象とする複合ク |\\n|| queryType=full | エリ式を作成します。 フィールド検索の例 |\\n| あいまい検索 | search パラメーター、 queryType=full | 構造やスペリングが似ている語句を照 合します。 あいまい検索の例 |\\n| 近接検索 | search パラメーター、 queryType=full | ドキュメント内で近くにある語句を検 索します。 近接検索の例 |\\n| 用語ブースト | search パラメーター、 queryType=full | ブーストされた語を含むドキュメント の順位を、含まないドキュメントより も引き上げます。 用語ブーストの例 |\\n| 正規表現検索 | search パラメーター、 queryType=full | 正規表現の内容に基づいて照合しま |\\n||| す。 正規表現の例 |\\n|| ワイルドカードま \\\\*\\\\~ または?を使用した たはプレフィック パラメーター、queryType=full ス検索  search | プレフィックスとチルダ(\\\\~) または 1 つの文字(?)に基づいて照合します。 ワイルドカード検索の例 |\\n\\n\\n## 次のステップ\\n\\nクエリの実装について詳しく見るには、構文ごとに例を確認します。フルテキスト検 索を初めて使用する場合は、クエリ エンジンの機能をよく理解しておくことをお勧め します。\\n\\n● 簡易クエリの例\\n\\n● 高度なクエリを作成するための Lucene 構文のクエリの例\\n\\n● Azure Al Search でのフル テキスト検索のしくみgit\\n\\n# Azure Al Search でのセマンティック ラ ンク付け\\n\\n[アーティクル]·2024/02/08\\n\\nAzure Al Search のセマンティック ランク付けでは、検索結果を再ランク付けするため に、言語理解を利用して検索結果の関連性をある程度高めます。この記事では、概要 について説明します。最後のセクションでは、販売状況と価格について説明します。\\n\\nセマンティック ランカーはプレミアム機能であり、使用量に基づいて課金されます。 最初にこの記事を読んで基礎知識を得ることをお勧めしますが、それはかまわないので すぐに使いたいという方は、これらの手順を実行してください。\\n\\n✔ リージョン別の提供状況を確認する☑\\n\\n✔ Azure portal にサインインとして、検索サービスが Basic 以上であることを確認す る\\n\\n✔ セマンティック ランク付けを有効にして価格プランを選択する\\n\\n✔ 検索インデックスでセマンティック構成を設定する\\n\\n✔ セマンティックキャプションとハイライトを返すようにクエリを設定する\\n\\n✔ 必要に応じて、セマンティック回答を返す\\n\\n\\n## 4 注意\\n\\nセマンティック ランク付けでは、生成 AI やベクトルは使用されません。ベクト ルのサポートと類似性検索をお探しの場合、 「Azure Al Search でのベクトル検 索」で詳細をご確認いただけます。\\n\\n\\n## セマンティック ランク付けとは\\n\\nセマンティック ランカーは、テキストベースのクエリに対する、BM25 でランク付け されたあるいは RRF でランク付けされた最初の検索結果の品質を高めるクエリ関連機 能のコレクションです。これを検索サービスで有効にすると、セマンティック ランク 付けによってクエリの実行パイプラインに2 つの機能が追加されます。\\n\\n● 1つ目として、BM25 または RRF を使用してスコア付けされた、最初の結果セッ トに対する二次ランク付けが追加されます。この二次ランク付けでは多言語の、 Microsoft Bing から変化したディープ ラーニング モデルが使用されて、セマンテ ィック的に最も関連性の高い結果が奨励されます。\\n:selected: :selected: :selected: :selected: :selected: :selected: :unselected: :unselected:\\n● 2つ目として、キャプションと回答が抽出されて応答で返されます。これを検索 ページにレンダリングして、ユーザーの検索エクスペリエンスを向上させること ができます。\\n\\nセマンティック リランカーの機能を次に示します。\\n\\n〔〕 テーブルを展開する\\n\\n| 特徴量 | 説明 |\\n| - | - |\\n| セマンテ イックラ ンク付け | クエリのコンテキストまたはセマンティックの意味を利用して、事前にランク付け された結果に対して新しい関連スコアを計算します。 |\\n| セマンテ イックキ ャプショ ンとハイ ライト | コンテンツを最もよく要約している逐語的な文やフレーズをドキュメントから抽出 :unselected: し、スキャンを簡単にするために重要な部分を強調表示します。結果を要約するキ ャプションは、個々のコンテンツ フィールドが検索結果ページに対して高密度であ る場合に便利です。強調表示されたテキストにより、最も関連性の高い用語とフレ ーズが目立つため、ユーザーはその一致が関連していると見なされた理由を迅速に 判断できます。 |\\n| セマンテ イック回 答 | セマンティック クエリから返される省略可能な追加のサブ構造体。これにより、質 問のようなクエリに直接回答することができます。ドキュメントには、回答の特性 を持つテキストが含まれている必要があります。 |\\n\\n\\n## セマンティック ランカーのしくみ\\n\\nセマンティック ランク付けは、クエリと結果を Microsoft がホストする言語理解モデ ルにフィードし、より適切な一致をスキャンします。\\n\\nコンセプトを次の図で説明します。\"capital\"という用語を考えてみましょう。これ は、コンテキストが財務、法律、地理、文法なのかによって意味が異なります。 言語 理解では、セマンティックのランカーによってコンテキストが検出され、クエリの意図 に沿う結果が昇格されます。\\n<figure>\\n\\n![](figures/33)\\n\\n<!-- FigureContent=\"Crime Tax Punishment Provinces Money States Investments Capital Capital Gains Building Finance Country Letters\" -->\\n\\n</figure>\\n\\n\\nセマンティックの順位付けは、リソースと時間の両方を消費します。クエリ操作の予 想待機時間内に処理を完了するために、セマンティック ランク付けへの入力が統合さ れ、削減されるため、再ランク付けの手順をできるだけ早く完了できます。\\n\\nセマンティック ランク付けには、サマライゼーションとスコアリングという 2 つの手 順があります。出力が再スコアリングされた結果、キャプション、および回答で構成 されます。\\n\\n\\n## 入力が収集されて要約されるしくみ\\n\\nセマンティック ランク付けでは、クエリのサブシステムからサマライゼーション モデ ルとランク付けモデルに検索結果が入力として渡されます。ランク付けモデルには入 カサイズに制約があり、集中的に処理されるため、検索結果は効率的に処理できるよう なサイズで構造化(要約) されている必要があります。\\n\\n1\\\\. セマンティック ランク付けは、テキスト クエリの BM25 でランク付けされた検 索結果から、またはハイブリッド クエリの RRF でランク付けされた結果から開始 されます。テキスト フィールドのみが再ランク付け実行で使用され、結果の数が 50 個を超える場合でも、セマンティック ランク付けが行われるのは上位 50 個の 結果のみです。通常、セマンティック ランク付けで使用されるフィールドは、情 報を提供する説明的なものです。\\n\\n2\\\\. 検索結果の中の各ドキュメントで、サマライゼーション モデルが受け入れるのは 2,000 トークンまでで、この場合、1トークンはおよそ 10 文字です。入力がセマ ンティック構成の一覧にある \"title\"、\"keyword\"、および \"content\" フィールドか らアセンブルされます。\\n\\n3\\\\. 長さが非常に長い文字列は、全体の長さが概要作成手順の入力要件を満たすよう にトリミングされます。優先順位の高い順序でセマンティック構成にフィールド を追加することが重要なのは、このトリミングの実行があるためです。テキスト を多用するフィールドを持つ非常に大きいドキュメントがある場合は、最大値制 限を超えたテキストは無視されます。\\n\\n〔〕 テーブルを展開する\\n\\n| セマンティック フィールド | トークンの限度 |\\n| - | - |\\n| 「タイトル」 | 128 トークン |\\n| \"keywords | 128 トークン |\\n| \"content\\' | 残りのトークン |\\n\\n4\\\\. サマライゼーション出力は各ドキュメントの要約文字列であり、各フィールドの 中の最も関連した情報で構成されます。要約文字列がスコアリングのためランカ ーへ送られて、キャプションと回答を求めて機械読み取り理解モデルへ送られま す。\\n\\nセマンティック ランカーへ渡されるそれぞれの生成後の要約文字列の最大長さ は、256 トークンです。\\n\\n\\n### セマンティック ランカーの出力\\n\\n各要約文字列から、機械読み取り理解モデルは最も代表的な一節を見つけ出します。 出力は次のようになります:\\n\\n● ドキュメントのセマンティック キャプション。各キャプションは、プレーン テ キストバージョンと強調表示バージョンで使用できます。また、多くの場合、ド キュメントあたり 200 語未満です。\\n\\nanswers パラメーターを指定した場合、クエリが質問として提示された場合、そ の質問に対して適していそうな回答を提供する長い文字列で文節が見つかる場合 を想定し、オプションのセマンティック回答が返されます。\\n\\nキャプションと回答は、常にインデックスからの逐語的なテキストです。このワーク フローには、新しいコンテンツを作成または構成する生成 AI モデルはありません。\\n :unselected:\\n要約がスコアリングされるしくみ\\n\\nスコアリングは、キャプションと、256 のトークン長を埋める要約文字列の中の他のあ らゆるコンテンツに対して行われます。\\n\\n1\\\\. キャプションは、指定されたクエリに対して相対的な概念とセマンティックの関 連性に対して評価されます。\\n\\n2\\\\. @search.rerankerScore が各ドキュメントに、指定のクエリのドキュメントのセ マンティック関連性に基づいて割り当てられます。スコアの範囲は4から0(高 から低) です。スコアが高いほど関連性が高いことを示します。\\n\\n3\\\\. 一致は、スコア順に一覧表示され、クエリ応答ペイロードに含められます。ペイ ロードには、回答、プレーンテキスト、強調表示されたキャプション、select 句 :unselected: で取得または指定されたフィールドが含まれます。\\n\\n\\n#### 1 注意\\n\\n2023 年7月14 日より、@search.rerankerScore の分布が変更されます。スコア の結果を、テスト以外で判断することはできません。この応答プロパティにハー :unselected: ドしきい値の依存関係がある場合、テストを返して、しきい値に適切な新しい値 を確認してください。\\n\\n\\n### セマンティック機能と制限\\n\\nセマンティック ランカーは新しいテクノロジなので、実行できることとできないこと :unselected: についての期待値を設定することが重要です。\"できることは、次のようなことで す。\\n\\n● セマンティック的に元のクエリの意図に近い一致を昇格させます。\\n\\n● キャプションおよび回答として使用できる文字列を見つけ出します。キャプショ ンと回答は応答で返され、検索結果ページに表示できる文字列が検出されます。\\n\\nセマンティック ランク付けで“できない\"ことは、コーパス全体に対してクエリを再実 :unselected: 行して、セマンティックな関連がある結果を検出することです。セマンティック ラン ク付けでは、既定のランク付けアルゴリズムによってスコアリングされた上位 50 個の 結果で構成される既存の結果セットが再ランク付けされます。さらに、セマンティッ ク ランク付けで新しい情報や文字列を作成することはできません。キャプションと回 答は、コンテンツから逐語的に抽出されるので、結果に回答のようなテキストが含まれ ていない場合、その言語モデルではキャプションや回答は生成されません。\\n\\nセマンティック ランク付けはすべてのシナリオで有益なわけではありませんが、特定 のコンテンツではその機能から多くのメリットが得られます。セマンティック ランク\\n:unselected:\\n付けの言語モデルは、情報が豊富で、散文として構造化された検索可能なコンテンツに 最適です。ナレッジ ベース、オンライン ドキュメント、説明的なコンテンツを含むド キュメントでは、セマンティック ランク付け機能から最も多くのメリットが得られま す。\\n\\nテクノロジは Bing と Microsoft Research が基になっており、Azure Al Search インフラ :unselected: ストラクチャにアドオン機能として統合されます。研究と AI 投資でバックアップされ るセマンティック ランク付けに関する詳細については、「Bing の AI が Azure Al Search にパワーを与えるしくみ(Microsoft Research ブログ)☑」を参照してください。\\n\\n次の動画では、機能の概要について説明しています。 https://www.youtube-nocookie.com/embed/yOf0WfVd\\\\_V0 z\\n\\n\\n## 可用性と料金\\n\\nセマンティック ランカーは、利用可能なリージョンとにおいて、Basic レベル以上の検 索サービスで使用できます。\\n\\nセマンティック ランカーを有効にする場合は、機能に対応する価格プランを選択しま す。\\n\\n● クエリ ボリュームが低い場合(月間 1000 件未満)、セマンティック ランク付けは 無料です。\\n\\n● クエリのボリュームが大きい場合、標準価格プランを選択します。\\n\\nAzure Al Search の価格ページロでは、さまざまな通貨とサイクル間隔での課金レート が表示されます。\\n\\nセマンティック ランク付けの料金は、 queryType=semantic を含む、検索文字列が空で ないクエリ要求(たとえば search=pet friendly hotels in New York)を使用した場合に 発生します。検索文字が空の場合(search =\\\\* )、queryType が semantic に設定されてい ても課金されません。\\n\\n\\n## 関連項目\\n\\n● セマンティック ランク付けを有効にする\\n\\n● セマンティック ランク付けを構成する\\n\\n● ブログ:ハイブリッド検索とランク付け機能でベクトル検索の性能を上回る☑\\n\\n# キーワード検索での関連性(BM25 スコ アリング)\\n\\n[アーティクル]·2024/05/08\\n\\nこの記事では、フルテキスト検索で検索スコアを計算するために使用される BM25 関 連性スコアリングのアルゴリズムについて説明します。BM25 関連性は、フルテキス ト検索に限定されます。フィルター クエリ、オートコンプリート、提案されたクエ リ、ワイルドカード検索、あいまい検索の各クエリは、関連性についてスコアリングも ランク付けもされません。\\n\\n\\n## フルテキスト検索で使用されるスコアリング ア ルゴリズム\\n\\nAzure Al Search には、フルテキスト検索用の次のスコアリング アルゴリズムが用意さ れています:\\n\\n〔〕 テーブルを展開する\\n\\n| アルゴリズム | 使用法 | Range |\\n| - | - | - |\\n| BM25Similarity | 2020 年7月以降に作成されたすべての検索サービスでアルゴリ ズムを修正しました。このアルゴリズムは構成可能ですが、古い アルゴリズム(クラシック)に切り替えることはできません。 | 無制 限。 |\\n| ClassicSimilarity || 以前の検索サービスに存在します。BM25 をオプトインし、イン 0 <  |\\n| | デックスごとにアルゴリズムを選択できます。 | 1.00 |\\n\\nBM25 とクラシックはいずれも TF-IDF タイプの取得関数です。この関数では、単語の 出現頻度(TF) と逆文書頻度(IDF) が変数として使用され、ドキュメントとクエリの組み ごとに関連スコアが計算されます。ドキュメントとクエリの組みはその後、ランク付け の結果に使用されます。概念的にはクラシックと似ていますが、BM25 は確率論的情 報取得に根ざしており、ユーザーの調査で測定した場合のように、より直感的な一致が 生成されます。\\n\\nBM25 には高度なカスタマイズ オプションがあります。たとえば、ユーザーは、一致 した単語の出現頻度で関連性スコアが変動するしくみを決定できます。詳細について は、スコアリング アルゴリズムの構成に関するページを参照してください。\\n\\n4 注意\\n\\n2020 年7月より前に作成された検索サービスを使用している場合、スコアリング アルゴリズムは当時の既定のアルゴリズム(ClassicSimilarity)である可能性が高 く、その場合は、インデックスごとにアップグレードを行うことができます。詳 細については、「以前のサービスで BM25 スコアリングを有効にする」を参照し てください。\\n\\n次のビデオ セグメントでは、Azure Al Search で使用される一般提供の優先度付けアル ゴリズムの説明に早送りしています。詳しい背景情報については、ビデオ全編をご覧 ください。\\n\\nhttps://www.youtube-nocookie.com/embed/Y\\\\_X6USgvB1g? version=3&start=322&end=643\\n\\n\\n## BM25 ランク付けのしくみ\\n\\n関連性のスコアリングとは、検索スコア(@search.score) を計算することです。これ は、現在のクエリのコンテキストにおける項目の関連性のインジケーターとして機能し ます。範囲は無制限です。ただし、スコアが高いほど、項目の関連性が高くなりま す。\\n\\n検索スコアは、文字列入力とクエリ自体の統計プロパティに基づいて計算されます。 Azure Al Search では、検索語句に一致するドキュメント(searchModeに応じて一部ま たは全部) が検索され、検索語句の多くのインスタンスを含むドキュメントが優先され ます。データ インデックス全体での語句の出現頻度は低いがドキュメント内ではよく 使用されている場合、検索スコアはより高くなります。関連性を計算するこのアプロ :unselected: ーチの基礎となる手法は、TF-IDF (単語の出現頻度 - 逆文書頻度)と呼ばれています。\\n\\n検索スコアは、結果セット全体で繰り返すことができます。同じ検索スコアを持つ項 目が複数ヒットした場合、同じスコアを持つ項目の順序付けは定義されていないので安 定しません。クエリを再度実行すると、特に、複数のレプリカで無料のサービスまた は課金対象サービスを使用している場合は、項目の位置が変わる場合があります。同 ースコアの項目が2 つ存在する場合、どちらが最初に表示されるかは特定できませ ん。\\n\\n繰り返しスコアの間の関係を解除するには、$orderby 句を追加することで、まずスコ :unselected: アで並べ替えを行い、次に別の並べ替え可能なフィールド($orderby=search.score() desc, Rating desc など) で並べ替えを行うことができます。詳細については、$orderby に関するページを参照してください。\\n\\nスコア付けには、インデックスで searchable としてマークされているフィールド、ま たはクエリで searchFields としてマークされているフィールドが使用されます。\\n\\nretrievable としてマークされたフィールド、またはクエリの select で指定されたフ ィールドのみが、検索スコアと共に検索結果に返されます。\\n\\n\\n### 4 注意\\n\\n@search.score = 1 は、スコア付けまたは順位付けが行われていない結果セットを 示します。スコアは、すべての結果にわたって均一です。スコア付けされていな い結果が発生するのは、クエリ フォームがファジー検索、ワイルドカード、また は正規表現のクエリである場合、または空の検索である場合(search =\\\\* がフィル ターとペアで指定され、一致を返す主な手段がフィルターである場合があります) です。\\n\\n\\n## テキスト結果のスコア\\n\\n結果がランク付けされるたびに、@search.score プロパティには結果の順序付けに使用 される値が含まれます。\\n\\n次の表に、各一致、アルゴリズム、および範囲で返されるスコアリング プロパティを 示します。\\n\\n〔〕 テーブルを展開する\\n\\n|||||\\n| - | - | - | - |\\n| 検索メソッ ド || パラメーター スコアリング アルゴリズム  | Range |\\n| フルテキス ト検索 | @search.score | BM25 アルゴリズム。インデックスで指定されたパラ メーターを使用します。 | 無制 限。 |\\n\\n\\n## スコア バリエーション\\n\\n検索スコアは、一般的な意味での関連性を示すものであり、同じ結果セット内の他のド キュメントと比べた場合の一致の強さが反映されています。ただし、スコアは、ある クエリと次のものとの間で必ずしも一貫しているとは限らないため、クエリを操作して いると、検索ドキュメントの順序付け方法における小さな不一致に気付くことがありま す。これが発生する理由については、次のいくつかの説明があります。\\n\\n〔〕 テーブルを展開する\\n\\n| 原因 | 説明 | :unselected:  :selected:  :unselected: \\n| - | - |\\n|\\n|| |\\n|| |\\n|| |\\n| 同一のスコア | 複数のドキュメントのスコアが同じである場合、それらはいずれも最初に表示される可能 性があります。 | :selected: | |\\n|\\n|| |\\n| デ 一タ の 不 安定性   | ドキュメントを追加、変更、または削除すると、インデックス コンテンツは変動しま す。インデックスの更新が徐々に処理されるに従って用語頻度は変化し、一致するドキ ユメントの検索スコアに影響を与えます。 || |\\n|\\n|\\n|\\n|\\n| 複 数 の レプリカ     | 複数のレプリカを使用するサービスの場合、クエリは、各レプリカに対して並列に発行さ れます。検索スコアを計算するために使用されるインデックス統計はレプリカごとに計 算され、クエリ応答の中で結果がマージされ、順序付けられます。レプリカのほとんど || は互いのミラーですが、状態の小さな違いのために統計は異なる場合があります。たと || えば、あるレプリカで、他のレプリカからマージされた、その統計に寄与しているドキュ || メントが削除されることがあります。通常、レプリカごとの統計の違いは、小さなイン || デックスの方がより顕著です。この条件について詳しくは、以下のセクションで説明し ます。 |\\n\\n\\n### クエリ結果に対するシャーディング効果\\n\\n\"シャード\"とは、インデックスのチャンクです。Azure Al Search では、インデックス をさらに“シャード\"に分割し、(シャードを新しい検索単位に移動することによって) パーティションを追加するプロセスを高速化しています。検索サービスでは、シャー :unselected: ド管理は実装の詳細であり、構成できませんが、インデックスがシャード化されている とわかっていれば、順位付けやオートコンプリートの動作で不定期に発生する異常を容 易に把握できます。\\n\\n● 異常のランク付け: 検索スコアは最初にシャード レベルで計算され、続いて単位 の結果セットに集計されます。シャード コンテンツの特性に応じて、あるシャー ドからの一致が別のシャードの一致よりも高い順位になる場合があります。検索 結果の順位付けが直観に反しているように感じられる場合は、シャーディングの 影響が原因である可能性が最も高いです(特にインデックスが小さい場合)。イン デックス全体でグローバルにスコアを計算するように選択すれば、これらの順位 付けの異常を回避できますが、その場合、パフォーマンスが低下します。\\n:unselected:\\n● オートコンプリートの異常: オートコンプリート クエリでは、部分的に入力され た語句の最初のいくつかの文字で照合が行われますが、スペルの少しの間違いを 許容するあいまいパラメーターが使用されます。オートコンプリートの場合、あ いまい一致は現在のシャード内の用語に限定されます。たとえば、シャードに \"Microsoft\" が含まれており、\"micro\"という部分的な語句が入力された場合、検 索エンジンはそのシャード内の\"Microsoft\" と一致しますが、インデックスの残り の部分を保持した他のシャードでは一致しません。\\n\\n次の図は、レプリカ、パーティション、シャード、および検索単位間の関係を示してい ます。ここでは、2 つのレプリカと2つのパーティションを持つサービスで、どのよ うに1つのインデックスが 4 つの検索単位にわたっているかを例で示しています。4 つの検索単位それぞれには、インデックスのシャードの半分だけが格納されます。左 側の列の検索単位は、シャードの前半を格納して最初のパーティションを構成し、右側 の列の検索単位は、シャードの後半を格納して 2 番目のパーティションを構成してい ます。レプリカが 2 つあるため、各インデックス シャードのコピーは2 つあります。 上部の行の検索単位は、1 つのコピーを格納して最初のレプリカを構成し、下部の行の 検索単位は、別のコピーを格納して 2 番目のレプリカを構成しています。\\n\\n<figure>\\n\\n![](figures/34)\\n\\n<!-- FigureContent=\"Partition 1 Partition 2 Search Unit Search Unit Replica 1 Shard 1 Shard 2 Shard i Shard i+1 Shard i+2 Shard n Search Unit Search Unit Replica 2 Shard 1 Shard 2 ... Shard i Shard i+1 Shard i+2 ... Shard n\" -->\\n\\n</figure>\\n\\n\\n上の図は1つの例にすぎません。パーティションとレプリカはさまざまに組み合わせ ることができ、最大で合計 36 の検索単位が可能です。\\n\\n\\n#### 4 注意\\n\\nレプリカとパーティションの数は、均等に12 分割されます(具体的には、1、2、 3、4、6、12)。 Azure Al Search では各インデックスを 12 のシャードに事前に分 割して、それぞれがすべてのパーティションに均等に分散されるようにします。 たとえば、サービスに3 つのパーティションがあり、インデックスを作成する場 合、各パーティションにはインデックスの 4 つのシャードを含めます。 Azure Al Search でインデックスがどのようにシャードされるかは実装の詳細であり、今後 のリリースで変更される場合があります。今日 12 個であっても、今後も必ず 12 個になるとは限りません。\\n\\n# スコアリング統計とスティッキー セッション\\n\\nスケーラビリティのために、Azure Al Search はシャーディング プロセスを通じて各イ :unselected: ンデックスを水平方向に配布します。つまり、インデックスの 部分は物理的に個別で す。\\n\\n既定では、ドキュメントのスコアは、“シャード内\"のデータの統計プロパティに基づ :unselected: いて計算されます。このアプローチは、一般に、データの大規模なコーパスでは問題 にならず、すべてのシャードの情報に基づいてスコアを計算する必要がある場合よりも パフォーマンスが向上します。ただし、このパフォーマンスの最適化を使用すると、2 つの非常に類似したドキュメント(またはまったく同一のドキュメント)は、それぞれ が異なるシャードになる場合、関連性スコアが異なる可能性があります。\\n\\nすべてのシャードの統計プロパティに基づいてスコアを計算する場合、これを行うに :unselected: は、 scoringStatistics=global をクエリ パラメーターとして追加します(またはクエリ 要求の本文パラメーターとして \"scoringStatistics\":\"global\" を追加します)。\\n\\nHTTP\\n\\nPOST https://[service name].search.windows.net/indexes/hotels/docs/search? api-version=2020-06-30 {\\n\\n\"search\": \"<query string>\",\\n\\n\"scoringStatistics\": \"global\"\\n\\n}\\n\\nscoringStatistics を使用すると、同じレプリカのすべてのシャードで同じ結果が得ら れるようになります。ただし、レプリカはインデックスの最新の変更で常に更新され るため、それぞれ若干異なる場合があります。一部のシナリオでは、ユーザーが“クエ リセッション\"中により一貫した結果を得られるようにすることが必要な場合がありま す。このようなシナリオでは、クエリの一部として sessionId を指定できます。\\n\\nsessionId は、一意のユーザー セッションを参照するために作成する一意の文字列で す。\\n\\nHTTP\\n\\nPOST https://[service name].search.windows.net/indexes/hotels/docs/search? api-version=2020-06-30 {\\n\\n\"search\": \"<query string>\",\\n\\n\"sessionId\": \"<string>\"\\n\\n}\\n:unselected: :unselected:\\n同じ sessionId が使用されていれば、同じレプリカをターゲットにするためにベスト エフォートの試行が行われるので、ユーザーに表示される結果の一貫性が向上します。\\n\\n\\n## 4 注意\\n\\n同じ sessionId 値を繰り返し再利用すると、レプリカ間での要求の負荷分散が妨 げられ、検索サービスのパフォーマンスに悪影響を与える可能性があります。\\n\\nsessionld として使用される値は、\\'\\\\_\\'文字で始めることはできません。\\n\\n\\n## 関連性のチューニング\\n\\nAzure Al Search では、BM25 アルゴリズムパラメーターを構成し、次のメカニズムを 使用して検索の関連性を調整し、検索スコアを向上させることができます:\\n\\n〔〕 テーブルを展開する\\n\\n| アプローチ | 実装 | 説明 |\\n| - | - | - |\\n| スコアリング ア ルゴリズムの構 成 | Search index | |\\n| スコアリング プ ロファイル :unselected: :selected: | Search index | コンテンツ特性に基づいて一致の検索スコアを向上させる基準が指 定されます。たとえば、収益の見込みに基づいて一致をブーストし たり、より新しい項目のレベルを上げたり、場合によっては在庫期 間が長すぎる項目をブーストしたりできます。スコアリング プロ :unselected: :unselected: :unselected: | ファイルは、インデックス定義の一部であり、重み付けされたフィ ールド、関数、およびパラメーターで構成されます。スコアリング プロファイルの変更によって既存のインデックスを更新できます。 インデックスの再構築は必要ありません。\\n| セマンティック| クエリ ランク付け  要求 | 検索結果に機械読解を適用し、意味的に関連性の高い結果を上位に 昇格させます。 |\\n| featuresMode パラメーター | クエリ 要求 | このパラメーターは、多くの場合、スコアのアンパックに使用され ますが、カスタム スコアリングソリューションロを提供するコー ドで使用できます。 |\\n\\n\\n## featuresMode パラメーター(プレビュー)\\n\\nドキュメントの検索の要求には、フィールド レベルでの関連性に関するさらに詳細な 情報を提供できる新しい featuresMode パラメーターがあります。 @searchScore はド キュメント全体に対して計算されますが(このクエリのコンテキストにおけるこのドキ\\n\\nユメントの関連度)、featuresMode を使用すると、 @search. features 構造体で表現され た、個々のフィールドに関する情報を取得できます。この構造体には、クエリで使用 されるすべてのフィールド(クエリ内の searchFields を介した特定のフィールド、また はインデックス内で検索可能として属性が付けられているすべてのフィールド)が含ま れます。フィールドごとに、次の値が取得されます。\\n\\n● フィールド内で見つかった一意のトークン数\\n\\n● 類似性スコア。つまり、クエリ用語に対するフィールド内容の類似度のメジャー\\n\\n● 用語の頻度。つまり、フィールド内でクエリ用語が見つかった回数\\n\\n\"Description\" および \"title\" フィールドを対象とするクエリの場合、 @search. features を含む応答は次のようになります。\\n\\n<figure>\\n\\n![](figures/35)\\n\\n<!-- FigureContent=\"JSON \"value\": [ { \"@search.score\": 5.1958685, \"@search. features\": { \"description\": { \"uniqueTokenMatches\": 1.0, \"similarityScore\": 0.29541412, \"termFrequency\" : 2 }, \"title\": { \"uniqueTokenMatches\": 3.0, \"similarityScore\": 1.75451557, \"termFrequency\" : 6 } } } ]\" -->\\n\\n</figure>\\n\\n\\nカスタムのスコアリング ソリューションロでこれらのデータ ポイントを使用したり、 この情報を使用して検索の関連性の問題をデバッグしたりできます。\\n\\n\\n## フルテキスト クエリ応答のランク付けされた結 果の数\\n\\n既定では、改ページを使用していない場合、検索エンジンはフルテキスト検索では上位 50 位のランクの一致を返します。 top パラメーターを使用して、返される項目数を減 らしたり増やしたりできます(1 回の応答で 1000 個まで)。フルテキスト検索には、最 大 1,000 件の一致という制限が適用されます(API 応答の制限を参照)。1,000 件の一致 が見つかると、検索エンジンはそれ以上の検索を行いません。\\n\\n返す結果をさらに増やす、または減らすには、ページング パラメーター top、skip、 next を使用します。ページングは、各論理ページ上の結果の数を決定し、完全なペイ ロードを導く方法です。詳細については、「検索結果の操作方法」を参照してくださ :unselected: い。\\n\\n\\n## 関連項目\\n\\n· スコアリング プロファイル\\n :unselected:\\n● REST API リファレンス\\n\\n● ドキュメント API の検索\\n\\n· Azure Al Search .NET SDK\\n\\n# ベクトル検索での関連性\\n\\n[アーティクル]·2024/04/18\\n\\nベクトル クエリの実行時、検索エンジンは類似のベクトルを検索して、検索結果に返 される最適な候補を見つけます。ベクトル コンテンツのインデックス付け方法に応じ て、関連する一致の検索は網羅的であるか、ニアネイバーに制限されて処理が高速化さ れます。候補が見つかると、類似性メトリックを使用して、一致の強度に基づいて各 結果のスコアが付けられます。\\n\\nこの記事では、関連性のある一致を見つけるために使用されるアルゴリズムと、スコア リングに使用される類似性メトリックについて説明します。また、検索結果が期待に 沿っていない場合に関連性を向上させるヒントも提供します。\\n\\n\\n## ベクトル検索で使用されるアルゴリズム\\n\\nベクトル検索アルゴリズムには、完全な k ニアレスト ネイバー(KNN) や Hierarchical Navigable Small World (HNSW) があります。\\n\\n● 完全な KNN は、ベクトル空間全体をスキャンするブルート フォース検索を実行 します。\\n\\n● HNSW は、近似ニアレスト ネイバー(ANN) 検索を実行します。\\n\\n検索とスコアリングに使用されるのは、インデックスに searchable としてマークされ ているフィールド、またはクエリの searchFields としてマークされたベクトル フィー ルドだけです。\\n\\n\\n## 完全な KNN を使用する場合\\n\\n完全な KNN は、データ ポイントのすべてのペア間の距離を計算し、クエリ ポイント の正確な k ニアレスト ネイバーを見つけます。これは、高い再現率が最も重要であ り、ユーザーにクエリ待ち時間のトレードオフを受け入れる意思があるシナリオを対象 としています。計算負荷が高いため、小規模から中規模のデータセット、または精度 要件がクエリ パフォーマンスの考慮事項を上回る場合は、完全な KNN を使用します。\\n\\n2 番目の用途は、近似ニアレスト ネイバー アルゴリズムの呼び戻しを評価するデータ セットの構築です。完全な KNN を使用して、ニアレストネイバーのグラウンド トウ ルースのセットを構築できます。\\n\\n完全な KNN サポートは、2023-11-01 REST API および 2023-10-01-Preview REST API と、そのいずれかの REST API バージョンを対象とする Azure SDK クライアント ライブ\\n\\n<!-- PageHeader=\"ラリで利用できます。\" -->\\n\\n\\n## HNSW を使用する場合\\n\\nインデックス作成中に、HNSW は、データ ポイントを階層グラフ構造に編成して、よ り高速な検索のために追加のデータ構造を作成します。HNSW には、検索アプリケー ションのスループット、待機時間、および再現目標を達成するために調整できるいくつ かの構成パラメーターがあります。たとえば、クエリ時に、ベクトル フィールドに HNSW のインデックスが付いている場合でも、包括的な検索のオプションを指定でき ます。\\n\\nクエリの実行中、HNSW はグラフ内を移動して高速な近隣クエリを有効にします。こ のアプローチにより、検索の正確さと計算効率のバランスが取れます。HNSW は、大 規模なデータ セットを検索するときの効率が高いため、ほとんどのシナリオに推奨さ れます。\\n\\n\\n## ニアレストネイバー検索のしくみ\\n\\nベクトル クエリは、同じ埋め込みモデルから生成されたベクトルで構成される埋め込 み空間に対して実行されます。一般に、クエリ要求内の入力値は、ベクトル インデッ クスに埋め込みを生成したのと同じ機械学習モデルにフィードされます。出力は、同 じ埋め込み空間内のベクトルです。同様のベクトルが近接してクラスター化されるた め、一致を見つけることは、クエリ ベクトルに最も近いベクトルを見つけ、関連する ドキュメントを検索結果として返すのと同じです。\\n\\nたとえば、クエリ要求がホテルに関する場合、モデルは、ホテルに関するドキュメント を表すベクトルのクラスター内のどこかに存在するベクトルにクエリをマップします。 類似度メトリックに基づいて、クエリに最も似ているベクトルを特定すると、最も関連 性の高いドキュメントが決まります。\\n\\n完全な KNN に対してベクトル フィールドのインデックスが作成されると、クエリは \"all neighbors\" に対して実行されます。HNSW 用にインデックスが作成されたフィー ルドの場合、検索エンジンは HNSW グラフを使用して、ベクトル インデックス内のノ ードのサブセットを検索します。\\n\\n\\n## HNSW グラフの作成\\n\\nインデックス作成中、検索サービスは HNSW グラフを構築します。HNSW グラフに新 しいベクトルのインデックスを作成する目的は、効率的なニアレストネイバー検索でき るような方法でグラフ構造に追加することです。次の手順は、このプロセスをまとめ たものです。\\n\\n1\\\\. 初期化: 空の HNSW グラフ、または新しいインデックスでない場合は既存の HNSW グラフから開始します。\\n\\n2\\\\. エントリ ポイント: これは階層グラフの最上位レベルであり、インデックス作成 の開始点として機能します。\\n\\n3\\\\. グラフへの追加: 階層レベルが異なると、グラフの細分性が異なり、レベルが高い ほどグローバルになり、レベルが低いほど細かく表示されます。グラフ内の各ノ :unselected: ードは、ベクトル ポイントを表します。\\n\\n● 各ノードは、近隣にある最大 m 個のネイバーに接続されます。これが m パ ラメーターです。\\n\\n● 候補接続と見なされるデータ ポイントの数は、 efConstruction パラメータ ーによって管理されます。この動的リストは、アルゴリズムが考慮する既 存のグラフ内の最も近いポイントのセットを形成します。 efConstruction 値が大きいほど、多くのノード数が考慮され、多くの場合、各ベクトルの口 :unselected: ーカル近傍が高密度になります。\\n\\n● これらの接続では、構成された類似性 metric を使用して距離を決定しま す。一部の接続は、さまざまな階層レベルで接続する“長距離\"接続であ り、検索効率を向上させるショートカットをグラフに作成します。\\n\\n4\\\\. グラフの枝刈りと最適化: これは、すべてのベクトルのインデックス作成後に発生 する可能性があり、HNSW グラフのナビゲート性と効率が向上します。\\n\\n\\n## クエリ時の HNSW グラフ内の移動\\n\\nベクトル クエリは、一致をスキャンするために階層グラフ構造内を移動します。プロ :unselected: セスの手順の概要を次に示します:\\n\\n1\\\\. 初期化: アルゴリズムは、階層グラフの最上位レベルで検索を開始します。この エントリ ポイントには、検索の開始点として機能するベクトルのセットが含まれ ています。\\n\\n2\\\\. トラバーサル: 次に、グラフをレベルごとに走査し、最上位レベルから下位レベル に移動し、コサイン類似性など、構成された距離メトリックに基づいてクエリべ クトルに近い候補ノードを選択します。\\n\\n3\\\\. 枝刈り: 効率を向上させるために、アルゴリズムは、ニアレストネイバーを含む可 能性が高いノードのみを考慮して、検索領域を刈り込みます。これは、潜在的な 候補の優先順位キューを維持し、検索が進むにつれてそれを更新することによっ\\n\\nて達成されます。このキューの長さは、パラメーター efSearch によって構成さ れます。\\n\\n4\\\\. 絞り込み: アルゴリズムがより低く、より細かいレベルに移行すると、HNSW は クエリの近くでより多くのネイバーを考慮し、ベクトルの候補セットを絞り込 み、精度を向上させます。\\n\\n5\\\\. 完了: 検索は、ニアレストネイバーの必要な数が特定された場合、または他の停止 条件が満たされたときに完了します。このニアレスト米バーのこの必要な数は、 クエリ時間パラメーター k によって制御されます。\\n\\n\\n## 類似性の測定に使用される類似性メトリック\\n\\nアルゴリズムは、類似性を評価する候補ベクトルを検索します。このタスクを実行す るために、類似性メトリック計算では、候補ベクトルとクエリ ベクトルを比較し、類 似性を測定します。アルゴリズムは、検索された最も類似しているベクトルの順序付 けされたセットを追跡し続け、アルゴリズムが完了したときにランク付けされた結果セ ットを形成します。\\n\\n〔〕 テーブルを展開する\\n\\n|| メトリック 説明  |\\n| - | - |\\n| cosine | このメトリックは、2 つのベクトル間の角度を測定し、ベクトルの長さの違いによ る影響を受けません。数学的には、2 つのベクトル間の角度を計算します。コサ :unselected: インは Azure OpenAl 埋め込みモデルで使用される類似性メトリックであるため、 Azure OpenAl を使用している場合は、ベクトル構成で cosine を指定します。 |\\n| dotProduct | このメトリックは、2 つのベクトルの各ペアの長さと、それらの間の角度の両方を 測定します。数学的には、ベクトルの大きさとそれらの間の角度の積を計算しま す。正規化されたベクトルの場合、これは cosine 類似性と同じですが、パフォー マンスは若干高くなります。 |\\n| euclidean | (別名 12 norm) このメトリックは、2 つのベクトル間のベクトル長の差を測定しま す。 数学的には、2 つのベクトルの差の 12ノルムである2 つのベクトル間のユー クリッド距離を計算します。 |\\n\\n\\n## ベクトル検索結果のスコア\\n\\nスコアが計算されて、それぞれの一致に割り当てられ、最も高い一致が k の結果とし て返されます。 @search.score :unselected: プロパティにスコアが含まれています。次の表は、ス コアが該当する範囲を示しています。\\n:unselected:\\n<!-- PageHeader=\"〔〕 テーブルを展開する\" -->\\n\\n| 検索メソッド | パラメーター | スコアリング メトリック | Range |\\n| - | - | - | - |\\n| ベクトル検索 | @search.score | コサイン :unselected: | 0.333 - 1.00 |\\n\\ncosine メトリックについては、計算された @search.score がクエリ ベクトルとドキュ メント ベクトルの間のコサイン値ではないことに注意することが重要です。代わり に、Azure Al Search では、スコア関数が単調に減少するような変換が適用されます。 つまり、スコア値は、類似性が悪化すると常に値が減少します。この変換により、検 索スコアをランク付け目的で使用できます。\\n\\n類似性スコアにはいくつかの微妙な違いがあります\\n\\n● コサイン類似性は、2 つのベクトル間の角度のコサインとして定義されます。\\n :unselected:\\n● コサイン距離は 1 - cosine\\\\_similarity :unselected: として定義されます。\\n\\n単調に減少する関数が作成されるように @search.score は 1 / (1 + cosine\\\\_distance) として定義されます。\\n\\n合成値の代わりにコサイン値が必要な開発者は、数式を使用して、検索スコアをコサイ ン距離に戻すことができます:\\n\\nC#\\n\\ndouble ScoreToSimilarity (double score)\\n\\n{\\n\\ndouble cosineDistance = (1 - score) / score; return -cosineDistance + 1;\\n\\n}\\n\\n元のコサイン値を持たせておくと、低品質の結果をトリミングするためのしきい値を設 定するカスタム ソリューションにおいて有用です。\\n\\n\\n## 関連性のチューニングに関するヒント\\n\\n関連性のある結果が得られない場合は、クエリの構成の変更を試してください。ベク トル クエリには、スコアリング プロファイルやフィールド、用語ブーストなどの特定 のチューニング機能はありません。\\n\\n● チャンクサイズと重複について実験します。チャンクサイズを大きくし、チャ ンク間のコンテキストまたは継続性を維持するのに十分な重複を確保します。\\n:unselected: :unselected: :unselected:\\n● HNSW の場合は、近接グラフの内部構成を変更するさまざまなレベルの efConstruction を試します。既定値は 400 です。範囲は 100 \\\\~1,000 です。\\n\\n· チャット モデルを使用している場合は、k の結果を増やして、より多くの検索結 果をチャット モデルにフィードします。\\n\\n● セマンティック ランク付けを使用してハイブリッド クエリを試します。ベンチ マーク テストでは、この組み合わせが一貫して最も関連性の高い結果をもたらし ました。\\n\\n\\n## 次のステップ\\n\\n● クイックスタートを試す\\n\\n● 埋め込みの詳細\\n\\n· データ チャンクの詳細\\n\\n# Reciprocal Rank Fusion (RRF) を使用し たハイブリッド検索での関連性スコアリ ング\\n\\n[アーティクル]·2023/10/26\\n\\n\\n## 1 重要\\n\\nハイブリッド検索では、現在パブリック プレビュー段階にあるベクター機能を補 足の利用規約☑のもとに使用します。\\n\\nReciprocal Rank Fusion (RRF) は、以前にランク付けされた複数の結果の検索スコアを評 価して、統合された結果セットを生成するアルゴリズムです。Azure Cognitive Search では、並列で実行される 2 つ以上のクエリがある場合は、常に RRF が使用されます。 各クエリがランク付けされた結果セットを生成し、RRF はランク付けをマージして、ク エリ応答で返される 1つの結果セットに均質化するために使用されます。RRF が必要 なシナリオの例としては、ハイブリッド検索、同時に実行される複数のベクター クエ リなどがあります。\\n\\nRRF は、“逆順位\"の概念に基づいています。これは、検索結果のリスト内の最初の関 連ドキュメントのランクの逆数です。この手法の目標は、元のランキング内の項目の 位置を考慮して、複数のリストで上位にランク付けされた項目により高い重要性を与え ることです。これにより、最終的なランク付けの全体的な品質と信頼性が向上し、複 数の順序付けされた検索結果を融合するタスクに役立ちます。\\n\\n\\n## RRF ランク付けのしくみ\\n\\nRRF は、複数の方法から検索結果を取得し、結果の各ドキュメントに逆順位スコアを割 り当て、スコアを組み合わせて新しいランク付けを作成することで機能します。この 概念では、複数の検索方法で上位の位置に表示されるドキュメントはより関連性が高い 可能性があるため、結合された結果の上位にランク付けされます。\\n\\nRRF プロセスの簡単な説明を次に示します。\\n :unselected:\\n1. 並列で実行される複数のクエリからランク付けされた検索結果を取得します。\\n\\n2\\\\. ランク付けされた各リストの結果に対して逆順位スコアを割り当てます。 RRF は、各結果セットの一致ごとに新しい @search.score を生成します。検索結果の ドキュメントごとに、エンジンはリスト内の位置に基づいて逆順位スコアを割り\\n\\n当てます。スコアは 1/(rank + k) として計算されます。ここで、rank はリスト 内のドキュメントの位置であり、k は定数で、60 などの小さな値に設定されてい る場合に最適に実行されることが実験で確認されています。この k 値は RRF ア ルゴリズムの定数であり、最も近い近傍の数を制御する k とは完全に別の定数で あることに注意してください。\\n\\n3\\\\.スコアを結合します。各ドキュメントについて、エンジンは各検索システムから 取得した逆順位スコアを合計し、各ドキュメントの結合スコアを生成します。\\n\\n4\\\\. エンジンは、結合されたスコアに基づいてドキュメントをランク付けし、それら を並べ替えます。結果のリストは融合されたランキングです。\\n\\nスコアリングには、インデックスで searchable としてマークされているフィールド、 またはクエリで searchFields としてマークされているフィールドのみが使用されま す。 retrievable としてマークされたフィールド、またはクエリの select で指定され たフィールドのみが、検索スコアと共に検索結果に返されます。\\n\\n\\n## 並列クエリの実行\\n\\nRRF は、複数のクエリが実行されるたびに使用されます。次の例は、並列クエリ実行 が行われるクエリ パターンを示しています。\\n\\n● フルテキスト クエリと1 つのベクター クエリ(単純なハイブリッド シナリオ) は、2 つのクエリ実行に相当します。\\n\\n● フルテキスト クエリと、2 つのベクター フィールドを対象とする 1 つのベクター クエリは、3 つのクエリ実行に相当します。\\n\\n● フルテキスト クエリと、5 つのベクター フィールドを対象とする 2 つのベクター クエリは、11 個のクエリ実行に相当します\\n\\n\\n## ハイブリッド検索結果のスコア\\n\\n結果がランク付けされるたびに、@search.score プロパティには結果の順序付けに使用 される値が含まれます。スコアは、方法ごとに異なるランク付けアルゴリズムによっ て生成されます。各アルゴリズムには、独自の範囲と大きさがあります。\\n\\n次の表に、各関連性ランク付けアルゴリズムの各一致で返されるスコアリングプロパ ティ、アルゴリズム、スコアの範囲を示します。\\n\\n| 検索メソ ッド | パラメーター | スコアリング ア ルゴリズム | Range |\\n| - | - | - | - |\\n| フルテキ スト検索 (full-text search) | @search.score | BM25 アルゴリ ズム | 上限なし。 |\\n| ベクトル 検索 | @search.score | HNSW アルゴリ ズム。HNSW 構 成で指定された 類似性メトリッ クを使用しま す。 | 0.333 - 1.00 (Cosine)、Euclidean と DotProduct の場合は 0 から 1。 |\\n| ハイブリ ッド検索 | @search. score | RRF アルゴリズ ム | 上限は結合されるクエリの数によって 制限され、各クエリは RRF スコアに対 して最大約 1 を寄与します。たとえ ば、3 つのクエリをマージすると、2つ の検索結果のみがマージされる場合よ りも RRF スコアが高くなります。 |\\n| セマンテ ィックラ ンク付け | @search.rerankerScore | セマンティック ランク付け | 1.00 - 4.00 |\\n\\nセマンティック ランク付けは RRF には関係していません。そのスコア\\n\\n(@search.rerankerScore)は、常にクエリ応答で個別に報告されます。セマンティック ランク付けでは、フルテキスト検索結果とハイブリッド検索結果を再ランク付けでき、 それらの検索結果には意味的に豊富なコンテンツを持つフィールドが含まれていると仮 定しています。\\n\\n\\n## ハイブリッド クエリ応答のランク付けされた結 果の数\\n\\n既定では、改ページを使用していない場合、検索エンジンは、フルテキスト検索では上 位 50 位のランクの一致、ベクトル検索では最も類似した k 個の一致を返します。八 イブリッド クエリでは、top によって応答の結果の数が決まります。既定に基づい て、統合結果セットの上位 50 にランク付けされた一致が返されます。\\n\\n多くの場合、検索エンジンは top や k よりも多くの結果を見つけます。より多くの結 果を返すには、ページング パラメーター top、skip、 next を使用します。ページン グは、各論理ページ上の結果の数を決定し、完全なペイロードを導く方法です。\\n:unselected:\\nフルテキスト検索には、最大 1,000 件の一致という制限が適用されます(「API 応答の 制限」を参照)。1,000 件の一致が見つかると、検索エンジンはそれ以上の検索を行い ません。\\n\\n詳細については、「検索結果の操作方法」を参照してください。\\n\\n\\n## 関連項目\\n\\n● ハイブリッド検索の詳細を確認する\\n\\n● ベクトル検索の詳細を確認する\\n\\n# Azure Al Search のセキュリティの概要\\n\\n[アーティクル]·2024/04/11\\n\\nこの記事では、データと操作を保護する Azure Al Search のセキュリティ機能について 説明します。\\n\\n\\n## データ フロー(ネットワーク トラフィック パタ ーン)\\n\\nAzure Al Search サービスは Azure でホストされ、通常はパブリック ネットワーク接続 を使用してクライアント アプリケーションからアクセスされます。このようなパター ンとなることが多いですが、他のトラフィック パターンにも注意する必要がありま す。開発環境と運用環境をセキュリティ保護するには、すべてのエントリ ポイントと 送信トラフィックについて理解しておく必要があります。\\n\\nAzure Al Search には、3 つの基本的なネットワーク トラフィック パターンがありま す。\\n\\n● クライアントによって行われる、検索サービスへのインバウンド要求(主要なパタ ーン)\\n\\n● 検索サービスによって発行される、Azure やそれ以外の場所の他のサービスへの アウトバウンド要求\\n\\n● セキュリティ保護された Microsoft バックボーン ネットワークを介して行われる 内部サービス間の要求\\n\\n\\n## 受信トラフィック\\n\\n検索サービス エンドポイントを対象とする受信要求には、次が含まれます。\\n\\n● 検索サービスでオブジェクトを作成、読み取り、更新、または削除する\\n\\n● 検索ドキュメントのインデックスを読み込む\\n\\n● インデックスのクエリ\\n\\n● インデクサーまたはスキルセットの実行をトリガーする\\n\\nREST API のページで、検索サービスによって処理される受信要求の全範囲が説明され ています。\\n\\n少なくとも、すべての受信要求は、次のいずれかのオプションを使用して認証される必 要があります。\\n\\n● キーベースの認証(デフォルト)。受信要求が、有効な API キーを提供します。\\n\\n● ロールベースのアクセス制御。 認可は、検索サービスの Microsoft Entra ID とロ\\n :unselected: :unselected:\\nールの割り当て経由で行われます。\\n\\nさらに、ネットワーク セキュリティ機能を追加して、エンドポイントへのアクセスを さらに制限できます。IP ファイアウォールでの受信の規則、またはパブリック インタ ーネットから検索サービスを完全に遮断するプライベート エンドポイントのいずれか を作成することができます。\\n\\n\\n## 内部トラフィック\\n\\n内部要求は、Microsoft によってセキュリティで保護され、管理されます。これらの接 続を構成または制御することはできません。ネットワーク アクセスをロックダウンし :unselected: ている場合、顧客が内部トラフィックを構成できないため、顧客からのアクションは必 要ありません。\\n\\n内部トラフィックは次で構成されます。\\n\\n● タスク(Microsoft Entra ID 経由の認証と認可、Azure Monitor に送信されたリソー ス ログ、Azure Private Link を使用したプライベート エンドポイント接続など) の :unselected: サービス間呼び出し。\\n\\n● 組み込みスキルのための Azure AI サービス API への要求。\\n\\n● セマンティック ランク付けをサポートする機械学習モデルに対して行われた要 求。\\n\\n\\n## 送信トラフィック\\n\\n送信要求は、ユーザーがセキュリティで保護および管理できます。送信要求は、検索 サービスから他のアプリケーションに向けて発生します。通常、これらの要求は、ク エリ時にテキストベースのインデックス作成、スキルベースの AI エンリッチメント、 ベクター化を行うために、インデクサーによって行われます。送信要求には、読み取 りと書き込みの両方の操作が含まれます。\\n\\n次の一覧は、セキュリティで保護された接続を構成できる送信要求の完全な列挙です。 検索サービスは、検索自体のため、およびインデクサーまたはカスタム スキルのため に要求を行います。\\n\\n〔〕 テーブルを展開する\\n\\n| 操作 | シナリオ |\\n| - | - |\\n| インデク サー | 外部データ ソースに接続してデータを取得します。詳細については、「Azure ネッ トワーク セキュリティで保護されたコンテンツへのインデクサー アクセス」を参照 してください。 |\\n\\n| 操作 | シナリオ |\\n| - | - |\\n| インデク サー | Azure Storage に接続して、ナレッジストア、キャッシュされたエンリッチメント、 デバッグ セッションを持続させます。 |\\n| カスタム スキル | サービス外でホストされている外部コードを実行している Azure Functions、Azure Web アプリ、またはその他のアプリに接続します。スキルセットの実行中に、外部 処理に対する要求が送信されます。 |\\n| インデク サーと垂 直統合 | Azure OpenAl とデプロイされた埋め込みモデルに接続するか、カスタム スキルを経 :unselected: 由して、指定する埋め込みモデルに接続します。検索サービスは、インデックス作 成中にベクター化のために埋め込みモデルにテキストを送信します。 |\\n| ベクター 化 | クエリ時に Azure OpenAl またはその他の埋め込みモデルに接続して、ベクター検索 のためにユーザー テキスト文字列をベクターに変換します。 |\\n| 検索サー ビス | 機密データの暗号化および復号化に使用されるカスタマー マネージド暗号化キーを 取得するために、Azure Key Vault に接続します。 |\\n\\n送信接続は、キーまたはデータベース ログインを含むリソースのフル アクセス接続文 字列を使用して確立することも、Microsoft Entra ID とロールベースのアクセスを使用 している場合はマネージド ID を使用して確立することもできます。\\n\\nファイアウォールの内側にある Azure リソースにアクセスするには、他の Azure リソ ースに検索サービス要求を許可する受信規則を作成します。\\n\\nAzure Private Link によって保護された Azure リソースにアクセスするには、インデク サーが接続を確立するために使用する共有プライベート リンクを作成します。\\n\\n\\n### 同じリージョンの検索サービスとストレージ サービスの例外\\n\\nAzure Storage と Azure Al Search が同じリージョンにある場合、ネットワーク トラフ イックはプライベート IP アドレス経由でルーティングされ、Microsoft バックボーン ネットワークで発生します。プライベート IP アドレスが使用されるため、ネットワー ク セキュリティ用に IP ファイアウォールまたはプライベート エンドポイントを構成す ることはできません。\\n\\n次のいずれかの方法を使用して、同じリージョンの接続を構成します。\\n\\n● 信頼されたサービスの例外\\n\\n· リソース インスタンス ルール\\n\\nネットワークのセキュリティ\\n\\nネットワーク セキュリティは、ネットワーク トラフィックに制御を適用することによ り、未承認のアクセスや攻撃からリソースを保護します。Azure Al Search は、未承認 のアクセスに対する防御の前線になり得るネットワーク機能をサポートしています。\\n\\n\\n## IP ファイアウォール経由の受信接続\\n\\n検索サービスは、パブリック IP アドレスを使用して、アクセスを許可するパブリック エンドポイントによりプロビジョニングされます。パブリック エンドポイントを経由 するトラフィックを制限するには、特定の IP アドレスまたは IP アドレスの特定の範囲 から要求を許可する受信ファイアウォール規則を作成します。すべてのクライアント 接続は、許可された IP アドレスを使用して行う必要があります。それ以外の場合、接 続は拒否されます。\\n\\n<figure>\\n\\n![](figures/36)\\n\\n<!-- FigureContent=\"Deny Internet ER Private ER Gateway Peering On-premises App service Azure Cognitive Search ( Virtual Network (10.0.0.0/16) Azure Cognitive Search Virtual Network\" -->\\n\\n</figure>\\n\\n\\nファイアウォール アクセスを構成するには、ポータルを使用します。\\n\\nまたは、管理 REST API を使用します。API バージョン 2020-03-13 以降では、IpRule パラメーターを指定することで、検索サービスへのアクセスを付与する IP アドレスを 個別に、あるいは範囲で特定することで、サービスへのアクセスを制限できます。\\n\\n\\n## プライベート エンドポイントへの受信接続(ネットワーク 分離、インターネット トラフィックなし)\\n\\nより強力なセキュリティには、Azure Al Search のプライベート エンドポイントを確立 して、仮想ネットワーク上のクライアントが Private Link を介して、検索インデックス 内のデータに安全にアクセスできるようにします。\\n\\nプライベート エンドポイントでは、検索サービスに接続するために仮想ネットワーク のアドレス空間の IP アドレスが使用されます。クライアントと検索サービス間のネッ トワーク トラフィックは、仮想ネットワークおよび Microsoft バックボーン ネット ワーク上のプライベートリンクを経由することで、パブリック インターネット上での 露出を排除します。仮想ネットワークを使用すると、オンプレミス ネットワークやイ ンターネットで、リソース間の安全な通信が可能になります。\\n<figure>\\n\\n![](figures/37)\\n\\n<!-- FigureContent=\"Deny Internet Deny Internet ER Gateway 10.0.0.5 ER Private Peering Private Link > On-premises VM Private Endpoint Azure Cognitive Search (-> Virtual Network: (10.0 0.0/76)- Azure Cognitive Search Virtual Network\" -->\\n\\n</figure>\\n\\n\\nこのソリューションは最も安全ですが、追加のサービスを使用すると、さらなるコスト がかかります。そのため、使用の前に利点の詳細を明確に理解しておく必要がありま す。コストの詳細については、価格ページロを参照してください。これらのコンポー :unselected: ネントを連携させる方法の詳細については、こちらのビデオをご覧ください。プライ ベート エンドポイント オプションの説明は、ビデオの 5:48 から始まります。エンド ポイントを設定する方法については、Azure Al Search でのプライベート エンドポイン トの作成に関するページを参照してください。\\n\\n\\n## 認証\\n\\n検索サービス宛ての要求が承認された後も、要求が許可されているかどうかを判断する 認証と認可を受ける必要があります。Azure Al Search は、2 つの方法をサポートしま す。\\n\\n● Microsoft Entra 認証では、認証された ID として(要求ではなく)呼び出し元が確 立されます。Azure ロールの割り当てが認可を決定します。\\n :unselected:\\n● キーベースの認証は、API キーにより(呼び出し元のアプリやユーザーではなく) 要求に対して行われます。このキーは、要求が信頼できるソースからの要求であ ることを証明する、ランダムに生成された数字と文字で構成される文字列です。 キーは要求ごとに必要です。有効なキーの送信は、要求が信頼されたエンティテ イのものであることの証明と見なされます。\\n\\n両方の認証方法を使用することも、検索サービスで使用可能にしない方法を無効にする こともできます。\\n\\n\\n## 承認\\n\\nAzure Al Search には、サービス管理とコンテンツ管理のためのさまざまな認可モデル が用意されています。\\n\\nAzure サービス管理\\n\\nリソース管理は、Microsoft Entra テナント内のロールベースのアクセス制御によって 認可されます。\\n\\nAzure Al Search では、Resource Manager を使用して、サービスの作成または削除、 API キーの管理、サービスのスケーリング、セキュリティの構成が行われます。その ため、ポータル、PowerShell、Management REST API のどれを使用しているかにかか わらず、Azure で割り当てられているロールによって、これらのタスクを実行できるユ :unselected: ーザーが決定されます。\\n\\n3 つの基本ロール(所有者、共同作成者、閲覧者)が検索サービスの管理に適用されま す。ロールの割り当ては、サポートされている任意の方法(ポータル、PowerShell な :unselected: ど) を使用して行うことができ、サービス全体に適用されます。\\n\\n\\n## 4 注意\\n\\nAzure 全体のメカニズムを使用して、サブスクリプションまたはリソースをロック :unselected: し、管理者権限を持つユーザーが検索サービスを誤って、または許可なく削除し ないようにすることができます。詳細については、リソースのロックによる予期 :unselected: せぬ削除の防止に関するページを参照してください。\\n\\n\\n## コンテンツへのアクセスを承認する\\n\\nコンテンツ管理とは、検索サービスで作成およびホストされるオブジェクトを指しま :unselected: す。\\n\\n● ロールベースの認可の場合、Azure のロールの割り当てを使用して、操作に対す :unselected: る読み書きアクセスを確立します。\\n\\n● キーベースの認可の場合、API キーと修飾されたエンドポイントによってアクセ スが決定されます。エンドポイントはサービス自体、インデックス コレクショ ン、特定のインデックス、ドキュメント コレクション、特定のドキュメントなど である場合があります。連結されている場合、エンドポイント、操作(作成また は更新要求など)、キーの種類(管理者またはクエリ)によってコンテンツへのアク セスと操作が認可されます。\\n\\n\\n## インデックスへのアクセスの制限\\n\\nAzure ロールを使用している場合は、プログラムによって実行される限り、個々のイン :unselected: デックスに対するアクセス許可を設定できます。\\n\\nキーを使用すると、サービスに対する管理者キーを持っている人は誰でも、そのサービ スのインデックスの読み取り、変更、削除を行えます。インデックスが誤って削除さ\\n:unselected: :unselected: :unselected:\\nれたり、悪意によって削除されたりすることを防止するうえで、コード資産の社内ソー :unselected: ス管理は、望ましくないインデックスの削除または変更を元に戻すための解決策になり ます。Azure Al Search は可用性を確保するためにクラスター内のフェールオーバーを 備えていますが、インデックスの作成または読み込みに使用される専用コードを格納し たり実行したりしません。\\n\\nインデックス レベルでセキュリティ境界を必要とするマルチテナント ソリューション の場合、通常、アプリケーション コードの中間層でインデックス分離を処理します。 マルチテナントのユース ケースの詳細については、「マルチテナント Saas アプリケー ションと Azure Al Search の設計パターン」を参照してください。\\n\\n\\n## ドキュメントへのアクセスの制限\\n\\n\"行レベル セキュリティ\"とも呼ばれるドキュメント レベルのユーザー アクセス許可 は、Azure Al Search でネイティブにはサポートされていません。Azure Cosmos DB な ど、行レベル セキュリティを提供する外部システムからデータをインポートする場 合、Azure Al Search によってインデックス付けされているため、そのようなアクセス 許可はデータと共に転送されません。\\n\\n検索結果のコンテンツに対するアクセス許可が必要な場合、ユーザー ID に基づいてド キュメントを含めるか、除外するフィルターを適用する手法があります。この回避策 では、グループまたはユーザー ID を表す文字列フィールドをデータ ソースに追加しま す。このフィールドは、インデックスでフィルター可能にできます。次の表では、承 認されていないコンテンツの検索結果をトリミングする 2 つのアプローチについて説 :unselected: 明しています。\\n\\n〔〕 テーブルを展開する\\n\\n| アプローチ | 説明 |\\n| - | - |\\n| ID フィルターに基 づいたセキュリテ ィによるトリミン グ | ユーザー ID アクセス制御を実装する基本的なワークフローについて記載 しています。 また、インデックスへのセキュリティ ID の追加について取 り上げているほか、そのフィールドに対してフィルター処理を行い、禁止 されているコンテンツの結果をトリミングする方法について説明していま す。 |\\n| Microsoft Entra ID に基づくセキュリ ティ トリミング | この記事は前の記事を拡張したものであり、Azure クラウド プラットフォ ームの無料サービスの1つである Microsoft Entra ID から ID を取得する 手順について説明しています。 |\\n\\n<figure>\\n\\n![](figures/38)\\n\\n<!-- FigureContent=\"データの保存場所\" -->\\n\\n</figure>\\n\\n :unselected:\\nお客様は、検索サービスを設定するときに、顧客データがどこで格納および処理される かを決定する場所またはリージョンを選びます。構成した機能が別の Azure リソース に依存し、そのリソースが別のリージョンにプロビジョニングされている場合を除き、 Azure Al Search は、指定したリージョンの外部に顧客データを格納しません。\\n\\n現在、検索サービスが書き込む唯一の外部リソースは Azure Storage です。ストレー ジ アカウントは、お客様が指定したストレージ アカウントであり、任意のリージョン に存在する可能性があります。エンリッチメントキャッシュ、デバッグ セッション、 ナレッジ ストアのいずれかの機能を使用する場合、検索サービスは Azure Storage に 書き込みます。\\n\\n\\n## データ所在地のコミットメントに対する例外\\n\\nオブジェクト名は、お客様が選んだリージョンまたは場所以外の場所に格納され、処理 されます。お客様は、名前のフィールドに機密データを配置することや、これらのフ ィールドに機密データが格納されるように設計したアプリケーションを作成することは できません。このデータは、Microsoft がサービスのサポートを提供するために使うテ レメトリ ログに表示されます。オブジェクト名には、インデックス、インデクサー、 :unselected: データ ソース、スキルセット、リソース、コンテナー、キー コンテナー ストアの名前 が含まれます。\\n\\nテレメトリ ログは 1年半保持されます。その間、Microsoft は次の条件下でオブジェ :unselected: クト名にアクセスして参照する場合があります。\\n\\n● 問題の診断、機能の改善、バグの修正を行います。このシナリオでは、データ ア クセスは内部のみであり、サードパーティがアクセスすることはありません。\\n\\n● サポート中、問題への迅速な解決策を提供し、必要に応じて製品チームを昇格さ せるために、この情報が使用される場合があります\\n\\n\\n### データ保護\\n\\nストレージ層には、インデックスやシノニム マップ、およびインデクサー、データ ソ ース、スキルセットの定義など、ディスクに保存されるすべてのサービス マネージド コンテンツに対するデータ暗号化が組み込まれています。サービス マネージド暗号化 :unselected: は、長期データ ストレージと一時データ ストレージの両方に適用されます。\\n\\n必要に応じて、インデックス付きコンテンツの補足暗号化用にカスタマー マネージド キー(CMK) を追加し、保存データの二重暗号化を行うことができます。2020 年8月1 日以降に作成されたサービスでは、CMK 暗号化は一時ディスクの短期データにも拡張 されています。\\n:unselected: :unselected: :selected: :selected:\\n## 転送中のデータ\\n\\nAzure Al Search では、暗号化は接続時および転送時に開始されます。パブリックイン ターネット上の検索サービスでは、Azure Al Search によって HTTPS ポート 443 がリッ スンされます。すべてのクライアントとサービスの間の接続では、TLS 1.2 暗号化が使 用されます。より前のバージョン(1.0 または 1.1) はサポートされていません。\\n\\n\\n## 保存データ\\n\\n検索サービスによって内部で処理されるデータについて、次の表でデータ暗号化モデル を説明しています。ナレッジ ストア、インクリメンタル エンリッチメント、インデク サーベースのインデックス作成などの一部の機能は、他の Azure サービスのデータ構 造から読み書きされます。Azure Storage に依存するサービスでは、そのテクノロジの 暗号化機能を使用できます。\\n\\n〔〕 テーブルを展開する\\n\\n| モデ ル | キー | 必要条 件 | 制限 | 適用対象 |\\n| - | - | - | - | - |\\n| サー バー 側暗 号化 | Microsoft 1 マネージド キー | なし (組み込 み) | なし。2018 年1月24日以 降に作成されたコンテンツ については、すべてのリー ジョンのすべての階層で使 用できます。 | データ ディスクおよび一時デ ィスク上のコンテンツ(インデ ックスとシノニム マップ) と定 義(インデクサー、データ ソー ス、スキルセット) |\\n| サー バー 側暗 号化 | カスタマー マネージド キー | Azure Key Vault | 2020 年 8月1日以降に作 成されたコンテンツについ ては、特定のリージョンの 請求対象階層で使用できま す。 | データ ディスク上のコンテン ツ(インデックスとシノニム マ ップ) |\\n| サー バー 側の 完全 暗号 化 | カスタマー マネージド キー | Azure Key Vault | 2021 年5月13日以降の検 索サービスについては、す べてのリージョンの請求対 象階層で使用できます。 | データ ディスクおよび一時デ ィスク上のコンテンツ(インデ ックスとシノニム マップ) |\\n\\n\\n## サービス マネージド キー\\n\\nサービス マネージド暗号化とは、256 ビット AES 暗号化☑を使用する Microsoft 内部 操作です。 (2018 年1月より前に作成された)完全に暗号化されていないインデックス に対する増分更新を含む、すべてのインデックス作成で自動的に行われます。\\n\\nサービス マネージド暗号化は、長期および短期ストレージ上のすべてのコンテンツに 適用されます。\\n\\n\\n### カスタマー マネージド キー(CMK)\\n\\nカスタマー マネージド キーには、Azure Key Vault という別の請求対象のサービスが必 要です。これのリージョンは別であってもかまいませんが、Azure Al Search と同じサ ブスクリプションのものである必要があります。\\n\\nCMK のサポートは、2 つのフェーズでロールアウトされました。最初のフェーズで検 索サービスを作成した場合、CMK 暗号化は長期ストレージと特定のリージョンに制限 されていました。2021 年5月以降の 2 番目のフェーズで作成されたサービスでは、任 意のリージョンで CMK 暗号化を使用できます。2 番目のウェーブのロールアウトの一 環として、コンテンツは長期ストレージと短期ストレージの両方で CMK 暗号化されま す。CMK のサポートの詳細については、「完全二重暗号化」を参照してください。\\n\\nCMK での暗号化を有効にすると、インデックスのサイズが増加し、クエリのパフォー マンスが低下します。これまでの観測に基づくと、実際のパフォーマンスはインデッ クスの定義やクエリの種類によって異なりますが、クエリ時間が 30 から 60 パーセン ト増加することが予想されます。パフォーマンスへの悪影響があるため、この機能を 本当に必要とするインデックスでのみ有効にすることをお勧めします。詳細について は、Azure Al Search でのカスタマー マネージド暗号化キーの構成に関するページを参 照してください。\\n\\n\\n## セキュリティと管理\\n\\n\\n### API キーを管理する\\n\\nAPI キーベースの認証に依存するということは、Azure のセキュリティのベスト プラク ティスに従って、定期的に管理者キーを再生成するための計画を立てる必要があること を意味します。Search サービスごとに最大2 個の管理キーがあります。API キーのセ キュリティと管理の詳細については、API キーの作成と管理に関する記事を参照してく ださい。\\n\\n\\n### アクティビティとリソースのログ\\n\\nAzure Al Search では、ユーザー ID はログに記録されないため、特定のユーザーに関す :unselected: る情報のログを参照することはできません。ただし、このサービスでは、\\n :unselected:\\nログの作 :unselected: 成、読み取り、更新、削除の各操作がログに記録されるため、これらのログを他のログ :unselected: :unselected: :unselected: と関連付けて、特定のアクションの機関を理解できる場合があります。\\n\\nAzure でアラートとログ記録インフラストラクチャを使用すると、クエリ ボリューム :unselected: の急増や、予想されるワークロードから逸脱したその他のアクションを検出できます。 :unselected: ログの設定の詳細については、ログ データの収集と分析およびクエリ要求の監視に関 :unselected: :unselected: する記事を参照してください。\\n\\n\\n## 認定資格とコンプライアンス\\n\\nAzure Al Search は通常の監査に参加し、パブリック クラウドと Azure Government の 両方について、グローバル、リージョン、および業界固有のさまざまな標準に対して認 :unselected: 定を受けています。完全な一覧については、公式の監査レポート ページから Microsoft Azure Compliance Offerings ホワイトペーパームをダウンロードしてくださ :unselected: い。\\n\\nコンプライアンスのために、Microsoft クラウド セキュリティ ベンチマークの安全性 の高いベスト プラクティスを、Azure Policy を使用して実装できます。Microsoft クラ ウド セキュリティ ベンチマークは、サービスやデータに対する脅威を軽減するために 実行する必要のある主要なアクションにマップされる、セキュリティ コントロールに :unselected: 体系化された、セキュリティに関する推奨事項を集めたものです。現在は、ネットワ ークセキュリティ、ログ記録および監視、データ保護などを含む 12 のセキュリティ :unselected: コントロールがあります。\\n :unselected: :unselected:\\nAzure Policy は、Microsoft クラウド セキュリティ ベンチマークの標準を含む複数の標 準に対するコンプライアンスの管理に役立つ、Azure に組み込まれた機能です。広く 知られたベンチマークについては、コンプライアンス非対応の場合に使用できる、基準 と実施可能な対応の両方の組み込みの定義が、Azure Policy によって提供されていま す。\\n\\nAzure Al Search には、現在 1つの定義が組み込まれています。これはリソース ログ用 :unselected: です。リソース ログが欠落している検索サービスを識別するポリシーを割り当てて、 :unselected: 有効にできます。詳細については、「Azure Al Search 用の Azure Policy 規制コンプラ イアンス コントロール」を参照してください。\\n :unselected:\\n\\n## このビデオを観る\\n\\nセキュリティ アーキテクチャと各機能カテゴリの概要については、こちらのビデオを ご覧ください。\\n\\nhttps://learn.microsoft.com/Shows/Al-Show/Azure-Cognitive-Search-Whats-new-in- security/player\\n\\n\\n## 関連項目\\n:unselected: :unselected: :unselected:\\n● Azure セキュリティの基礎\\n\\n· Azure Security ZZ\\n\\n· Microsoft Defender for Cloud\\n\\n# Azure ネットワーク セキュリティで保護 されたコンテンツへのインデクサー アク セス\\n\\n[アーティクル]·2024/05/06\\n\\n概念に関するこの記事では、Azure リソースが Azure 仮想ネットワークにデプロイされ :unselected: ている場合に、検索インデクサーがネットワーク セキュリティによって保護されてい るコンテンツにアクセスする方法について説明します。送信トラフィック パターンと インデクサー実行環境について説明します。また、Azure Al Search でサポートされる ネットワーク保護と、セキュリティ戦略に影響を与える可能性のある要因についても説 明します。最後に、Azure Storage はデータ アクセスと永続ストレージの両方に使用 されるため、この記事では、検索とストレージの接続に固有のネットワークの考慮事項 についても説明します。\\n\\n代わりに詳細な手順をお探しですか? インデクサー アクセスを許可するようにファイア ウォール規則を構成する方法またはプライベート エンドポイントを介して送信接続を 行う方法に関する記事を参照してください。\\n\\n\\n## インデクサーによってアクセスされるリソース\\n\\nAzure Al 検索インデクサーは、次の3 つの状況において、さまざまな Azure リソース への送信呼び出しを行うことができます。\\n\\n● インデックス作成中に外部のデータ ソースに接続する場合\\n\\n● カスタム スキルを含むスキルセットを介して外部のカプセル化されたコードに接 続する場合\\n\\n● スキルセットの実行中に Azure Storage に接続してエンリッチメントをキャッシ ュしたり、デバッグ セッションの状態を保存したり、ナレッジ ストアに書き込ん だりする場合\\n\\n通常の実行でインデクサーがアクセスする可能性がある Azure リソースの種類を次の 表に一覧表示します。\\n\\n〔〕 テーブルを展開する\\n\\n| リソース | インデクサー実行内の目的 |\\n| - | - |\\n| Azure Storage (BLOB, ADLS Gen 2、ファイル、テーブル) | データ ソース |\\n\\n| リソース | インデクサー実行内の目的 |\\n| - | - |\\n| Azure Storage (BLOB、テーブル) | スキルセット(エンリッチメントのキャッシュ、セッション のデバッグ、ナレッジ ストアのプロジェクション) |\\n| Azure Cosmos DB (さまざまな API) | データ ソース |\\n| Azure SQL データベース | データ ソース |\\n| Azure 仮想マシン上の SQL Server | データ ソース |\\n| SQL Managed Instance | データ ソース |\\n| Azure Functions | スキルセットに接続され、カスタム Web API スキルのホス ティングに使用される |\\n\\n\\n### 4 注意\\n\\nインデクサーは、組み込みのスキルのために Azure AI サービスにも接続します。 ただし、その接続は内部ネットワーク経由で行われ、制御下のネットワークプロ :unselected: ビジョニングの対象になりません。\\n\\nインデクサーは、次の方法を使用してリソースに接続します。\\n\\n● パブリック エンドポイントと資格情報\\n\\n● Azure Private Link を使用したプライベート エンドポイント\\n\\n● 信頼されたサービスとしての接続\\n\\n● IP アドレス指定を通して接続する\\n\\nAzure リソースが仮想ネットワーク上にある場合は、プライベート エンドポイントま たは IP アドレス指定を使用して、データへのインデクサー接続を許可する必要があり ます。\\n\\n\\n## サポートされているネットワーク保護\\n\\nAzure リソースは、Azure によって提供される任意の数のネットワーク分離メカニズム を使用して保護できます。リソースとリージョンに応じて Azure Al Search インデクサ ーは、次の表に示す制限付きで、IP ファイアウォールとプライベート エンドポイント 経由で送信接続を行うことができます。\\n\\n<!-- PageFooter=\"〔〕 テーブルを展開する\" -->\\n\\n| リソース | IP 制限 | プライベート エンドポ イント |\\n| - | - | - |\\n| Azure Storage のテキストベースの インデックス作成 (BLOB、ADLS Gen 2、ファイル、テーブル) || ストレージ アカウントと検索サ サポートされています | ービスが異なるリージョンにあ る場合にのみサポートされま す。 \\n| Azure Storage の AI エンリッチメ ント(キャッシュ、デバッグ セッシ ョン、ナレッジ ストア) || ストレージ アカウントと検索サ サポートされています | ービスが異なるリージョンにあ る場合にのみサポートされま す。 \\n| NoSQL 用 Azure Cosmos DB | サポートされています | サポートされています |\\n| Azure Cosmos DB for MongoDB | サポートされています | サポートされていない |\\n| Azure Cosmos DB for Apache Gremlin | サポートされています | サポートされていない |\\n| Azure SQL データベース | サポートされています | サポートされています |\\n| Azure 仮想マシン上の SQL Server | サポートされています | 該当なし |\\n| SQL Managed Instance | サポートされています | 該当なし |\\n| Azure Functions | サポートされています | Azure Functions の特定 の層に対してのみサポ ートされます |\\n\\n\\n## インデクサー実行環境\\n\\nAzure Al Search には、ジョブの特性に基づいて処理を最適化する\"インデクサー実行環 境\" の概念があります。2 つの環境があります。IP ファイアウォールを使用して Azure リソースへのアクセスを制御している場合は、実行環境について理解しておくと、両方 の環境を含む IP 範囲を設定するのに役立ちます。\\n\\n指定されたインデクサーの実行に対し、Azure Al Search で、そのインデクサーを実行 するための最適な環境が決定されます。インデクサーは、割り当てられているタスク の数と種類に応じて、2 つの環境のどちらかで実行されます。\\n\\n[] テーブルを展開する\\n\\n実行 説明 環境\\n\\nプラ 検索サービスの内部。プライベート環境で実行されているインデクサーは、コンピュー イベ ティング リソースを、同じ検索サービス上の他のインデックス作成およびクエリのワー\\n\\n実行 説明 環境\\n\\nート クロードと共有します。通常、この環境では、テキストベースのインデックス作成(スキ ルセットを使わない)を実行するインデクサーのみが実行されます。インデクサーとデー タの間にプライベート接続を設定する場合は、これが使用できる唯一の実行環境です。\\n\\nマル 追加料金なしで Microsoft によって管理およびセキュリティ保護されます。これは、ご自\\n\\nチテ 分の管理下にあるどのネットワーク プロビジョニングの対象にもなりません。この環境\\n\\nナン は、大量のコンピューティング処理を要する処理の負荷を軽減して、サービス固有のリソ\\n\\nト 1 ースをルーチン処理に残しておくために使います。リソースを大量に消費するインデク サー ジョブの例には、スキルセットのアタッチ、大規模なドキュメントの処理、大量の ドキュメントの処理などがあります。\\n\\n\\n## インデクサー実行の IP 範囲の設定\\n\\nこのセクションでは、どちらの実行環境からの要求でも許可する IP ファイアウォール 構成について説明します。\\n\\nAzure リソースがファイアウォールの内側に存在する場合は、インデクサー要求を発信 できるすべての IP に対してインデクサー接続を許可する受信規則を設定します。これ には、検索サービスで使用される IP アドレスと、マルチテナント環境で使用される IP アドレスが含まれます。\\n\\n● 検索サービス(およびプライベート環境) の IP アドレスを取得するには、 nslookup (または ping)で検索サービスの完全修飾ドメイン名(FQDN) を使用します。パブ リック クラウドの検索サービスの FQDN は、 <service-name>.search.windows.net です。\\n\\n● インデクサーが実行される可能性のあるマルチテナント環境の IP アドレスを取得 するには、 AzureCognitiveSearch サービス タグを使用します。\\n\\nAzure サービス タグには、リージョンごとのマルチテナント環境の公開された IP アドレス範囲が含まれています。これらの IP は、Discovery API またはダウンロ :unselected: ード可能な JSON ファイルを使用して調べることができます。IP 範囲はリージョ ン別に割り当てられるため、開始する前に検索サービスのリージョンを確認して ください。\\n\\n\\n## Azure SQL の IP 規則の設定\\n\\nマルチテナント環境の IP 規則を設定する場合、特定の SQL データ ソースで IP アドレ スの指定に対する単純なアプローチがサポートされます。規則内のすべての IP アドレ :unselected: スを列挙する代わりに、 AzureCognitiveSearch サービス タグを指定する ネットワーク セキュリティ グループ規則を作成できます。\\n\\nデータ ソースが次のいずれかである場合は、サービス タグを指定できます。\\n\\n● Azure 仮想マシン上の SQL Server\\n\\n● SQL マネージド インスタンス\\n\\nマルチテナント環境の IP 規則にサービス タグを指定した場合でも、nslookup から取 得した、プライベート実行環境(検索サービスそのものを意味する) の明示的な受信規 則が必要であることに注意してください。\\n\\n\\n## 接続方法の選択\\n\\n検索サービスは、仮想マシン上でネイティブに実行されている特定の仮想ネットワーク にプロビジョニングすることはできません。一部の Azure リソースでは仮想ネットワ ーク サービス エンドポイントが提供されますが、この機能は Azure AI 検索では提供さ れません。次のいずれかの方法の実装を計画してください。\\n\\n〔〕 テーブルを展開する\\n\\n| アプローチ | 詳細 |\\n| - | - |\\n| Azure リソー スへの受信 接続をセキ ュリティで 保護する | インデクサーによるデータの要求を許可する受信ファイアウォール規則を Azure リソースに構成します。ファイアウォール構成には、マルチテナント実行のサ ービス タグと検索サービスの IP アドレスを含める必要があります。インデクサ ーへのアクセスを許可するファイアウォール規則の構成に関する記事を参照して ください。 |\\n| Azure Al 検 索と Azure リソースの 間のプライ ベート接続 | リソースへの接続のために検索サービスによって排他的に使用される共有プライ ベート リンクを構成します。接続は内部ネットワークを経由し、パブリックイ ンターネットをバイパスします。リソースが完全にロックダウンされている場 合(保護された仮想ネットワークで実行されている場合、またはパブリック接続 で使用できない場合)、プライベート エンドポイントが唯一の選択肢となりま す。プライベート エンドポイントを経由した送信接続の作成に関するページを 参照してください。 |\\n\\nプライベート エンドポイント経由の接続は、検索サービスのプライベート実行環境か ら開始する必要があります。\\n\\nIP ファイアウォールの構成は無料です。Azure Private Link に基づくプライベート エン ドポイントは、課金に影響します。詳細については、「Azure Private Link の価格☑」 :unselected: をご覧ください。\\n\\nネットワーク セキュリティを構成したら、続いてロールの割り当てによって、どのユ :unselected: ーザーとグループにデータと操作に対する読み取りと書き込みのアクセス権があるかを 指定します。\\n\\n# プライベート エンドポイントの使用に関する考慮事項\\n\\nこのセクションでは、プライベート接続オプションに絞って説明します。\\n\\n● 共有プライベート リンクには、課金対象の検索サービスが必要です。その最小レ ベルは、テキスト ベースのインデックス作成向けの Basic、またはスキル ベース のインデックス作成向けの Standard 2 (S2) のいずれかです。詳細については、プ ライベート エンドポイントの数に対するレベルの制限に関する記事を参照してく ださい。\\n\\n● 共有プライベート リンクが作成されると、それは検索サービスで常にその特定の Azure リソースへのすべてのインデクサー接続に使用されます。プライベート接 続はロックされ、内部的に強制されます。パブリック接続のためにプライベート :unselected: 接続をバイパスすることはできません。\\n\\n● 課金対象の Azure Private Link リソースが必要です。\\n\\n● サブスクリプション所有者がプライベート エンドポイント接続を承認する必要が あります。\\n\\n● インデクサーのマルチテナント実行環境をオフにする必要があります。\\n\\nこれを行うには、インデクサーの executionEnvironment を \"Private\" に設定しま す。この手順により、すべてのインデクサー実行が、検索サービス内でプロビジ ヨニングされたプライベート環境に限定されます。この設定のスコープは、検索 サービスではなくインデクサーです。すべてのインデクサーをプライベート エン ドポイント経由で接続する場合は、それぞれに次の構成が必要です。\\n\\n<figure>\\n\\n![](figures/39)\\n\\n<!-- FigureContent=\"JSON { \"name\" : \"myindexer\", ... other indexer properties \"parameters\" : { ... other parameters \"configuration\" : { ... other configuration properties \"executionEnvironment\": \"Private\" } } }\" -->\\n\\n</figure>\\n\\n\\nリソースに対して承認されたプライベート エンドポイントができると、private に設定 されているインデクサーは、Azure リソース用に作成および承認されたプライベート リンクを介してアクセスを取得しようとします。\\n\\nAzure Al 検索で、プライベート エンドポイントの呼び出し元に適切なロールの割り当 :unselected: てがなされていることが検証されます。たとえば、読み取り専用のアクセス許可があ るストレージ アカウントへのプライベート エンドポイント接続を要求した場合、この 呼び出しは拒否されます。\\n\\nプライベート エンドポイントが承認されていない場合、またはインデクサーがプライ ベート エンドポイント接続を使用していない場合は、インデクサーの実行履歴に transientFailure エラー メッセージが表示されます。\\n\\n\\n## トークン認証を使用してネットワーク セキュリ ティを補完する\\n\\nファイアウォールとネットワーク セキュリティは、データと操作への未承認のアクセ\\n\\nスを防ぐための最初の手順です。次の手順となるのが承認です。\\n\\nロールベースのアクセスをお勧めします。この場合、Microsoft Entra ID のユーザーと :unselected: グループは、サービスへの読み取りと書き込みのアクセス権を決定するロールに割り当 :unselected: てられます。組み込みロールの説明とカスタム ロールを作成する手順については、 :unselected: :unselected: :unselected: ールベースのアクセス制御を使用した Azure AI 検索への接続に関するページを参照し てください。\\n\\nキーベースの認証が必要ない場合は、API キーを無効にし、ロールの割り当てのみを使 :unselected: 用することをお勧めします。\\n\\n\\n## ネットワークで保護されたストレージ アカウン トへのアクセス\\n\\n検索サービスでは、インデックスとシノニム リストを格納します。ストレージを必要 とするその他の機能の場合、Azure Al Search は Azure Storage に依存します。エンリ ッチメント キャッシュ、デバッグ セッション、ナレッジ ストアは、このカテゴリに分 類されます。各サービスの場所と、ストレージ用のネットワーク保護によって、デー タ アクセス戦略が決まります。\\n\\n\\n## 同じリージョンのサービス\\n\\nAzure Storage では、ファイアウォール経由でアクセスするには、要求が別のリージョ ンから送信されている必要があります。Azure Storage と Azure Al Search が同じリー ジョンにある場合は、検索サービスのシステム ID の下にあるデータにアクセスして、 ストレージ アカウントの IP 制限を回避できます。\\n:selected: :selected:\\nシステム ID を使用してデータ アクセスをサポートするには、次の 2 つのオプションが あります。\\n\\n● Azure Storage で信頼済みサービスとして実行し、信頼済みサービスの例外を使用 するように検索を構成します。\\n\\n● Azure リソースからの受信要求を許可するリソース インスタンス ルールを Azure Storage で構成します。\\n\\n上記のオプションは認証用の Microsoft Entra ID によって異なります。つまり、 Microsoft Entra ログインで接続する必要があります。現在、ファイアウォール経由の :unselected: 同じリージョン接続では、Azure Al Search システム割り当てマネージド ID のみがサポ ートされています。\\n\\n\\n## 異なるリージョンのサービス\\n\\n検索とストレージが異なるリージョンにある場合は、前述のオプションを使用するか、 お使いのサービスからの要求を許可する IP ルールを設定できます。ワークロードによ :unselected: っては、次のセクションで説明するように、複数の実行環境のルールを設定する必要が ある場合があります。\\n\\n\\n## 次のステップ\\n\\nAzure 仮想ネットワークにデプロイされたソリューションのインデクサー データ アク :unselected: セスオプションについて理解したら、次の手順として次のハウツー記事のいずれかを 確認します。\\n\\n● プライベート エンドポイントへのインデクサー接続を確立する方法\\n\\n● IP ファイアウォールを介してインデクサー接続を確立する方法\\n\\n# Azure Al Search の Azure Policy 規制コ ンプライアンスコントロール\\n\\n[アーティクル]·2024/02/29\\n\\nAzure Policy を使用して Microsoft クラウド セキュリティ ベンチマークのレコメンデー ションを適用している場合は、準拠していないサービスを識別して修正するためにポリ シーを作成できることは、既にご存じかもしれません。このようなポリシーは、カス タムの場合もあれば、よく知られたベスト プラクティスのためのコンプライアンス条 件と適切なソリューションを提供する組み込み定義に基づく場合もあります。\\n\\nAzure Al Search の場合、現在、以下に示す1 つの組み込み定義があり、ポリシー割り 当てで使用できます。組み込みは、ログ記録と監視のためのものです。作成するポリ :unselected: シーでこの組み込み定義を使用すると、システムによってリソース ログのない検索サ :unselected: ービスがスキャンされ、それに応じて有効にされます。\\n\\nAzure Policy の規制コンプライアンスにより、さまざまなコンプライアンス基準に関連 するコンプライアンス ドメインおよびセキュリティ コントロールに対して、“組み込 み\" と呼ばれる、Microsoft によって作成および管理されるイニシアチブ定義が提供さ れます。このページでは、Azure Al Search のコンプライアンス ドメインおよびセキュ リティ コントロール の一覧を示します。セキュリティ コントロールの組み込みを個 別に割り当てることで、Azure リソースを特定の基準に準拠させることができます。\\n\\n各組み込みポリシー定義のタイトルは、Azure portal のポリシー定義にリンクしていま す。[ポリシーのバージョン]列のリンクを使用すると、Azure Policy GitHub リポジト リムのソースを表示できます。\\n\\n\\n## 1 重要\\n\\n各コントロールは、1 つ以上の Azure Policy. 定義に関連付けられています。これ :unselected: らのポリシーは、コントロールのコンプライアンスの評価に役立つ場合がありま\\n :unselected: :unselected:\\nす。ただし、多くの場合、コントロールと1つ以上のポリシーとの間には、一対 :unselected: :unselected: 一、または完全な一致はありません。そのため、Azure Policy での準拠は、ポリ シー自体のみを指しています。これによって、コントロールのすべての要件に完 :unselected: 全に準拠していることが保証されるわけではありません。また、コンプライアン ス標準には、現時点でどの Azure Policy 定義でも対応されていないコントロール が含まれています。したがって、Azure Policy でのコンプライアンスは、全体の コンプライアンス状態の部分的ビューでしかありません。これらのコンプライア ンス標準に対するコントロールと Azure Policy 規制コンプライアンス定義の間の :unselected: 関連付けは、時間の経過と共に変わることがあります。\\n:unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected: :unselected:\\n# CIS Microsoft Azure Foundations Benchmark\\n\\n\\n## 1.3.0\\n\\nすべての Azure サービスに対して使用可能な Azure Policy 組み込みがこのコンプライア ンス標準にどのように対応するのかを確認するには、Azure Policy の規制コンプライア ンス- CIS Microsoft Azure Foundations Benchmark 1.3.0 に関するページを参照してく ださい。このコンプライアンス標準の詳細については、CIS Microsoft Azure Foundations Benchmark△ に関するページを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| Domain || コント コントロールのタイトル :unselected: :unselected: ロール ID  :unselected: :unselected: | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| 5 ログと :unselected: 監視 | 5.3 | 診断ログがそれをサポートする :unselected: すべてのサービスで有効になっ ていることを確認する | Search サービスのリソー ス ログを有効にする必 :unselected: 要がある☑ | 5.0.0 ℃ |\\n\\n\\n## CIS Microsoft Azure Foundations Benchmark 1.4.0\\n\\nすべての Azure サービスで使用可能な Azure Policy の組み込みがこのコンプライアンス 標準にどのように対応しているのかを確認するには、CIS v1.4.0 に関する Azure Policy の規制コンプライアンスの詳細に関する記事を参照してください。このコンプライア ンス標準の詳細については、CIS Microsoft Azure Foundations Benchmarkご に関するペ ージを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| Domain || コント コントロールのタイトル :unselected: :unselected: ロール ID  :unselected: :unselected: | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| 5 ログと :unselected: 監視 | 5.3 | 診断ログがサポートされているす :unselected: べてのサービスに対して有効にな っていることを確認する。 | Search サービスのリソ ース ログを有効にする :unselected: 必要がある☑ | 5.0.0亿 |\\n\\nCIS Microsoft Azure Foundations Benchmark 2.0.0\\n\\nすべての Azure サービスで使用可能な Azure Policy の組み込みがこのコンプライアンス 標準にどのように対応しているのかを確認するには、CIS v2.0.0 に関する Azure Policy の規制コンプライアンスの詳細に関する記事を参照してください。このコンプライア ンス標準の詳細については、CIS Microsoft Azure Foundations Benchmarkロ に関するペ ージを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| Domain :unselected: :unselected: || コント コントロールのタイトル ロール ID :unselected: :unselected: | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| 5 | 5.4 | Azure Monitor リソース ログが、それ :unselected: をサポートするすべてのサービスで 有効になっていることを確認します | Search サービスのリ ソース ログを有効に :unselected: する必要がある☑ | 5.0.0 亿 |\\n\\n\\n## CMMC レベル 3\\n\\nすべての Azure サービスで使用可能な Azure Policy 組み込みがこのコンプライアンス標 準にどのように対応するのかを確認するには、Azure Policy の規制コンプライアンス- CMMC レベル3に関する記事をご覧ください。このコンプライアンス標準の詳細につ いては、サイバーセキュリティ成熟度モデル認定(CMMC)△ に関するドキュメントを ご覧ください。\\n\\n〔〕 テーブルを展開する\\n\\n| Domain | コントロ :unselected: :unselected: ール ID | コントロールのタイトル :unselected: | ポリシー (Azure portal) | ポリシー のバージ ヨン (GitHub) |\\n| - | - | - | - | - |\\n| アクセス 制御 | AC.1.001 | 情報システムへのアクセスを、許 可されているユーザー、許可され ているユーザーの代わりに動作す るプロセス、およびデバイス(他の 情報システムを含む) に制限する。 | Azure Al Services !J ソースでネットワー ク アクセスを制限 する必要がある☑ | 3.1.0亿 |\\n| アクセス 制御 | AC.1.002 | 情報システムへのアクセスを、許 可されているユーザーが実行を許 可されているトランザクションお よび機能の種類に制限する。 | Azure Al Services !J ソースでネットワー ク アクセスを制限 する必要がある☑ | 3.1.0亿 |\\n| アクセス 制御 | AC.2.016 | 承認された認可に従って CUI のフ :unselected: ーを制御する。 | Azure Al Services !J ソースでネットワー | 3.1.0亿 |\\n\\n| Domain || コントロ コントロールのタイトル :unselected: ール ID  :unselected: | ポリシー (Azure portal) | ポリシー のバージ ヨン (GitHub) |\\n| - | - | - | - | - |\\n| | | | ク アクセスを制限 する必要がある☑ | |\\n| 構成管理 | CM.3.068 | 不要なプログラム、関数、ポー ト、プロトコル、およびサービス の使用を制限、無効化、または禁 止する。 | Azure Al Services !J ソースでネットワー ク アクセスを制限 する必要がある☑ | 3.1.0亿 |\\n| システム と通信の 保護 | SC.1.175 | 組織システムの外部境界と主要な 内部境界で、通信(つまり、組織シ ステムによって送受信される情報) を監視、制御、および保護する。 | Azure Al Services ! ソースでネットワー ク アクセスを制限 する必要がある☑ | 3.1.0亿 |\\n| システム と通信の 保護 | SC.3.183 | ネットワーク通信トラフィックを 既定で拒否し、ネットワーク通信 トラフィックを例外的に許可する (つまり、すべて拒否し、例外的に 許可する)。 | Azure Al Services !J ソースでネットワー クアクセスを制限 する必要がある☑ | 3.1.0亿 |\\n\\n\\n## FedRAMP High\\n\\nすべての Azure サービスに対して使用可能な Azure Policy 組み込みがこのコンプライア ンス標準にどのように対応するのかを確認するには、Azure Policy の規制コンプライア ンス- FedRAMP High に関するページを参照してください。このコンプライアンス標 準の詳細については、FedRAMP High ☑ に関するページを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| Domain | コント :unselected: :unselected: ロール ID | コントロール :unselected: のタイトル | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| アクセス制御 | AC-2 | アカウント管 理 | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n| アクセス制御 | AC-2 (1) | システム アカ ウント管理の 自動化 | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n| アクセス制御 | AC-2 (7) | :unselected: ールベース のスキーム | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n\\n| Domain  :unselected:| コント :unselected: ロール ID  :selected: | コントロール :unselected: のタイトル| ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| アクセス制御 | AC-3 | アクセスの適 用 | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n| アクセス制御 | AC-4 | 情報フローの :unselected: 適用 | Azure Al Services リソースでネットワ ーク アクセスを制限する必要がある☑ | 3.1.0 亿 |\\n| アクセス制御 | AC-4 | 情報フローの :unselected: 適用 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n|| アクセス制御 AC-4  | 情報フローの :unselected: 適用 | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-4 | 情報フローの :unselected: 適用 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 | リモート アク セス | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 | リモート アク セス | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 (1) | 自動監視/制 御 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 (1) | 自動監視/制 御 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| 監査とアカウ ンタビリティ | AU-6 (4) | 一元的なレビ ユーと分析 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0 亿 |\\n| 監査とアカウ ンタビリティ | AU-6 (5) | 統合またはス キャンと監視 機能 | Search サービスのリソース ログを有効 :unselected: :selected:  にする必要がある☑| 5.0.0 亿 |\\n| 監査とアカウ ンタビリティ | AU-12 | 監査の生成 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0 亿 |\\n| 監査とアカウ ンタビリティ | AU-12 (1) | システム全体 または時間相 関の監査証跡 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0 亿 |\\n| 識別と認証 | IA-2 | 識別と認証 (組織のユーザ | Azure Al Services リソースのキー アク セスが無効になっている必要がありま | 1.1.0 亿 |\\n\\n| Domain  :unselected:| コント :unselected: ロール ID | コントロール :unselected: :selected:  のタイトル| ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| | | 一) | す(ローカル認証を無効にする)☑ :unselected: | |\\n| 識別と認証 | IA-4 | 識別子の管理 | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Al Services リソースでネットワ ーク アクセスを制限する必要がある☑ | 3.1.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n|| システムと通 SC-7  信の保護 | 境界保護 | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセスポイ ント | Azure Al Services リソースでネットワ ーク アクセスを制限する必要がある☑ | 3.1.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセス ポイ ント | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセス ポイ ント | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセス ポイ ント | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n\\n\\n## FedRAMP Moderate\\n\\nすべての Azure サービスに対して使用可能な Azure Policy 組み込みがこのコンプライア ンス標準にどのように対応するのかを確認するには、Azure Policy の規制コンプライア ンス- FedRAMP Moderate に関するページを参照してください。このコンプライアン ス標準の詳細については、FedRAMP Moderate △ に関するページを参照してくださ い。\\n\\n<!-- PageFooter=\"〔〕 テーブルを展開する\" -->\\n\\n| Domain  :unselected:| コント :unselected: ロール ID | コントロー :unselected: :unselected: ルのタイト ル | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| アクセス制御 | AC-2 | アカウント 管理 | Azure Al Services リソースのキー アク セスが無効になっている必要があります ローカル認証を無効にする)☑ :unselected: :selected: | 1.1.0 亿 |\\n| アクセス制御 | AC-2 (1) | システムア カウント管 理の自動化 | Azure Al Services リソースのキー アク セスが無効になっている必要があります ローカル認証を無効にする)☑ :unselected: :selected: | 1.1.0 ℃ |\\n| アクセス制御 | AC-2 (7) | :unselected: ールベー スのスキー ム | Azure Al Services リソースのキー アク セスが無効になっている必要があります (ローカル認証を無効にする)☑ :unselected: :selected: | 1.1.0 亿 |\\n| アクセス制御 | AC-3 | アクセスの 適用 | Azure Al Services リソースのキー アク セスが無効になっている必要があります (ローカル認証を無効にする)☑ :unselected: :selected: | 1.1.0 亿 |\\n| アクセス制御 | AC-4 | 情報フロー :unselected: の適用 | Azure Al Services リソースでネットワー ク アクセスを制限する必要がある☑ | 3.1.0 ℃ |\\n| アクセス制御 | AC-4 | 情報フロー :unselected: の適用 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-4 | 情報フロー :unselected: の適用 | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0 亿 |\\n|| アクセス制御 AC-4  | 情報フロー :unselected: の適用 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 ℃ |\\n| アクセス制御 | AC-17 | リモートア クセス | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 | リモートア クセス | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 (1) | 自動監視/制 御 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 (1) | 自動監視/制 御 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| 監査とアカウ ンタビリティ | AU-12 | 監査の生成 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0 亿 |\\n\\n| Domain  :unselected:| コント :unselected: ロール ID | コントロー :unselected: :unselected:  ルのタイト ル| ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| 識別と認証 | IA-2 | 識別と認証 (組織のユー ザー) | Azure Al Services リソースのキー アク セスが無効になっている必要があります :unselected: :selected:  ローカル認証を無効にする)☑| 1.1.0 亿 |\\n| 識別と認証 | IA-4 | 識別子の管 理 | Azure Al Services リソースのキー アク セスが無効になっている必要があります :unselected: :selected:  ローカル認証を無効にする)☑| 1.1.0亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Al Services リソースでネットワー ク アクセスを制限する必要があるで | 3.1.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセスポ イント | Azure Al Services リソースでネットワー クアクセスを制限する必要がある☑ | 3.1.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセスポ イント | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセスポ イント | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0 ℃ |\\n| システムと通 信の保護 | SC-7 (3) | アクセスポ イント | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n\\n\\n## HIPAA HITRUST 9.2\\n\\nすべての Azure サービスに対して使用可能な Azure Policy 組み込みがこのコンプライア ンス標準にどのように対応するのかを確認するには、Azure Policy の規制コンプライア ンス- HIPAA HITRUST 9.2 に関するページを参照してください。このコンプライアンス 標準の詳細については、HIPAA HITRUST 9.2亿 に関するページを参照してください。\\n\\n<!-- PageFooter=\"〔〕 テーブルを展開する\" -->\\n\\n| Domain | コントロール ID :unselected: :unselected:  :unselected: :unselected: | コントロールのタイト ル| ポリシー (Azure portal) | ポリシー のバージ ヨン (GitHub) |\\n| - | - | - | - | - |\\n| 12 監査口 :unselected: グと監視 | 1208.09aa3System.1- 09.aa | 1208.09aa3System.1- 09.aa 09.10 監視 | Search サービスの リソース ログを有 :unselected: 効にする必要があ る已 | 5.0.0亿 |\\n\\n\\n## Microsoft クラウド セキュリティ ベンチマーク\\n\\nMicrosoft クラウド セキュリティ ベンチマークでは、Azure 上のクラウド ソリューシ ョンをセキュリティで保護する方法に関する推奨事項が提供されます。このサービス を完全に Microsoft クラウド セキュリティ ベンチマークにマップする方法について は、「Azure Security Benchmark mapping files ☑ 」 (Azure セキュリティ ベンチマーク のマッピング ファイル)を参照してください。\\n\\nすべての Azure サービスに対して使用可能な Azure Policy 組み込みを、このコンプライ アンス基準に対応させる方法については、Azure Policy の規制コンプライアンス- Microsoft クラウド セキュリティ ベンチマークに関するページを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| Domain || コント コントロールのタイ :unselected: :unselected: ロール トル ID  :unselected: :unselected: :selected: :selected: :selected: | ポリシー (Azure portal) | ポリシー のバージ ョン (GitHub) |\\n| - | - | - | - | - |\\n|| ネットワーク NS-2  のセキュリテ イ | ネットワーク制御を 使用してクラウド サ ービスをセキュリテ イで保護します | Azure Al Services リソースでネッ トワーク アクセスを制限する必 要がある☑ | 3.1.0亿 |\\n| ID 管理 | IM-1 | 一元的な ID および 認証システムを使用 する | Azure Al Services リソースのキー アクセスが無効になっている必要 があります(ローカル認証を無効 :unselected: にする)☑ | 1.1.0亿 |\\n| ログと脅威検 :unselected: 出 | LT-3 | セキュリティ調査の ためのログを有効に :unselected: する | Search サービスのリソース ログ :unselected: を有効にする必要がある☑ | 5.0.0 亿 |\\n\\n<!-- PageFooter=\"NIST SP 800-171 R2\" -->\\n\\nすべての Azure サービスに対して使用可能な Azure Policy 組み込みがこのコンプライア ンス標準にどのように対応するのかを確認するには、Azure Policy の規制コンプライア ンス- NIST SP 800-171 R2 に関するページを参照してください。このコンプライアン ス標準の詳細については、NIST SP 800-171 R2 d に関するページを参照してくださ い。\\n\\n〔〕 テーブルを展開する\\n\\n| [ドメ イン] || コント コントロールのタイトル :unselected: :unselected: ロール ID  :unselected: :unselected: | ポリシー (Azure portal) | ポリシー のバージ ヨン (GitHub) |\\n| - | - | - | - | - |\\n| アクセ ス制御 | 3.1.1 | 承認されているユーザー、承認さ れているユーザーの代わりに動作 するプロセス、およびデバイス(他 のシステムを含む) へのシステム アクセスを制限する。 | Azure Al Services リソース のキー アクセスが無効に なっている必要があります (ローカル認証を無効にす :unselected: る)☑ | 1.1.0 亿 |\\n| アクセ ス制御 | 3.1.1 | 承認されているユーザー、承認さ れているユーザーの代わりに動作 するプロセス、およびデバイス(他 のシステムを含む) へのシステム アクセスを制限する。 | Azure Cognitive Search + ービスでは、プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセ ス制御 | 3.1.1 | 承認されているユーザー、承認さ れているユーザーの代わりに動作 するプロセス、およびデバイス(他 のシステムを含む) へのシステム アクセスを制限する。 | Azure Cognitive Search + ービスはプライベートリ ンクを使用する必要がある☑ | 1.0.0 ℃ |\\n| アクセ ス制御 | 3.1.12 | リモート アクセス セッションの監 視および制御を行う。 | Azure Cognitive Search ービスでは、プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0亿 |\\n| アクセ ス制御 | 3.1.12 | リモート アクセス セッションの監 視および制御を行う。 | Azure Cognitive Search + ービスはプライベート リ ンクを使用する必要がある☑ | 1.0.0 ℃ |\\n| アクセ ス制御 | 3.1.13 | リモート アクセス セッションの機 密性を保護するため暗号化メカニ ズムを採用する。 | Azure Cognitive Search + ービスでは、プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセ ス制御 | 3.1.13 | リモート アクセス セッションの機 密性を保護するため暗号化メカニ ズムを採用する。 | Azure Cognitive Search + ービスはプライベート リ ンクを使用する必要がある☑ | 1.0.0 亿 |\\n\\n| [ドメ イン] || コント コントロールのタイトル :unselected: :unselected: ロール ID  :unselected: :unselected: | ポリシー (Azure portal) | ポリシー のバージ ヨン (GitHub) |\\n| - | - | - | - | - |\\n| アクセ ス制御 | 3.1.14 | 管理対象のアクセス制御ポイント を介してリモート アクセスをルー ティングする。 | Azure Cognitive Search ービスでは、プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセ ス制御 | 3.1.14 | 管理対象のアクセス制御ポイント を介してリモート アクセスをルー ティングする。 | Azure Cognitive Search + ービスはプライベート リ ンクを使用する必要がある☑ | 1.0.0 亿 |\\n| アクセ ス制御 | 3.1.2 | システム アクセスを、許可された ユーザーが実行を許可されている トランザクションおよび機能の種 類に限定する。 | Azure Al Services リソース のキー アクセスが無効に なっている必要があります (ローカル認証を無効にす :unselected: :selected:  る)☑| 1.1.0 亿 |\\n| アクセ ス制御 :selected: | 3.1.3 | 承認された認可に従って CUI のフ コーを制御する。 :unselected: | Azure Al Services リソース でネットワーク アクセス を制限する必要がある☑ | 3.1.0 ℃ |\\n| アクセ ス制御 :selected: | 3.1.3 | 承認された認可に従って CUI のフ ローを制御する。 :unselected: | Azure Cognitive Search + ービスでは、プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセ ス制御 | 3.1.3 | 承認された認可に従って CUI のフ コーを制御する。 :unselected: | Azure Cognitive Search ₺ ービスではパブリック ネ ットワーク アクセスを無 効にする必要がある☑ | 1.0.0 亿 |\\n| アクセ ス制御 | 3.1.3 | 承認された認可に従って CUI のフ コーを制御する。 :unselected: | Azure Cognitive Search + ービスはプライベート リ ンクを使用する必要がある☑ | 1.0.0 亿 |\\n| システ ムと通 信の保 護 | 3.13.1 | 組織システムの外部境界と主要な 内部境界で、通信(つまり、組織シ ステムによって送受信される情報) を監視、制御、および保護する。 | Azure Al Services リソース でネットワーク アクセス を制限する必要がある☑ | 3.1.0 ℃ |\\n| システ ムと通 信の保 護 | 3.13.1 | 組織システムの外部境界と主要な 内部境界で、通信(つまり、組織シ ステムによって送受信される情報) を監視、制御、および保護する。 | Azure Cognitive Search + ービスでは、プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 ℃ |\\n| システ ムと通 | 3.13.1 | 組織システムの外部境界と主要な 内部境界で、通信(つまり、組織シ | Azure Cognitive Search + ービスではパブリックネ | 1.0.0 亿 |\\n\\n| [ドメ イン] || コント コントロールのタイトル :unselected: :unselected: ロール ID  :unselected: :unselected: | ポリシー (Azure portal) | ポリシー のバージ ヨン (GitHub) |\\n| - | - | - | - | - |\\n| 信の保 護 | | ステムによって送受信される情報) を監視、制御、および保護する。 | ットワーク アクセスを無 効にする必要がある☑ | |\\n| システ ムと通 信の保 護 | 3.13.1 | 組織システムの外部境界と主要な 内部境界で、通信(つまり、組織シ ステムによって送受信される情報) を監視、制御、および保護する。 | Azure Cognitive Search ₺ ービスはプライベート リ ンクを使用する必要がある☑ | 1.0.0 亿 |\\n| システ ムと通 信の保 護 | 3.13.2 | 組織のシステム内で効果的な情報 セキュリティを促進するアーキテ クチャ設計、ソフトウェア開発手 法、システム エンジニアリングの 原則を採用する。 | Azure Al Services リソース でネットワーク アクセス を制限する必要がある☑ | 3.1.0 亿 |\\n| システ ムと通 信の保 護 | 3.13.2 | 組織のシステム内で効果的な情報 セキュリティを促進するアーキテ クチャ設計、ソフトウェア開発手 法、システム エンジニアリングの 原則を採用する。 | Azure Cognitive Search + ービスでは、プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| システ ムと通 信の保 護 | 3.13.2 | 組織のシステム内で効果的な情報 セキュリティを促進するアーキテ クチャ設計、ソフトウェア開発手 法、システム エンジニアリングの 原則を採用する。 | Azure Cognitive Search ₺ ービスではパブリック ネ ットワーク アクセスを無 効にする必要がある☑ | 1.0.0 亿 |\\n| システ ムと通 信の保 護 | 3.13.2 | 組織のシステム内で効果的な情報 セキュリティを促進するアーキテ クチャ設計、ソフトウェア開発手 法、システム エンジニアリングの 原則を採用する。 | Azure Cognitive Search + ービスはプライベート リ ンクを使用する必要がある☑ | 1.0.0 亿 |\\n| システ ムと通 信の保 護 | 3.13.5 | 内部ネットワークから物理的また は論理的に分離されている、公的 にアクセス可能なシステム コンポ ーネントのサブネットワークを実 装する。 | Azure Al Services リソース でネットワーク アクセス を制限する必要がある☑ | 3.1.0亿 |\\n| システ ムと通 信の保 護 | 3.13.5 | 内部ネットワークから物理的また は論理的に分離されている、公的 にアクセス可能なシステム コンポ ーネントのサブネットワークを実 装する。 | Azure Cognitive Search + ービスでは、プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 ℃ |\\n| システ ムと通 | 3.13.5 | 内部ネットワークから物理的また は論理的に分離されている、公的 | Azure Cognitive Search ービスではパブリック ネ | 1.0.0 亿 |\\n\\n| [ドメ イン] || コント コントロールのタイトル :unselected: :unselected: ロール ID  :unselected: :unselected: | ポリシー (Azure portal) | ポリシー のバージ ヨン (GitHub) |\\n| - | - | - | - | - |\\n| 信の保 護 | | にアクセス可能なシステム コンポ ーネントのサブネットワークを実 装する。 | ットワーク アクセスを無 効にする必要がある☑ | |\\n| システ ムと通 信の保 護 | 3.13.5 | 内部ネットワークから物理的また は論理的に分離されている、公的 にアクセス可能なシステム コンポ ーネントのサブネットワークを実 装する。 | Azure Cognitive Search + ービスはプライベート リ ンクを使用する必要がある☑ | 1.0.0 亿 |\\n| システ ムと通 信の保 護 | 3.13.6 | ネットワーク通信トラフィックを 既定で拒否し、ネットワーク通信 トラフィックを例外的に許可する (つまり、すべて拒否し、例外的に 許可する)。 | Azure Al Services リソース でネットワーク アクセス を制限する必要がある☑ | 3.1.0 亿 |\\n| システ ムと通 信の保 護 | 3.13.6 | ネットワーク通信トラフィックを 既定で拒否し、ネットワーク通信 トラフィックを例外的に許可する (つまり、すべて拒否し、例外的に 許可する)。 | Azure Cognitive Search + ービスではパブリック ネ ットワーク アクセスを無 効にする必要がある☑ | 1.0.0 亿 |\\n| 監査と アカウ ンタビ リティ | 3.3.1 | 違法または承認されていないシス テム アクティビティの監視、分 析、調査、および報告を有効にす るために必要な範囲までシステム 監査ログとレコードを作成して保 :unselected: :unselected:  持する| Search サービスのリソー ス ログを有効にする必要 がある☑ :unselected: | 5.0.0 亿 |\\n| 監査と アカウ ンタビ リティ | 3.3.2 | 個々のシステム ユーザーのアクシ ョンからそのユーザーまで一意に 確実にたどれるようにし、彼らが 自分のアクションの責任を負える ようにする。 | Search サービスのリソー スログを有効にする必要 :unselected: がある☑ | 5.0.0 亿 |\\n| 識別と 認証 | 3.5.1 | システム ユーザー、ユーザーの代 わりに動作するプロセス、および :unselected: デバイスを特定する。 | Azure Al Services リソース のキー アクセスが無効に なっている必要があります (ローカル認証を無効にす :unselected: る)☑ | 1.1.0 亿 |\\n| 識別と 認証 | 3.5.2 | 組織システムへのアクセスを許可 するための前提条件として、ユー ザー、プロセス、またはデバイス の ID を認証(または検証) する。 | Azure Al Services リソース のキー アクセスが無効に なっている必要があります (ローカル認証を無効にす :unselected: る)☑ | 1.1.0 亿 |\\n\\n| [ドメ イン]  :unselected:  :unselected: || コント コントロールのタイトル :unselected:| ポリシー | ポリシー |\\n|| ロール :unselected: ID | | (Azure portal) | のバージ ヨン (GitHub) |\\n| - | - | - | - | - |\\n| 識別と 認証 | 3.5.5 | 定義された期間、識別子の再利用 を防止する。 | Azure Al Services リソース のキー アクセスが無効に なっている必要があります (ローカル認証を無効にす :unselected: る)☑ | 1.1.0 ℃ |\\n| 識別と 認証 | 3.5.6 | 定義された非アクティブな期間の 経過後に識別子を無効にする。 | Azure Al Services リソース のキー アクセスが無効に なっている必要があります (ローカル認証を無効にす :unselected: る)☑ | 1.1.0 亿 |\\n\\n\\n## NIST SP 800-53 Rev. 4\\n\\nすべての Azure サービスに対して使用可能な Azure Policy 組み込みがこのコンプライア ンス標準にどのように対応するのかを確認するには、Azure Policy の規制コンプライア ンス- NIST SP 800-53 Rev. 4 に関するページを参照してください。このコンプライア ンス標準の詳細については、NIST SP 800-53 Rev. 4亿 に関するページを参照してくださ い。\\n\\n〔〕 テーブルを展開する\\n\\n|| [ドメイン] コント  ロール :unselected: :unselected:  ID| コントロール のタイトル :unselected: :unselected: :selected: | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| アクセス 制御 | AC-2 | アカウント管 理 | Azure Al Services リソースのキー アクセ スが無効になっている必要があります ローカル認証を無効にする)で :unselected: | 1.1.0亿 |\\n| アクセス 制御 | AC-2 (1) | システム アカ ウント管理の 自動化 | Azure Al Services リソースのキー アクセ スが無効になっている必要があります ローカル認証を無効にする)☑ :unselected: | 1.1.0亿 |\\n| アクセス 制御 | AC-2 (7)| :unselected: :selected:  ールベース のスキーム | Azure Al Services リソースのキー アクセ スが無効になっている必要があります (ローカル認証を無効にする)☑ :unselected: | 1.1.0亿 |\\n| アクセス 制御 | AC-3 | アクセスの適 用 | Azure Al Services リソースのキー アクセ スが無効になっている必要があります (ローカル認証を無効にする)☑ :unselected: | 1.1.0亿 |\\n\\n:unselected:  :selected: \\n||| [ドメイン] コント コントロール  :unselected: ロール ID :unselected: のタイトル| ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| アクセス 制御 | AC-4 | 情報フローの 適用 :unselected: | Azure Al Services リソースでネットワー クアクセスを制限する必要がある☑ | 3.1.0亿 |\\n| アクセス 制御 | AC-4 | 情報フローの 適用 :unselected: | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0亿 |\\n| アクセス 制御 | AC-4 | 情報フローの 適用 :unselected: | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある已 :selected: | 1.0.0亿 |\\n| アクセス 制御 | AC-4 | 情報フローの :unselected: 適用 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0亿 |\\n| アクセス 制御 | AC-17 | リモート アク セス | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス 制御 | AC-17 | リモート アク セス | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0亿 |\\n| アクセス 制御 | AC-17 (1) | 自動監視/制御 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0亿 |\\n| アクセス 制御 | AC-17 (1) | 自動監視/制御 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0亿 |\\n| 監査とア カウンタ ビリティ | AU-6 (4) | 一元的なレビ ユーと分析 :unselected: | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0 亿 |\\n| 監査とア カウンタ ビリティ | AU-6 (5) | 統合またはス キャンと監視 機能 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0亿 |\\n| 監査とア カウンタ ビリティ | AU-12 | 監査の生成 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0亿 |\\n| 監査とア カウンタ ビリティ | AU-12 (1) | システム全体 または時間相 関の監査証跡 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0亿 |\\n| 識別と認 証 | IA-2 | 識別と認証(組 織のユーザー) | Azure Al Services リソースのキー アクセ スが無効になっている必要があります (ローカル認証を無効にする)☑ :unselected: | 1.1.0亿 |\\n\\n||| [ドメイン] コント コントロール  :unselected: :unselected:  ロール ID :unselected: のタイトル | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| 識別と認 証 | IA-4 | 識別子の管理 | Azure Al Services リソースのキー アクセ スが無効になっている必要があります (ローカル認証を無効にする)☑ :unselected: | 1.1.0亿 |\\n| システム と通信の 保護 | SC-7 | 境界保護 | Azure Al Services リソースでネットワー クアクセスを制限する必要がある☑ | 3.1.0亿 |\\n| システム と通信の 保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0亿 |\\n| システム と通信の 保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0℃ |\\n| システム と通信の 保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0亿 |\\n| システム と通信の 保護 | SC-7 (3) | アクセス ポイ ント | Azure Al Services リソースでネットワー クアクセスを制限する必要がある已 | 3.1.0亿 |\\n| システム と通信の 保護 | SC-7 (3) | アクセス ポイ ント | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0亿 |\\n| システム と通信の 保護 | SC-7 (3) | アクセス ポイ ント | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある已 | 1.0.0 亿 |\\n| システム と通信の 保護 | SC-7 (3) | アクセス ポイ ント | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0亿 |\\n\\n\\n## NIST SP 800-53 Rev. 5\\n\\nすべての Azure サービスに対して使用可能な Azure Policy 組み込みがこのコンプライア ンス標準にどのように対応するのかを確認するには、Azure Policy の規制コンプライア ンス- NIST SP 800-53 Rev. 5 に関するページを参照してください。このコンプライア ンス標準の詳細については、NIST SP 800-53 Rev. 5亿 に関するページを参照してくださ い。\\n\\n〔〕 テーブルを展開する\\n\\n| Domain | コント :unselected: :unselected: ロール ID | コントロール :unselected: :unselected: のタイトル | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| アクセス制御 | AC-2 | アカウント管 理 | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n| アクセス制御 | AC-2 (1) | システム アカ ウント管理の 自動化 | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n| アクセス制御 | AC-2 (7) | 特権ユーザー アカウント | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n| アクセス制御 | AC-3 | アクセスの適 用 | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n|| アクセス制御 AC-4  | 情報フローの :unselected: 適用 | Azure Al Services リソースでネットワ ーク アクセスを制限する必要がある☑ | 3.1.0 亿 |\\n|| アクセス制御 AC-4  :selected: | 情報フローの :unselected: 適用 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n|| アクセス制御 AC-4  :selected: | 情報フローの :unselected: 適用 | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-4 :selected: | 情報フローの :unselected: 適用 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 | リモート アク セス | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 | リモート アク セス | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 (1) | 監視および制 御 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| アクセス制御 | AC-17 (1) | 監視および制 御 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| 監査とアカウ | AU-6 | 一元的なレビ | Search サービスのリソース ログを有効 :unselected: | 5.0.0 亿 |\\n\\n| Domain  :unselected:| コント :unselected: ロール ID  :selected: | コントロール :unselected: のタイトル| ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| ンタビリティ | (4) | ユーと分析 | にする必要がある☑ :unselected: | |\\n| 監査とアカウ ンタビリティ | AU-6 (5) | 監査レコード の統合分析 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0 亿 |\\n| 監査とアカウ ンタビリティ | AU-12 | 監査レコード の生成 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0 亿 |\\n| 監査とアカウ ンタビリティ | AU-12 (1) | システム全体 および時間相 関の監査証跡 | Search サービスのリソース ログを有効 :unselected: にする必要がある☑ | 5.0.0 亿 |\\n| 識別と認証 | IA-2 | 識別と認証 (組織のユーザ 一) | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)☑ :unselected: | 1.1.0 亿 |\\n| 識別と認証 | IA-4 | 識別子の管理 | Azure Al Services リソースのキー アク セスが無効になっている必要がありま す(ローカル認証を無効にする)で :unselected: | 1.1.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Al Services リソースでネットワ ーク アクセスを制限する必要がある☑ | 3.1.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 | 境界保護 | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセス ポイ ント | Azure Al Services リソースでネットワ ーク アクセスを制限する必要がある☑ | 3.1.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセス ポイ ント | Azure Cognitive Search サービスでは、 プライベート リンクをサポートする SKU を使用する必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセス ポイ ント | Azure Cognitive Search サービスではパ ブリック ネットワーク アクセスを無効 にする必要がある☑ | 1.0.0 亿 |\\n| システムと通 信の保護 | SC-7 (3) | アクセス ポイ ント | Azure Cognitive Search サービスはプラ イベート リンクを使用する必要がある☑ | 1.0.0 ℃ |\\n\\n# NL BIO Cloud Theme\\n\\nすべての Azure サービスで使用可能な Azure Policy の組み込みがこのコンプライアンス 標準にどのように対応しているのかを確認するには、「NL BIO Cloud Theme に関する Azure Policy の規制コンプライアンスの詳細」を参照してください。このコンプライア ンス標準の詳細については、「ベースライン情報セキュリティ政府サイバーセキュリテ イーデジタル政府(digitaleoverheid.nl) △」を参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| [ドメイン] || コント コントロールのタイトル :unselected: :unselected: ロール ID  :unselected: :unselected: | ポリシー (Azure portal) | ポリシー のバージ ヨン (GitHub) |\\n| - | - | - | - | - |\\n| U.07.1 データ 分離 - 分離 | U.07.1 | データの永続的な分離は、 マルチテナント アーキテク チャの1つです。パッチは 制御された方法で実現され ます。 | Azure Al Services リソー スでネットワーク アクセ スを制限する必要がある☑ | 3.1.0亿 |\\n| U.07.1 データ の分離 -分離 | U.07.1 | データの永続的な分離は、 マルチテナント アーキテク チャの1つです。パッチは 制御された方法で実現され ます。 | Azure Cognitive Search + ービスでは、プライベー ト リンクをサポートする SKU を使用する必要があ る☑ | 1.0.0亿 |\\n| U.07.1 データ の分離 - 分離 | U.07.1 | データの永続的な分離は、 マルチテナント アーキテク チャの1つです。パッチは 制御された方法で実現され ます。 | Azure Cognitive Search ービスではパブリック ネ ットワーク アクセスを無 効にする必要がある☑ | 1.0.0亿 |\\n| U.07.1 データ の分離 -分離 | U.07.1 | データの永続的な分離は、 マルチテナント アーキテク チャの1つです。パッチは 制御された方法で実現され ます。 | Azure Cognitive Search + ービスはプライベートリ ンクを使用する必要があ るこ | 1.0.0亿 |\\n| U.07.3 データ の分離 - 管理 機能 | U.07.3 | U.07.3 - CSC データおよび/ または暗号化キーを表示あ るいは変更する権限は、制 御された方法で付与され、 使用状況がログに記録され :unselected: ます。 | Azure Al Services リソー スのキー アクセスが無効 になっている必要があり ます(ローカル認証を無効 :unselected: にする)☑ | 1.1.0亿 |\\n|| U.10.2 IT サー U.10.2  ビスとデータ | CSP の責任の下、アクセス 権が管理者に付与されま す。 | Azure Al Services リソー スのキー アクセスが無効 になっている必要があり | 1.1.0亿 |\\n\\n| 【ドメメアイクヤス - ユーザー || コント コントロールのタイトル :unselected: :unselected: ロール \\n| | ID  :unselected: | ポサシューカル認証を無効 (AzdrePoFfal) | ポリシー のバージ || | | ヨン |\\n| - | - | - | - | - |\\n| U.10.3 IT サー ビスとデータ | U.10.3 | IT サービスとデータにアク セスできるのは、認証され | Azure Al Services UV- スのキー アクセスが無効 | 1.1.0亿 (GitHub) |\\n| へのアクセス - ユーザー | | た機器を持つユーザーだけ です。 | になっている必要があり ます(ローカル認証を無効 :unselected: にする)☑ | |\\n| U.10.5 IT サー ビスとデータ へのアクセス - 適格性 | U.10.5 | IT サービスとデータへのア クセスは技術的な手段によ って制限され、実装されて います。 | Azure Al Services UJ y- スのキー アクセスが無効 になっている必要があり ます(ローカル認証を無効 にする)☑ | 1.1.0亿 |\\n| U.15.1 ログ記 :unselected: 録と監視 - イ ベントの記録 | U.15.1 | ポリシー規則の違反は、CSP と CSC によって記録されま す。 | Search サービスのリソー ス ログを有効にする必要 :unselected: がある☑ | 5.0.0亿 |\\n\\n\\n## インド準備銀行の銀行向けの IT フレームワーク v2016\\n\\nすべての Azure サービスで使用可能な Azure Policy 組み込みがこのコンプライアンス標 準にどのように対応するのかを確認するには、Azure Policy 規制コンプライアンス- RBI ITF Banks v2016 に関する記事を参照してください。このコンプライアンス標準の 詳細については、RBI ITF Banks v2016 (PDF)△ を参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| Domain | コント :unselected: :unselected: ロール ID | コントロール :unselected: :unselected: のタイトル | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| フィッシング 詐欺対策 | | フィッシング 詐欺対策-14.1 | Azure Al Services リソースでネット ワーク アクセスを制限する必要が ある心 | 3.1.0亿 |\\n\\n\\n## RMIT マレーシア\\n\\nすべての Azure サービスで使用可能な Azure Policy 組み込みがこのコンプライアンス標 準にどのように対応するのかを確認するには、Azure Policy 規制コンプライアンス-\\n\\nRMIT マレーシアに関する記事をご覧ください。このコンプライアンス標準の詳細につ いては、RMIT マレーシアロ に関するドキュメントをご覧ください。\\n\\n<!-- PageHeader=\"〔〕 テーブルを展開する\" -->\\n\\n| Domain || コントロ コントロールの :unselected: :unselected: ール ID  タイトル | ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| デジタル サービ スのセキュリテ イ | 10.66 | デジタル サービ スのセキュリテ イ ー 10.66 | Search サービスの診断設定を イベント ハブにデプロイする☑ :unselected: | 2.0.0 亿 |\\n| デジタル サービ スのセキュリテ イ | 10.66 | デジタル サービ スのセキュリテ イー 10.66 | Search サービスの診断設定を Log Analytics ワークスペース にデプロイする☑ :unselected: | 1.0.0 亿 |\\n\\n\\n## SWIFT CSP-CSCF v2021\\n\\nすべての Azure サービスで使用できる Azure Policy の組み込みが、このコンプライアン ス標準にどのように対応するかを確認するには、「SWIFT CSP-CSCF v2021 についての Azure Policy の規制コンプライアンスの詳細」を参照してください。このコンプライア ンス標準の詳細については、「SWIFT CSP CSCF v2021☑」を参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| [ドメイン] | コント ロール :unselected: :unselected:  ID| コントロ :unselected: :unselected: :selected:  ールのタ イトル| ポリシー (Azure portal) | ポリシーの バージョン (GitHub) |\\n| - | - | - | - | - |\\n| システムまたはトランザクシ ョン レコードに対する異常な アクティビティの検出 | 6.4 | ログ記録 :unselected: と監視 | Search サービスのリソー ス ログを有効にする必要 がある☑ :unselected: | 5.0.0亿 |\\n\\n\\n## SWIFT CSP-CSCF v2022\\n\\nすべての Azure サービスで使用できる Azure Policy の組み込みが、このコンプライアン ス標準にどのように対応するかを確認するには、「SWIFT CSP-CSCF v2022 についての Azure Policy の規制コンプライアンスの詳細」を参照してください。このコンプライア ンス標準の詳細については、「SWIFT CSP CSCF v2022℃」を参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| ドメイン  :unselected: | コン トロ :unselected:| コントロールのタイトル :unselected: :unselected: | ポリシー (Azure portal) | ポリシー のバージ |\\n|| ール ||| ョン |\\n|| ID || | (GitHub) |\\n| 6.システムまたはト ランザクション レコ ードに対する異常な アクティビティの検 出 :unselected: | 6.4 | セキュリティ イベントを記 録し、ローカルの SWIFT 環 :unselected: 境内での異常なアクションと 操作を検出する。 | Search サービスの リソース ログを有 :unselected: 効にする必要があ る已 | 5.0.0 亿 |\\n\\n\\n## 次のステップ\\n\\n● Azure Policy の規制コンプライアンスの詳細を確認します。\\n\\n● Azure Policy GitHub リポジトリ☑のビルトインを参照します。\\n\\n# Azure Cognitive Search の Azure セキュ リティ ベースライン\\n\\n[アーティクル]·2023/09/22\\n\\nこのセキュリティ ベースラインは、Microsoft クラウド セキュリティ ベンチマーク バ ージョン 1.0 のガイダンスをAzure Cognitive Searchに適用します。 Microsoft クラウド セキュリティ ベンチマークでは、Azure 上のクラウド ソリューションをセキュリティ で保護する方法に関する推奨事項が提供されます。コンテンツは、Microsoft クラウド セキュリティ ベンチマークと、Azure Cognitive Searchに適用できる関連ガイダンスに よって定義されたセキュリティ制御によってグループ化されます。\\n\\nこのセキュリティ ベースラインとその推奨事項は、Microsoft Defender for Cloud を使 用して監視できます。Azure Policy定義は、[クラウド ポータルのMicrosoft Defender] ページの[規制コンプライアンス] セクションに一覧表示されます。\\n\\n機能に関連するAzure Policy定義がある場合は、Microsoft クラウド セキュリティ ベン チマークの制御と推奨事項への準拠を測定するのに役立つ、このベースラインに一覧表 示されます。一部の推奨事項では、特定のセキュリティ シナリオを有効にするために 有料Microsoft Defenderプランが必要になる場合があります。\\n\\n\\n## 4 注意\\n\\nAzure Cognitive Searchに適用されない機能は除外されています。 microsoft クラ ウド セキュリティ ベンチマークに完全にマップ Azure Cognitive Search方法につい ては、完全なAzure Cognitive Searchセキュリティ ベースライン マッピング ファ イルロを参照してください。\\n\\n\\n### セキュリティ プロファイル\\n\\nセキュリティ プロファイルは、Azure Cognitive Searchの影響の大きい動作をまとめた ものです。これにより、セキュリティに関する考慮事項が高まる可能性があります。\\n\\n〔〕 テーブルを展開する\\n\\n| サービス動作属性 | 値 |\\n| - | - |\\n| 製品カテゴリ | Al+ML、モバイル、Web |\\n| お客様は HOST/OS にアクセスできます | アクセス権なし |\\n:unselected:\\n| サービス動作属性 | 値 |\\n| - | - |\\n| サービスは顧客の仮想ネットワークにデプロイできます :unselected: | False |\\n| 顧客のコンテンツを保存する | :unselected: 0 |\\n\\n# ネットワークのセキュリティ\\n\\n詳細については、「Microsoft クラウド セキュリティ ベンチマーク:ネットワーク セキ ュリティ」を参照してください。\\n\\n\\n## NS-1: ネットワーク セグメント化の境界を確立する\\n\\n機能\\n\\nVirtual Network 統合\\n\\n説明: サービスは、顧客のプライベート Virtual Network (VNet) へのデプロイをサポー :unselected: トします。詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\nネットワーク セキュリティ グループのサポート\\n\\n説明: サービス ネットワーク トラフィックは、サブネット上のネットワーク セキュリ ティ グループルールの割り当てを尊重します。詳細については、こちらを参照してく ださい。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\n# NS-2: ネットワーク制御を使用してクラウド サービスをセ キュリティで保護する\\n\\n機能\\n\\nAzure Private Link\\n\\n説明: ネットワーク トラフィックをフィルター処理するためのサービス ネイティブ IP フィルタリング機能(NSG やAzure Firewallと混同しないように)。詳細については、こ ちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n機能に関する注意事項: プライベート エンドポイント経由の送信接続については、「プ ライベート エンドポイント経由で送信接続を行う」を参照してください。\\n\\n構成ガイダンス: プライベート エンドポイントをデプロイして、リソースのプライベー ト アクセス ポイントを確立します。検索サービス向けのパブリック エンドポイント 上のすべての接続をブロックします。仮想ネットワークからのデータの流出をブロッ :unselected: クし、仮想ネットワークのセキュリティを強化します。\\n\\nリファレンス: Azure Cognitive Searchへのセキュリティで保護された接続用のプライベ ート エンドポイントを作成する\\n\\n\\n## パブリック ネットワーク アクセスの無効化\\n\\n説明: サービスでは、サービス レベルの IP ACL フィルター規則(NSG またはAzure Firewallではなく)または[パブリック ネットワーク アクセスの無効化]トグル スイッチ を使用して、パブリック ネットワーク アクセスを無効にできます。詳細については、 こちらを参照してください。\\n\\n<!-- PageFooter=\"〔〕 テーブルを展開する\" -->\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス: Azure Cognitive Searchでは、Azure 仮想ネットワーク セキュリティ グループで見つかる IP 規則と同様に、ファイアウォール経由の受信アクセスの IP 規則 がサポートされています。IP 規則を利用することで、検索サービスのアクセスを、承 認された一連のマシンとクラウド サービスに制限できます。これらの承認された一連 のマシンやサービスから検索サービスに格納されているデータにアクセスするには、引 き続き呼び出し側で有効な認可トークンを提示する必要があります。\\n\\nリファレンス: Azure Cognitive Search用に IP ファイアウォールを構成する\\n\\n\\n## ID 管理\\n\\n詳細については、「Microsoft クラウド セキュリティ ベンチマーク: ID 管理」を参照し てください。\\n\\n\\n## IM-1: 一元的な ID および認証システムを使用する\\n\\n機能\\n\\nデータ プレーン アクセスに必要な Azure AD Authentication\\n\\n説明: サービスでは、データ プレーン アクセスに Azure AD 認証を使用できます。詳 細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | True | Microsoft |\\n\\n構成ガイダンス: 既定のデプロイでこれが有効になっているので、追加の構成は必要あ :unselected: りません。\\n\\nデータ プレーン アクセスのローカル認証方法\\n\\n説明: ローカルユーザー名やパスワードなど、データ プレーンアクセスでサポートされ :unselected: るローカル認証方法。 詳細については、こちらを参照してください。\\n:unselected: :unselected:\\n<!-- PageHeader=\"〔〕 テーブルを展開する\" -->\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n機能に関するメモ: ローカル認証方法またはアカウントの使用は避けてください。これ :unselected: らは可能な限り無効にする必要があります。代わりに、可能な場合は Azure AD を使用 して認証します。\\n\\n構成ガイダンス: Cognitive Search では、主要な認証方法としてキーベースの認証が使 用されます。インデックスの作成やクエリを実行する要求など、検索サービス エンド ポイントへの受信要求の場合、一般に使用可能な認証オプションは API キーだけで す。\\n\\nリファレンス: Azure Cognitive Search認証に API キーを使用する\\n\\n\\n### IM-3: アプリケーション ID を安全かつ自動的に管理する 機能\\n\\nマネージド ID\\n\\n説明: データ プレーン アクションでは、マネージド ID を使用した認証がサポートされ ます。詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス:可能な場合は、サービス プリンシパルの代わりに Azure マネージド ID を使用します。これにより、Azure Active Directory (Azure AD) 認証をサポートする Azure サービスとリソースに対して認証できます。マネージド ID の資格情報は、プラ ットフォームによって完全に管理、ローテーション、保護されており、ソース コード :unselected: または構成ファイル内でハードコーディングされた資格情報を使用せずに済みます。\\n\\nリファレンス: Azure Active Directory を使用して検索アプリへのアクセスを承認する\\n\\nサービス プリンシパル\\n\\n説明: データ プレーンでは、サービス プリンシパルを使用した認証がサポートされて います。詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス: この機能の構成に関する現在の Microsoft ガイダンスはありません。 organizationがこのセキュリティ機能を構成するかどうかを確認して確認してくださ い。\\n\\n\\n## IM-7: 条件に基づいてリソースへのアクセスを制限する\\n\\n機能\\n\\n\\n### データ プレーンへの条件付きアクセス\\n\\n説明: データ プレーンアクセスは、Azure AD 条件付きアクセス ポリシーを使用して制 御できます。詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス: ワークロード内の Azure Active Directory (Azure AD) 条件付きアクセス :unselected: に適用できる条件と条件を定義します。特定の場所からのアクセスのブロックや許 :unselected: 可、危険なサインイン動作のブロック、特定のアプリケーションに対するorganization :unselected: マネージド デバイスの要求など、一般的なユース ケースを検討してください。\\n\\n\\n### IM-8: 資格情報とシークレットの公開を制限する\\n\\n機能\\n\\nAzure Key Vault での、サービス資格情報とシークレットの統合とスト レージのサポート\\n\\n説明: データ プレーンでは、資格情報とシークレット ストアに対する Azure Key Vault のネイティブな使用がサポートされています。詳細については、こちらを参照してく ださい。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\n\\n## 特権アクセス\\n\\n詳細については、「Microsoft クラウド セキュリティ ベンチマーク:特権アクセス」を 参照してください。\\n\\n\\n## PA-1: 高い特権を持つ/管理者ユーザーを分離して制限する\\n\\n機能\\n\\nローカル 管理 アカウント\\n :unselected:\\n説明: サービスには、ローカル管理アカウントの概念があります。詳細については、こ :unselected: ちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\nPA-7: Just Enough Administration (最小限の特権の原則) に従う\\n\\n機能\\n\\n# Azure RBAC for Data Plane\\n\\n説明: Azure Role-Based Access Control (Azure RBAC) を使用して、サービスのデータ プ レーンアクションへのアクセスを管理できます。詳細については、こちらを参照して ください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス: Azure は、プラットフォーム上で実行されているすべてのサービスに 対してグローバル ロールベースのアクセス制御(RBAC) 承認システムを提供します。 :unselected: :unselected: Cognitive Search では、次の目的で Azure ロールを使用できます。\\n :unselected:\\n· コントロール プレーン操作(Azure Resource Managerを介したサービス管理タス\\n :unselected:\\nク)。\\n\\n● インデックスの作成、読み込み、クエリなどのデータ プレーン操作。\\n\\nリファレンス: Azure Cognitive Searchで Azure [\\n :unselected:\\nコールベースのアクセス制御(Azure RBAC) を使用する\\n\\nPA-8: クラウド プロバイダー サポートのアクセス プロセ スを決定する\\n\\n機能\\n\\n\\n## カスタマー ロックボックス\\n :unselected:\\n説明: カスタマー ロックボックスは、Microsoft サポート へのアクセスに使用できま :unselected: す。詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス: Microsoft がデータにアクセスする必要があるサポート シナリオで は、カスタマー ロックボックスを使用して確認し、Microsoft の各データ アクセス要 :unselected: 求を承認または拒否します。\\n:unselected:\\n## データの保護\\n\\n詳細については、「 Microsoft クラウド セキュリティ ベンチマーク: データ保護」を参 照してください。\\n\\n\\n## DP-1:機密データを検出、分類、ラベル付けする\\n\\n機能\\n\\n\\n## 機密データの検出と分類\\n\\n説明: ツール(Azure Purview や Azure Information Protection など) は、サービスでのデ ータの検出と分類に使用できます。詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\n\\n### DP-2: 機密データをターゲットにした異常と脅威を監視す る\\n\\n\\n## 機能\\n\\n\\n## データ漏えい/損失防止\\n\\n説明: サービスでは、機密データの移動(顧客のコンテンツ内) を監視するための DLP ソ リューションがサポートされています。詳細については、こちらを参照してくださ い。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\n\\n### DP-3: 転送中の機密データの暗号化\\n\\n機能\\n\\n\\n## 転送中データの暗号化\\n\\n説明: サービスでは、データ プレーンの転送中のデータ暗号化がサポートされていま す。 詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | True | Microsoft |\\n\\n構成ガイダンス: 既定のデプロイでこれが有効になっているので、追加の構成は必要あ :unselected: りません。\\n\\nリファレンス:転送中の暗号化中のデータをAzure Cognitive Searchする\\n\\nDP-4: 保存データ暗号化を既定で有効にする\\n\\n機能\\n\\n\\n### プラットフォーム キーを使用した保存データの暗号化\\n\\n説明: プラットフォーム キーを使用した保存データの暗号化がサポートされています。 保存中の顧客コンテンツは、これらの Microsoft マネージド キーで暗号化されます。 詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | True | Microsoft |\\n\\n構成ガイダンス: 既定のデプロイでこれが有効になっているので、追加の構成は必要あ :unselected: りません。\\n\\nリファレンス: サービスマネージド キーを使用した既定のデータ暗号化のAzure Cognitive Search\\n\\nDP-5: 必要に応じて保存データ暗号化でカスタマー マネー ジド キー オプションを使用する\\n\\n機能\\n\\n\\n### CMK を使用した保存データの暗号化\\n\\n説明: カスタマー マネージド キーを使用した保存データの暗号化は、サービスによっ て格納される顧客コンテンツでサポートされています。詳細については、こちらを参 照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス: 規制コンプライアンスに必要な場合は、カスタマー マネージド キー を使用した暗号化が必要なユース ケースとサービス スコープを定義します。それらの サービスでカスタマー マネージド キーを使って、保存データ暗号化を有効にして実装 します。\\n\\nリファレンス: Azure Cognitive Searchでデータ暗号化用にカスタマー マネージド キー を構成する\\n\\n\\n## DP-6: セキュア キー管理プロセスの使用\\n\\n機能\\n\\nAzure Key Vault でのキー管理\\n\\n説明: このサービスでは、カスタマー キー、シークレット、または証明書に対する Azure Key Vault統合がサポートされています。詳細については、こちらを参照してく ださい。\\n\\n<!-- PageFooter=\"〔〕 テーブルを展開する\" -->\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス: Azure Key Vaultを使用して、キーの生成、配布、ストレージなど、暗 号化キーのライフ サイクルを作成および制御します。定義されたスケジュールに基づ いて、またはキーの廃止や侵害が発生した場合に、Azure Key Vault とサービスのキー をローテーションして取り消します。ワークロード、サービス、またはアプリケーシ :unselected: ョン レベルでカスタマー マネージド キー(CMK) を使用する必要がある場合は、キー 管理のベスト プラクティスに従ってください。キー階層を使用して、キー コンテナー にキー暗号化キー (KEK) を使用して別のデータ暗号化キー(DEK) を生成します。キー が Azure Key Vaultに登録され、サービスまたはアプリケーションのキー ID を介して参 照されていることを確認します。独自のキー (BYOK) をサービスに持ち込む必要がある 場合(オンプレミスの HSM から Azure Key Vaultに HSM で保護されたキーをインポー トする場合など)、初期キーの生成とキー転送を実行するための推奨ガイドラインに従 ってください。\\n\\nリファレンス: Azure Cognitive Searchでデータ暗号化用にカスタマー マネージド キー を構成する\\n\\n\\n## DP-7: セキュリティで保護された証明書管理プロセスを使 :unselected: 用する\\n\\n機能\\n\\n\\n### Azure Key Vault での証明書管理\\n\\n説明: このサービスでは、顧客証明書に対する Azure Key Vault統合がサポートされま す。詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\n\\n## アセット管理\\n\\n詳細については、「Microsoft クラウド セキュリティ ベンチマーク:資産管理」を参照 してください。\\n\\n\\n### AM-2: 承認済みのサービスのみを使用する\\n\\n機能\\n\\nAzure Policy のサポート\\n\\n説明: サービス構成は、Azure Policy経由で監視および適用できます。詳細について は、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス: Microsoft Defender for Cloud を使用して、Azure リソースの構成を監 査および適用するAzure Policyを構成します。Azure Monitor を使用し、リソースで構 成の逸脱が検出されたときにアラートを作成します。[deny] と [deploy if not exists] 効 果Azure Policy使用して、Azure リソース全体でセキュリティで保護された構成を適用 します。\\n\\nリファレンス: Azure Cognitive Search ポリシー\\n\\n\\n## ログと脅威検出\\n\\n詳細については、「Microsoft クラウド セキュリティ ベンチマーク: ログ記録と脅威検 :unselected: 出」を参照してください。\\n\\n\\n## LT-1: 脅威検出機能を有効にする\\n\\n機能\\n\\n\\n## サービス/製品のオファリングのための Microsoft Defender\\n\\n説明: サービスには、セキュリティの問題を監視およびアラートするためのオファリン グ固有のMicrosoft Defender ソリューションがあります。詳細については、こちらを 参照してください。\\n:unselected:\\n<!-- PageHeader=\"〔〕 テーブルを展開する\" -->\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\n\\n## LT-4: セキュリティ調査のためのログを有効にする\\n\\n特徴\\n\\n\\n### Azure リソース ログ\\n :unselected:\\n説明: サービスは、サービス固有のメトリックとログ記録を強化できるリソース ログを :unselected: 生成します。お客様はこれらのリソース ログを構成し、ストレージ アカウントやログ :unselected: :unselected: 分析ワークスペースなどの独自のデータ シンクに送信できます。詳細については、こ ちらを参照してください。\\n\\nこ〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| True | False | Customer |\\n\\n構成ガイダンス: サービスのリソース ログを有効にして、Azure Cognitive Search操作 :unselected: :unselected: グ、検索メトリックなどを表示します。\\n\\nリファレンス: リソース ログAzure Cognitive Search\\n :unselected:\\n\\n## バックアップと回復\\n\\n詳細については、「Microsoft クラウド セキュリティ ベンチマーク:バックアップと回 復」を参照してください。\\n\\nBR-1:定期的な自動バックアップを保証する\\n\\n機能\\n:unselected:\\n## Azure Backup\\n\\n説明: サービスは、Azure Backup サービスによってバックアップできます。詳細につ いては、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\n\\n### サービス ネイティブ バックアップ機能\\n\\n説明: サービスでは、独自のネイティブ バックアップ機能がサポートされます(Azure Backupを使用していない場合)。 詳細については、こちらを参照してください。\\n\\n〔〕 テーブルを展開する\\n\\n| サポートされています | 既定で有効 | 構成の責任 |\\n| - | - | - |\\n| False | 適用しない | 適用しない |\\n\\n機能に関するメモ: Azure Cognitive Searchはプライマリ データ ストレージ ソリューシ ヨンではないので、Microsoft はセルフサービスのバックアップと復元のための正式な メカニズムを提供していません。ただし、独自のコードを使用してインデックスをバ ックアップおよび復元できます。「バックアップと復元の代替手段」を参照してくだ さい。\\n\\n構成ガイダンス: この機能は、このサービスをセキュリティで保護するためにサポート されていません。\\n\\n\\n## 次のステップ\\n\\n● Microsoft クラウド セキュリティ ベンチマークの概要を参照してください\\n\\n● Azure セキュリティ ベースラインの詳細について学習する\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_layout(\"../data/01_aisearch_docs/azure-search-concept.pdf\", \"../output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【TBD】With Image\n",
    "https://github.com/Azure-Samples/document-intelligence-code-samples/blob/main/Python(v4.0)/Retrieval_Augmented_Generation_(RAG)_samples/sample_figure_understanding.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Option] Text standardization and normalization\n",
    "- Utilizing LLMs for text standardization and normalization is a highly effective approach.\n",
    "- It can extend the capabilities of traditional rule-based text transformation.\n",
    "- However, since LLMs do not guarantee the same output every time, traditional rule-based transformations should be used when output consistency is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LLM such as GPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "system_message = \"\"\"\n",
    "# Your Role\n",
    "You are an excellent AI assistant for proofreading text data. Your task is to ensure the provided text data is of high quality. You are only allowed to proofread. Adding or removing context from the original document is not allowed. Additionally, you cannot change the structure of the document.\n",
    "\n",
    "# Examples of Corrections\n",
    "- Grammar errors and typos\n",
    "- OCR misrecognitions\n",
    "- Inconsistencies in terminology and expressions\n",
    "\n",
    "# Your input\n",
    "text: \n",
    "\"\"\"\n",
    "\n",
    "def correct_text_gpt(text):\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": system_message},\n",
    "\t\t{\"role\":\"user\",\"content\": text}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using traditional rule-based text transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is example function to clean text data\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove duplicate lines\n",
    "    lines = text.split(\"\\n\")\n",
    "    unique_lines = list(dict.fromkeys(lines))\n",
    "    return \"\\n\".join(unique_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
