{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 PUSH Type Import Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case\n",
    "- Azure AI Search の サービス仕様ドキュメントをインプットにする。\n",
    "  - https://learn.microsoft.com/ja-jp/azure/search/\n",
    "- ドキュメントは OCR が必要。\n",
    "- 開発者マニュアルは、構造化されたセクションとなっている。\n",
    "- 各セクションは非常に詳細かつ専門性の高い技術解説が記載されており、ドキュメントサイズも大きい。\n",
    "- ドキュメントには、テキスト、テーブル、図、グラフなどが含まれるが、ここでは、テキスト、テーブルデータのみを扱う。\n",
    "\n",
    "## チャンキング設計\n",
    "- Document Intelligence で、Markdown形式でテキストデータを抽出済み。\n",
    "- 1つのドキュメントに大量のコンテキストが含まれており、ドキュメントサイズも大きいため、チャンキングを実施する。\n",
    "- ドキュメントは、技術要素ごとに明確なセクションわけがされており、各セクションで見るとLLMが扱えないレベルのデータサイズではない。そのため、セクション単位でチャンキングする。\n",
    "- 各チャンキングのContentはEmbeddingする。\n",
    "- Overlapping は行わない。\n",
    "- 広い意味のコンテキストを保持するために、上位2つのヘッダー（Markdown形式：#, ##）をメタデータとして保持する。例えば、検索インデックスに関する記載があった場合に、それが「キーワード検索」に属する情報なのか、「ベクトル検索」に属する情報なのかを判断するために保持する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-dotenv langchain langchain-community langchain-openai langchainhub openai tiktoken azure-ai-documentintelligence azure-identity azure-search-documents==11.6.0b3 azure-ai-textanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AIServicesVisionParameters,\n",
    "    AIServicesVisionVectorizer,\n",
    "    AIStudioModelCatalogName,\n",
    "    AzureMachineLearningVectorizer,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIModelName,\n",
    "    AzureOpenAIParameters,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    BlobIndexerDataToExtract,\n",
    "    BlobIndexerParsingMode,\n",
    "    CognitiveServicesAccountKey,\n",
    "    DefaultCognitiveServicesAccount,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    FieldMapping,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    IndexerExecutionStatus,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    InputFieldMappingEntry,\n",
    "    KeyPhraseExtractionSkill,\n",
    "    OutputFieldMappingEntry,\n",
    "    ScalarQuantizationCompressionConfiguration,\n",
    "    ScalarQuantizationParameters,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataIdentity,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerIndexProjections,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    SearchIndexerSkillset,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SimpleField,\n",
    "    SplitSkill,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    "    VisionVectorizeSkill\n",
    ")\n",
    "from azure.search.documents.models import (\n",
    "    HybridCountAndFacetMode,\n",
    "    HybridSearch,\n",
    "    SearchScoreThreshold,\n",
    "    VectorizableTextQuery,\n",
    "    VectorizableImageBinaryQuery,\n",
    "    VectorizableImageUrlQuery,\n",
    "    VectorSimilarityThreshold,\n",
    ")\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display, HTML\n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "AZURE_AI_VISION_API_KEY = os.getenv(\"AZURE_AI_VISION_API_KEY\")\n",
    "AZURE_AI_VISION_ENDPOINT = os.getenv(\"AZURE_AI_VISION_ENDPOINT\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "BLOB_CONNECTION_STRING = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "INDEX_NAME = \"rag-search-index-push\"\n",
    "AZURE_SEARCH_ADMIN_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_AI_MULTI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_MULTI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_MULTI_SERVICE_KEY = os.getenv(\"AZURE_AI_MULTI_SERVICE_KEY\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "AZURE_DOCUMENT_INTELLIGENCE_KEY = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting with Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunking.split_documents import split_markdown_headings\n",
    "\n",
    "output_dir_documents = \"../output/01_output/documents\"\n",
    "md_files = [f for f in os.listdir(output_dir_documents) if f.endswith('.md')]\n",
    "\n",
    "splits_data = {}\n",
    "\n",
    "for md_file in md_files:\n",
    "    print(\"splits for {}\".format(md_file))\n",
    "    markdown_content = read_file(os.path.join(output_dir_documents, md_file))\n",
    "    splits = split_markdown_headings(markdown_content)\n",
    "    splits_data[md_file] = splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_Index-Design\n",
    "Tips for Azure AI Search Index-Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インデックスの定義\n",
    "- インデックスは単一で構成する。\n",
    "- インプットドキュメントが説明的な内容のため、`Hybrid + Semantic Ranker` を採用する。そのため、ベクトル検索、 Semantic Ranker ための設定をする。\n",
    "\n",
    "### フィールドの定義\n",
    "- シンプルなパターンとして、ローカルのデータを検索インデックスにインポートさせる。\n",
    "- ユーザのコンテキストを捉えるために、ベクトル検索を採用する。そのため、Embedding フィールドを構成する。\n",
    "  - Embedding Model: `text-embedding-ada-002`を採用する。\n",
    "- ドキュメントのヘッダー情報を`Metadata`フィールドに含める。\n",
    "- ドキュメントのタイトルを`Title`フィールドに含める。\n",
    "- ソースドキュメントの追跡用に `last_modified` を含める。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Enabling Semantic Ranker\n",
    "Uses Microsoft’s language understanding models to rerank search results, enhancing relevance and providing results more aligned with the user’s context.\n",
    "\n",
    "#### Implementation Considerations\n",
    "Semantic Ranker を有効にする際の考慮事項を記載します。\n",
    "- Semantic Ranker は、テキストクエリの BM25 でランク付けされた検索結果から、またはハイブリッド クエリの RRF でランク付けされた結果をリランキングします。\n",
    "- 検索結果の数が 50 個を超える場合でも、リランキングが行われるのは上位 50 個の結果のみです。そのため、処理されない結果があることに注意してください。\n",
    "- また、Semantic Ranker は、文章のコンテキストを理解させるために利用するため、適用対象のフィールドは説明的なものを指定することが推奨されます。ナレッジベース、オンラインドキュメントなど説明的なコンテンツを含むドキュメントでは、Semantic Ranker から最も多くのメリットが得られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified parameter\n",
    "USE_AAD_FOR_SEARCH = False  # Set this to False to use API key for authentication\n",
    "\n",
    "def authenticate_azure_search(api_key=None, use_aad_for_search=False):\n",
    "    if use_aad_for_search:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "    return credential\n",
    "\n",
    "azure_search_credential = authenticate_azure_search(api_key=AZURE_SEARCH_ADMIN_KEY, use_aad_for_search=USE_AAD_FOR_SEARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Configuration for Semantic Ranker\n",
    "def create_semantic_config():\n",
    "\tsemantic_config = SemanticConfiguration(\n",
    "\t\tname=\"my-semantic-config\",\n",
    "\t\tprioritized_fields=SemanticPrioritizedFields(\n",
    "\t\t\ttitle_field=SemanticField(field_name=\"title\"),\n",
    "\t\t\tcontent_fields=[SemanticField(field_name=\"content\")],\n",
    "\t\t\tkeywords_fields=[SemanticField(field_name=\"key_phrases\")],\n",
    "\t\t)\n",
    "\t)\n",
    "\t# Create the semantic settings with the configuration\n",
    "\tsemantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\treturn semantic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index(index_name, azure_openai_endpoint, azure_openai_embedding_deployment_id, azure_openai_key=None):\n",
    "    return SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=[\n",
    "            SearchField(\n",
    "                name=\"id\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                key=True,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=True,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                analyzer_name=\"keyword\"\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"content\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=False,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                analyzer_name=\"ja.microsoft\" # replace with your analyzer\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"title\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=False,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                analyzer_name=\"ja.microsoft\" # replace with your analyzer\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"key_phrases\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=False,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                analyzer_name=\"ja.microsoft\" # replace with your analyzer\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"vector\",\n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                hidden=False,\n",
    "                filterable=False,\n",
    "                sortable=False,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                vector_search_dimensions=1536,\n",
    "                vector_search_profile_name=\"profile\"\n",
    "            ),\n",
    "            SimpleField(\n",
    "\t\t\t\tname=\"last_modified\",\n",
    "                type=SearchFieldDataType.DateTimeOffset,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=True,\n",
    "                facetable=False,\n",
    "                searchable=False,\n",
    "\t\t\t),\n",
    "        ],\n",
    "        vector_search=VectorSearch(\n",
    "\t\t\talgorithms=[\n",
    "\t\t\t\tHnswAlgorithmConfiguration(\n",
    "\t\t\t\t\tname=\"myHnsw\",\n",
    "\t\t\t\t\tparameters=HnswParameters(\n",
    "\t\t\t\t\t\tm=4,\n",
    "\t\t\t\t\t\tef_construction=400,\n",
    "\t\t\t\t\t\tef_search=500,\n",
    "\t\t\t\t\t\tmetric=VectorSearchAlgorithmMetric.COSINE,\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t)\n",
    "\t\t\t],\n",
    "\t\t\tvectorizers=[\n",
    "\t\t\t\tAzureOpenAIVectorizer(\n",
    "\t\t\t\t\tname=\"myAzureOpenAIVectorizer\",\n",
    "\t\t\t\t\tkind=\"azureOpenAI\",\n",
    "\t\t\t\t\tazure_open_ai_parameters=AzureOpenAIParameters(\n",
    "\t\t\t\t\t\tresource_uri=azure_openai_endpoint,\n",
    "\t\t\t\t\t\tapi_key=azure_openai_key,\n",
    "\t\t\t\t\t\tdeployment_id=azure_openai_embedding_deployment_id,\n",
    "\t\t\t\t\t\tmodel_name=AzureOpenAIModelName.TEXT_EMBEDDING_ADA002,\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t)\n",
    "\t\t\t],\n",
    "\t\t\tprofiles=[\n",
    "\t\t\t\tVectorSearchProfile(\n",
    "\t\t\t\t\tname=\"profile\",\n",
    "\t\t\t\t\talgorithm_configuration_name=\"myHnsw\",\n",
    "\t\t\t\t\tvectorizer=\"myAzureOpenAIVectorizer\",\n",
    "\t\t\t\t)\n",
    "\t\t\t],\n",
    "    \t),\n",
    "        semantic_search=create_semantic_config() # Here we add the semantic search configuration\n",
    "\t)\n",
    "\n",
    "index = create_search_index(\n",
    "    INDEX_NAME,\n",
    "    AZURE_OPENAI_ENDPOINT,\n",
    "    \"text-embedding-ada-002\", # replace with your deployment name\n",
    "    AZURE_OPENAI_API_KEY\n",
    ")\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=AZURE_SEARCH_ENDPOINT, credential=azure_search_credential\n",
    ")\n",
    "index_client.create_or_update_index(index)\n",
    "\n",
    "print(f\"Created index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Key Phrase: Using Azure AI Language\n",
    "- We need to consider service limits for Azure AI Language: https://learn.microsoft.com/en-us/azure/ai-services/language-service/concepts/data-limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate the client using your key and endpoint \n",
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(AZURE_AI_MULTI_SERVICE_KEY)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=AZURE_AI_MULTI_SERVICE_ENDPOINT, \n",
    "            credential=ta_credential,\n",
    "            default_language=\"ja\" # replace with your language\n",
    "            )\n",
    "    return text_analytics_client\n",
    "\n",
    "def key_phrase_extraction_ai_language(client, documents):\n",
    "\n",
    "    try:\n",
    "\n",
    "        response = client.extract_key_phrases(documents = documents)[0]\n",
    "        return response\n",
    "\n",
    "    except Exception as err:\n",
    "        print(\"Encountered exception. {}\".format(err))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Key Phrage: Using Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_phrase_extraction_gpt(content):\n",
    "    client = AzureOpenAI(\n",
    "\t\tazure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "\t\tapi_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "\t\tapi_version=\"2024-02-01\"\n",
    "\t)\n",
    "    system_message = f\"\"\"\n",
    "    system:\n",
    "\tExtract the key phrases from the following text and return them as an array of strings: [ \"Key phrase 1\", \"Key phrase 2\", \"Key phrase 3\", ... ]. Ensure that each key phrase captures a significant concept or idea from the text.\n",
    "\tYou can contain the maximum of 20 key phrases.\n",
    "\t\n",
    "\t# Few-shot Exapmle:\n",
    "\tExample 1:\n",
    "\tText: \"Artificial Intelligence is transforming industries by automating processes and enhancing decision-making through data analysis.\"\n",
    "\tKey Phrases: [ \"Artificial Intelligence\", \"transforming industries\", \"automating processes\", \"enhancing decision-making\", \"data analysis\" ]\n",
    "\n",
    "\tExample 2:\n",
    "\tText: \"Climate change is a pressing global issue that requires immediate action to reduce carbon emissions and protect ecosystems.\"\n",
    "\tKey Phrases: [ \"Climate change\", \"pressing global issue\", \"immediate action\", \"carbon emissions\", \"ecosystems\" ]\n",
    " \n",
    "\tUser: \n",
    "\t\"\"\"\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": system_message},\n",
    "\t\t{\"role\":\"user\",\"content\": content}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\t# response_format={\"type\": \"json_object\"},\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Document Embeddings using OpenAI Ada 002\n",
    "def generate_document_embeddings(content):\n",
    "\tclient = AzureOpenAI(\n",
    "\t\tazure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "\t\tapi_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "\t\tapi_version=\"2024-02-01\"\n",
    "\t)\n",
    "\ttitle_response = client.embeddings.create(input=content, model=\"text-embedding-ada-002\")\n",
    "\treturn title_response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get update date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def get_file_modification_time(file_path):\n",
    "    # Get the modification time\n",
    "    mod_time = os.path.getmtime(file_path)\n",
    "    # Convert to a human-readable format\n",
    "    mod_time_utc = datetime.fromtimestamp(mod_time, tz=timezone.utc)\n",
    "    return mod_time_utc.isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert text and embeddings into Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "documnents_for_upload = []\n",
    "\n",
    "for md_file, splits in splits_data.items():\n",
    "\tprint(\"Processing {}\".format(md_file))\n",
    "\tfor split in splits:\n",
    "\t\t# print(\"Processing split {}\".format(md_file))\n",
    "\t\ttitle = md_file\n",
    "\t\tcontent = split.page_content\n",
    "\t\tkey_phrases = key_phrase_extraction_gpt(content)\n",
    "\t\tvector = generate_document_embeddings(content)\n",
    "\t\tlast_modified = get_file_modification_time(os.path.join(output_dir_documents, md_file))\n",
    "\t\tdocumnents_for_upload.append(\n",
    "\t\t\t{\n",
    "\t\t\t\t\"id\": str(uuid.uuid4()),\n",
    "\t\t\t\t\"content\": content,\n",
    "\t\t\t\t\"title\": title,\n",
    "\t\t\t\t\"key_phrases\": key_phrases,\n",
    "\t\t\t\t\"vector\": vector,\n",
    "\t\t\t\t\"last_modified\": last_modified\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "  \n",
    "documnents_for_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=INDEX_NAME, credential=azure_search_credential)\n",
    "search_client.merge_or_upload_documents(documnents_for_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_Query-Design\n",
    "Tips for Azure AI Search Query-Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Search\n",
    "- ユーザのクエリのコンテキストをとらえたい、かつサービスに特化したワードがクエリに含まれる可能性が高いため、Hybrid（フルテキスト検索＋ベクトル検索）＋Semantic Ranker を採用する。\n",
    "  - Hybrid検索のスコアはAzure AI Searchでは、Reciprocal Rank Fusion (RRF) が採用される。 \n",
    "- クエリはユーザのクエリをそのまま検索インデックスのクエリに利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=INDEX_NAME, credential=azure_search_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ベクトル検索時の設定要素について教えてください\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=50,\n",
    "    fields=\"vector\",\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    query_type='semantic',\n",
    "    query_language='ja',\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    top=5,\n",
    "    select=\"content, title, key_phrases\",\n",
    "\tsearch_fields=[\"content\", \"title\", \"key_phrases\"],\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Expansion\n",
    "- ユーザのクエリのコンテキストをとらえたい、かつサービスに特化したワードがクエリに含まれる可能性が高いため、Hybrid（フルテキスト検索＋ベクトル検索）＋Semantic Ranker を採用する。\n",
    "  - Hybrid検索のスコアはAzure AI Searchでは、Reciprocal Rank Fusion (RRF) が採用される。 \n",
    "- また、ユーザクエリから検索クエリを新しく生成する。\n",
    "  - クエリはユーザのクエリをスタンドアローンなクエリに変換する。\n",
    "  - また、検索のカバレッジを大きくするために、類似した入力クエリを複数生成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "system_message = \"\"\"\n",
    "# Your Task\n",
    "- Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\n",
    "- You also need to extend the original question to generate 5 related queries. This is done to capture the broader context of the user's question.\n",
    "- You must output json format. In other words, You must output array of questions that length is 5.\n",
    "\n",
    "# Json format example:\n",
    "{\n",
    "\t\"questions\": [\n",
    "\t\t\"related question 1\",\n",
    "\t\t\"related question 2\",\n",
    "\t\t\"related question 3\",\n",
    "\t\t\"related question 4\",\n",
    "\t\t\"related question 5\"\n",
    "\t]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def generate_expanded_query(text):\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": system_message},\n",
    "\t\t{\"role\":\"user\",\"content\": text}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\tresponse_format={\"type\": \"json_object\"},\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ベクトル検索時の設定要素について教えてください\"\n",
    "expanded_query = generate_expanded_query(query)\n",
    "parsed_data = json.loads(expanded_query)\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in parsed_data[\"questions\"]:\n",
    "\tvector_query = VectorizableTextQuery(\n",
    "\t\ttext=question,\n",
    "\t\tk_nearest_neighbors=50,\n",
    "\t\tfields=\"vector\",\n",
    "\t)\n",
    "\t# Perform the search\n",
    "\tresults = search_client.search(\n",
    "\t\tquery_type='semantic',\n",
    "  \t\tquery_language='ja',\n",
    "    \tsemantic_configuration_name='my-semantic-config',\n",
    "\t\tsearch_text=query,\n",
    "\t\tvector_queries=[vector_query],\n",
    "\t\ttop=5,\n",
    "\t\tselect=\"content, title, key_phrases\",\n",
    "\t\tsearch_fields=[\"content\", \"title\", \"key_phrases\"],\n",
    "\t)\n",
    "\tprint(\"query: \", question)\n",
    "\tfor result in results:\n",
    "\t\tprint(result)\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE (Hypothetical Document Embeddings)\n",
    "- ユーザのクエリのコンテキストをとらえたい、かつサービスに特化したワードがクエリに含まれる可能性が高いため、Hybrid（フルテキスト検索＋ベクトル検索）＋Semantic Ranker を採用する。\n",
    "  - Hybrid検索のスコアはAzure AI Searchでは、Reciprocal Rank Fusion (RRF) が採用される。 \n",
    "- また、ユーザクエリから検索クエリを新しく生成する。\n",
    "  - クエリはユーザのクエリをスタンドアローンなクエリに変換する。\n",
    "  - また、ユーザのクエリに基づいて仮想的な応答をLLMで作成し、それをベクトル変換した結果を用いて検索をかけるHyDEを採用します。クエリを検索対象のベクトルにより近いものに変換することで、検索精度を高めることを狙った手法です。\n",
    "  - HyDEはLLMがまったく知識を持たないような領域だと役に立たない可能性があるため採用する際は注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_hypothetical_query(text):\n",
    "    hypothetical_gen_instruction = f\"\"\"Please write a passage to answer the question\n",
    "\tQuestion: {text}\n",
    "\tPassage:\n",
    "\t\"\"\"\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": \"You are an AI assistant.\"},\n",
    "\t\t{\"role\":\"user\",\"content\": hypothetical_gen_instruction}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\t# response_format={\"type\": \"json_object\"},\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ベクトル検索時の設定要素について教えてください\"\n",
    "hypothetical_answer = generate_hypothetical_query(query)\n",
    "hypothetical_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query = VectorizableTextQuery(\n",
    "\ttext=hypothetical_answer,\n",
    "\tk_nearest_neighbors=50,\n",
    "\tfields=\"vector\",\n",
    ")\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    query_type='semantic',\n",
    "    query_language='ja',\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "\tsearch_text=hypothetical_answer,\n",
    "\tvector_queries=[vector_query],\n",
    "\ttop=5,\n",
    "\tselect=\"content, title, key_phrases\",\n",
    "\tsearch_fields=[\"content\", \"title\", \"key_phrases\"],\n",
    ")\n",
    "for result in results:\n",
    "\tprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05_Generate-Answer\n",
    "検索インデックスから取得したものをコンテキストとして与えて、それをベースにした回答を生成させるプロンプトを設定します。\n",
    "プロンプトエンジニアリングに関する包括的なガイダンスは以下を参照ください。\n",
    "\n",
    "https://learn.microsoft.com/ja-jp/azure/ai-services/openai/concepts/prompt-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    system_message = f\"\"\"\n",
    "    system:\n",
    "\tYou are an AI assistant that helps users answer questions given a specific context. You will be given a context and asked a question based on that context. Your answer should be as precise as possible and should only come from the context.\n",
    "\tPlease add citation after each sentence when possible in a form \"(Source: citation)\". \n",
    "\tcontext: {context}\n",
    "\tuser: \n",
    "\t\"\"\"\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": system_message},\n",
    "\t\t{\"role\":\"user\",\"content\": query}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\t# response_format={\"type\": \"json_object\"},\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\"\n",
    "for result in results:\n",
    "\tcontext_text += result[\"content\"] + \" \"\n",
    "\n",
    "answer = generate_answer(query, context_text)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06_Evaluation\n",
    "RAG の Evaluation は、「検索評価」と「生成評価」にわけて実施することが推奨される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検索評価 - 簡易評価\n",
    "- シンプルな検索評価として、検索結果の上位5件にユーザクエリを解決するための情報が含まれているかどうかを評価します。\n",
    "- クエリの種類は、想定されるエンドユーザーのクエリや、異なるドキュメントを答えとなるようなクエリを複数パターン用意します。\n",
    "  - 回答に複数の文が必要な抽象的な質問：\n",
    "  - 検索エンジンに一般的に入力されるものと同様の短縮されたクエリ：\n",
    "  - 回答が質問とは異なる単語やフレーズを使用しているクエリ：\n",
    "  - 回答が 1 つしかないクエリ\n",
    "  - 複数の内容を質問しているクエリ\n",
    "- ここではサンプルのため5パターンのクエリを用意しますが、包括的な評価をするためには、100件以上のパターンを用意することが推奨されます。（もちろん、ドキュメントの量やユーザのタスクによって異なるため、それぞれの要件にあわせて設計が必要です）\n",
    "  - [参考：評価用データセットの作成](https://github.com/microsoft/promptflow-resource-hub/blob/main/sample_gallery/golden_dataset/copilot-golden-dataset-creation-guidance.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query set\n",
    "queries = [\n",
    "\t\"AI Search について勉強しています。ベクトル検索時の設定要素について教えてください\",\n",
    "\t\"ハイブリッド検索　メリット\",\n",
    "\t\"Azure AI Search には、リランクのモデルが利用できるか？\",\n",
    "\t\"フルテキスト検索の取得は最大何件か\",\n",
    "\t\"Hybrid検索とセマンティックランカーの特徴と違いはなんですか？\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### シンプルクエリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クエリを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Test for simple query\n",
    "for query in queries:\n",
    "    vector_query = VectorizableTextQuery(\n",
    "        text=query,\n",
    "        k_nearest_neighbors=50,\n",
    "        fields=\"vector\",\n",
    "    )\n",
    "\n",
    "    # Perform the search\n",
    "    results = search_client.search(\n",
    "        query_type='semantic',\n",
    "        query_language='ja',\n",
    "    \tsemantic_configuration_name='my-semantic-config',\n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        top=5,\n",
    "        select=\"content, title, key_phrases\",\n",
    "        search_fields=[\"content\", \"title\", \"key_phrases\"],\n",
    "    )\n",
    "\n",
    "    print(\"Query:\")\n",
    "    pprint.pprint(query)\n",
    "    print(\"\\nResults:\")\n",
    "    for result in results:\n",
    "        pprint.pprint(result)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 回答に複数の文が必要な抽象的な質問  \n",
    "**質問:**  \n",
    "*AI Search について勉強しています。ベクトル検索時の設定要素について教えてください*\n",
    "\n",
    "- **評価:** 部分的に合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 3位\n",
    "  - **理由:** クエリに対応するベクトル検索の設定要素が具体的に記載されており、質問に直接対応していました。\n",
    "\n",
    "##### 2. 検索エンジンに一般的に入力されるものと同様の短縮されたクエリ  \n",
    "**質問:**  \n",
    "*ハイブリッド検索 メリット*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位\n",
    "  - **理由:** ハイブリッド検索のメリットや具体的な使用例、関連性の向上についての記述が見られ、クエリに対して的確に回答しています。\n",
    "\n",
    "##### 3. 回答が質問とは異なる単語やフレーズを使用しているクエリ  \n",
    "**質問:**  \n",
    "*Azure AI Search には、リランクのモデルが利用できるか？*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 2位\n",
    "  - **理由:** セマンティックランク付けについての詳細な説明があり、リランクに関する情報が提供されていますが、さらに具体的な言及が期待されます。\n",
    "\n",
    "##### 4. 回答が 1 つしかないクエリ  \n",
    "**質問:**  \n",
    "*フルテキスト検索の取得は最大何件か*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位\n",
    "  - **理由:** フルテキスト検索の最大取得件数に関する直接的な情報が提供され、クエリに対して明確に回答しています。\n",
    "\n",
    "##### 5. 複数の内容を質問しているクエリ  \n",
    "**質問:**  \n",
    "*Hybrid検索とセマンティックランカーの特徴と違いはなんですか？*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位、2位\n",
    "  - **理由:** ハイブリッド検索とセマンティックランカーの機能に関する比較が含まれており、クエリに対して適切な情報が提供されています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HyDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クエリを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Test for simple query\n",
    "for query in queries:\n",
    "    hypothetical_answer = generate_hypothetical_query(query)   \n",
    "    vector_query = VectorizableTextQuery(\n",
    "\t\ttext=hypothetical_answer,\n",
    "\t\tk_nearest_neighbors=50,\n",
    "\t\tfields=\"vector\",\n",
    "\t)\n",
    "\n",
    "    # Perform the search\n",
    "    results = search_client.search(\n",
    "        query_type='semantic',\n",
    "        query_language='ja',\n",
    "    \tsemantic_configuration_name='my-semantic-config',\n",
    "        search_text=hypothetical_answer,\n",
    "        vector_queries=[vector_query],\n",
    "        top=5,\n",
    "        select=\"content, title, key_phrases\",\n",
    "        search_fields=[\"content\", \"title\", \"key_phrases\"],\n",
    "    )\n",
    "\n",
    "    print(\"Query:\")\n",
    "    pprint.pprint(query)\n",
    "    print(\"\\nHyDE Query:\")\n",
    "    pprint.pprint(hypothetical_answer)\n",
    "    print(\"\\nResults:\")\n",
    "    for result in results:\n",
    "        pprint.pprint(result)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 回答に複数の文が必要な抽象的な質問  \n",
    "**質問:**  \n",
    "*AI Search について勉強しています。ベクトル検索時の設定要素について教えてください*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位\n",
    "  - **理由:** ベクトル検索に関連する設定要素（次元数、距離測定方法、インデックス構築など）が説明されています。\n",
    "\n",
    "##### 2. 検索エンジンに一般的に入力されるものと同様の短縮されたクエリ  \n",
    "**質問:**  \n",
    "*ハイブリッド検索 メリット*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 2位\n",
    "  - **理由:** ハイブリッド検索のメリット（精度の向上、セマンティックランク付けの効果、ベクトル検索とキーワード検索の統合など）が言及されています。\n",
    "\n",
    "##### 3. 回答が質問とは異なる単語やフレーズを使用しているクエリ  \n",
    "**質問:**  \n",
    "*Azure AI Search には、リランクのモデルが利用できるか？*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位\n",
    "  - **理由:** Azure AI Searchでセマンティックランク付けや再ランク付けのプロセスに関する情報が提供されています。\n",
    "\n",
    "##### 4. 回答が 1 つしかないクエリ  \n",
    "**質問:**  \n",
    "*フルテキスト検索の取得は最大何件か*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 2位\n",
    "  - **理由:** フルテキスト検索結果の取得件数に関する言及（既定の50件、最大1000件まで）が含まれています。\n",
    "\n",
    "##### 5. 複数の内容を質問しているクエリ  \n",
    "**質問:**  \n",
    "*Hybrid検索とセマンティックランカーの特徴と違いはなんですか？*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位, 2位, 3位\n",
    "  - **理由:** ハイブリッド検索とセマンティックランク付けに関するそれぞれの特徴についての言及が見られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検索評価 - 厳密な評価\n",
    "TBD\n",
    "- 評価指標の設計\n",
    "- クエリパターン\n",
    "- 評価用データの選定・設計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成評価 - 簡易評価\n",
    "- シンプルな検索評価として、検索結果の上位5件にユーザクエリを解決するための情報が含まれているかどうかを評価します。\n",
    "- クエリの種類は、想定されるエンドユーザーのクエリや、異なるドキュメントを答えとなるようなクエリを複数パターン用意します。\n",
    "  - 回答に複数の文が必要な抽象的な質問：\n",
    "  - 検索エンジンに一般的に入力されるものと同様の短縮されたクエリ：\n",
    "  - 回答が質問とは異なる単語やフレーズを使用しているクエリ：\n",
    "  - 回答が 1 つしかないクエリ\n",
    "  - 複数の内容を質問しているクエリ\n",
    "- ここではサンプルのため5パターンのクエリを用意しますが、包括的な評価をするためには、100件以上のパターンを用意することが推奨されます。（もちろん、ドキュメントの量やユーザのタスクによって異なるため、それぞれの要件にあわせて設計が必要です）\n",
    "  - [参考：評価用データセットの作成](https://github.com/microsoft/promptflow-resource-hub/blob/main/sample_gallery/golden_dataset/copilot-golden-dataset-creation-guidance.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG Pipeline\n",
    "def rag_pipeline_with_hyde(query):\n",
    "\thypothetical_answer = generate_hypothetical_query(query)\n",
    "\tvector_query = VectorizableTextQuery(\n",
    "\t\ttext=hypothetical_answer,\n",
    "\t\tk_nearest_neighbors=50,\n",
    "\t\tfields=\"vector\",\n",
    "\t)\n",
    "\t# Perform the search\n",
    "\tresults = search_client.search(\n",
    "\t\tquery_type='semantic',\n",
    "\t\tquery_language='ja',\n",
    "    \tsemantic_configuration_name='my-semantic-config',\n",
    "\t\tsearch_text=hypothetical_answer,\n",
    "\t\tvector_queries=[vector_query],\n",
    "\t\ttop=5,\n",
    "\t\tselect=\"content, title, key_phrases\",\n",
    "\t\tsearch_fields=[\"content\", \"title\", \"key_phrases\"],\n",
    "\t)\n",
    "\t\n",
    "\tcontext_text = \"\"\n",
    "\tfor result in results:\n",
    "\t\tcontext_text += result[\"content\"] + \" \"\n",
    "\n",
    "\treturn generate_answer(query, context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query set\n",
    "queries = [\n",
    "\t\"AI Search について勉強しています。ベクトル検索時の設定要素について教えてください\",\n",
    "\t\"ハイブリッド検索　メリット\",\n",
    "\t\"Azure AI Search には、リランクのモデルが利用できるか？\",\n",
    "\t\"フルテキスト検索の取得は最大何件か\",\n",
    "\t\"Hybrid検索とセマンティックランカーの特徴と違いはなんですか？\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for query in queries:\n",
    "    start_time = time.time()\n",
    "    answer = rag_pipeline_with_hyde(query)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
