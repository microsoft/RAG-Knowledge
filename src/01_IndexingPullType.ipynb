{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Pull Type Import Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case\n",
    "- Azure AI Search の サービス仕様ドキュメントをインプットにする。\n",
    "  - https://learn.microsoft.com/ja-jp/azure/search/\n",
    "- ドキュメントは OCR が必要。\n",
    "- 開発者マニュアルは、構造化されたセクションとなっている。\n",
    "- 各セクションは非常に詳細かつ専門性の高い技術解説が記載されており、ドキュメントサイズも大きい。\n",
    "- ドキュメントには、テキスト、テーブル、図、グラフなどが含まれるが、ここでは、テキスト、テーブルデータのみを扱う。\n",
    "\n",
    "## チャンキング設計\n",
    "- Document Intelligence で、Markdown形式でテキストデータを抽出済み。\n",
    "- 1つのドキュメントに大量のコンテキストが含まれており、ドキュメントサイズも大きいため、チャンキングを実施する。\n",
    "- チャンキングは Azure AI Search が提供する Text Split skill を利用する。\n",
    "- 各チャンキングのContentはEmbeddingする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-search-documents==11.6.0b4\n",
    "! pip install openai python-dotenv azure-identity cohere azure-ai-vision-imageanalysis\n",
    "! pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AIServicesVisionParameters,\n",
    "    AIServicesVisionVectorizer,\n",
    "    AIStudioModelCatalogName,\n",
    "    AzureMachineLearningVectorizer,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIModelName,\n",
    "    AzureOpenAIParameters,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    BlobIndexerDataToExtract,\n",
    "    BlobIndexerParsingMode,\n",
    "    CognitiveServicesAccountKey,\n",
    "    DefaultCognitiveServicesAccount,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    FieldMapping,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    IndexerExecutionStatus,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    InputFieldMappingEntry,\n",
    "    KeyPhraseExtractionSkill,\n",
    "    OutputFieldMappingEntry,\n",
    "    ScalarQuantizationCompressionConfiguration,\n",
    "    ScalarQuantizationParameters,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataIdentity,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerIndexProjections,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    SearchIndexerSkillset,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SimpleField,\n",
    "    SplitSkill,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    "    VisionVectorizeSkill\n",
    ")\n",
    "from azure.search.documents.models import (\n",
    "    HybridCountAndFacetMode,\n",
    "    HybridSearch,\n",
    "    SearchScoreThreshold,\n",
    "    VectorizableTextQuery,\n",
    "    VectorizableImageBinaryQuery,\n",
    "    VectorizableImageUrlQuery,\n",
    "    VectorSimilarityThreshold,\n",
    ")\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display, HTML\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "AZURE_AI_VISION_API_KEY = os.getenv(\"AZURE_AI_VISION_API_KEY\")\n",
    "AZURE_AI_VISION_ENDPOINT = os.getenv(\"AZURE_AI_VISION_ENDPOINT\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "BLOB_CONNECTION_STRING = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "INDEX_NAME = \"rag-search-index-pull\"\n",
    "AZURE_SEARCH_ADMIN_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_AI_MULTI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_MULTI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_MULTI_SERVICE_KEY = os.getenv(\"AZURE_AI_MULTI_SERVICE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified parameter\n",
    "USE_AAD_FOR_SEARCH = False  # Set this to False to use API key for authentication\n",
    "\n",
    "def authenticate_azure_search(api_key=None, use_aad_for_search=False):\n",
    "    if use_aad_for_search:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "    return credential\n",
    "\n",
    "azure_search_credential = authenticate_azure_search(api_key=AZURE_SEARCH_ADMIN_KEY, use_aad_for_search=USE_AAD_FOR_SEARCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットのアップロード\n",
    "Azure AI Search のインデクサーにより、データをインポートする場合は各データソースにデータを格納しておく必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.azure_blob_operation import upload_folder_to_blob\n",
    "\n",
    "connection_string = BLOB_CONNECTION_STRING\n",
    "container_name = \"rag-knowledge-01-aisearch\"\n",
    "local_folder_path = \"../output/01_output\"\n",
    "\n",
    "upload_folder_to_blob(connection_string, container_name, local_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a blob data source connector on Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_update_data_source(indexer_client, container_name, connection_string, index_name):\n",
    "    \"\"\"\n",
    "    Create or update a data source connection for Azure AI Search.\n",
    "    \"\"\"\n",
    "    container = SearchIndexerDataContainer(name=container_name, query=\"documents\") # Query is optional, but can be used to filter the folters in the blob container\n",
    "    data_source_connection = SearchIndexerDataSourceConnection(\n",
    "        name=f\"{index_name}-blob\",\n",
    "        type=\"azureblob\",\n",
    "        connection_string=connection_string,\n",
    "        container=container\n",
    "    )\n",
    "    try:\n",
    "        indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "        print(f\"Data source '{index_name}-blob' created or updated successfully.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to create or update data source due to error: {e}\")\n",
    "\n",
    "# Create a SearchIndexerClient instance\n",
    "indexer_client = SearchIndexerClient(AZURE_SEARCH_ENDPOINT, azure_search_credential)\n",
    "\n",
    "# Call the function to create or update the data source\n",
    "create_or_update_data_source(indexer_client, container_name, BLOB_CONNECTION_STRING, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a search index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インデックスの定義\n",
    "- インデックスは単一で構成する。\n",
    "- インプットドキュメントが説明的な内容のため、`Hybrid + Semantic Ranker` を採用する。そのため、ベクトル検索、 Semantic Ranker ための設定をする。\n",
    "\n",
    "### フィールドの定義\n",
    "- ドキュメントをチャンク分割したテキストを `chunk` フィールドに格納する。\n",
    "- ユーザのコンテキストを捉えるために、ベクトル検索を採用する。そのため、`Embedding` フィールドを構成する。\n",
    "  - Embedding Model: `text-embedding-ada-002`を採用する。\n",
    "- ドキュメントのタイトルを`Title`フィールドに含める。\n",
    "- ドキュメント内にドメイン固有なワードが頻繁に出現するため、`key_phrases` フィールドに各チャンク内のキーフレーズを含める。\n",
    "- ソースドキュメントの追跡用に `metadata_storage_last_modified`, `metadata_storage_path` を含める。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Enabling Semantic Ranker\n",
    "Uses Microsoft’s language understanding models to rerank search results, enhancing relevance and providing results more aligned with the user’s context.\n",
    "\n",
    "#### Implementation Considerations\n",
    "Semantic Ranker を有効にする際の考慮事項を記載します。\n",
    "- Semantic Ranker は、テキストクエリの BM25 でランク付けされた検索結果から、またはハイブリッド クエリの RRF でランク付けされた結果をリランキングします。\n",
    "- 検索結果の数が 50 個を超える場合でも、リランキングが行われるのは上位 50 個の結果のみです。そのため、処理されない結果があることに注意してください。\n",
    "- また、Semantic Ranker は、文章のコンテキストを理解させるために利用するため、適用対象のフィールドは説明的なものを指定することが推奨されます。ナレッジベース、オンラインドキュメントなど説明的なコンテンツを含むドキュメントでは、Semantic Ranker から最も多くのメリットが得られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Configuration for Semantic Ranker\n",
    "def create_semantic_config():\n",
    "\tsemantic_config = SemanticConfiguration(\n",
    "\t\tname=\"my-semantic-config\",\n",
    "\t\tprioritized_fields=SemanticPrioritizedFields(\n",
    "\t\t\ttitle_field=SemanticField(field_name=\"title\"),\n",
    "\t\t\tkeywords_fields=[SemanticField(field_name=\"key_phrases\")],\n",
    "\t\t\tcontent_fields=[SemanticField(field_name=\"chunk\")],\n",
    "\t\t)\n",
    "\t)\n",
    "\t# Create the semantic settings with the configuration\n",
    "\tsemantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\treturn semantic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index(index_name, azure_openai_endpoint, azure_openai_embedding_deployment_id, azure_openai_key=None):\n",
    "    return SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=[\n",
    "            SearchField(\n",
    "                name=\"chunk_id\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                key=True,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=True,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                analyzer_name=\"keyword\"\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"parent_id\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=True,\n",
    "                facetable=False,\n",
    "                searchable=False\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"chunk\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=False,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                analyzer_name=\"ja.microsoft\" # replace with your analyzer\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"title\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=False,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                analyzer_name=\"ja.microsoft\" # replace with your analyzer\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"key_phrases\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=False,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                analyzer_name=\"ja.microsoft\" # replace with your analyzer\n",
    "            ),\n",
    "            SearchField(\n",
    "                name=\"vector\",\n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                hidden=False,\n",
    "                filterable=False,\n",
    "                sortable=False,\n",
    "                facetable=False,\n",
    "                searchable=True,\n",
    "                vector_search_dimensions=1536,\n",
    "                vector_search_profile_name=\"profile\"\n",
    "            ),\n",
    "            SimpleField(\n",
    "\t\t\t\tname=\"metadata_storage_last_modified\",\n",
    "                type=SearchFieldDataType.DateTimeOffset,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=True,\n",
    "                facetable=False,\n",
    "                searchable=False,\n",
    "\t\t\t),\n",
    "            SimpleField(\n",
    "\t\t\t\tname=\"metadata_storage_path\",\n",
    "                type=SearchFieldDataType.String,\n",
    "                hidden=False,\n",
    "                filterable=True,\n",
    "                sortable=True,\n",
    "                facetable=False,\n",
    "                searchable=False,\n",
    "\t\t\t)\n",
    "        ],\n",
    "        vector_search=VectorSearch(\n",
    "\t\t\talgorithms=[\n",
    "\t\t\t\tHnswAlgorithmConfiguration(\n",
    "\t\t\t\t\tname=\"myHnsw\",\n",
    "\t\t\t\t\tparameters=HnswParameters(\n",
    "\t\t\t\t\t\tm=4,\n",
    "\t\t\t\t\t\tef_construction=400,\n",
    "\t\t\t\t\t\tef_search=500,\n",
    "\t\t\t\t\t\tmetric=VectorSearchAlgorithmMetric.COSINE,\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t)\n",
    "\t\t\t],\n",
    "\t\t\tvectorizers=[\n",
    "\t\t\t\tAzureOpenAIVectorizer(\n",
    "\t\t\t\t\tname=\"myAzureOpenAIVectorizer\",\n",
    "\t\t\t\t\tkind=\"azureOpenAI\",\n",
    "\t\t\t\t\tazure_open_ai_parameters=AzureOpenAIParameters(\n",
    "\t\t\t\t\t\tresource_uri=azure_openai_endpoint,\n",
    "\t\t\t\t\t\tapi_key=azure_openai_key,\n",
    "\t\t\t\t\t\tdeployment_id=azure_openai_embedding_deployment_id,\n",
    "\t\t\t\t\t\tmodel_name=AzureOpenAIModelName.TEXT_EMBEDDING_ADA002,\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t)\n",
    "\t\t\t],\n",
    "\t\t\tprofiles=[\n",
    "\t\t\t\tVectorSearchProfile(\n",
    "\t\t\t\t\tname=\"profile\",\n",
    "\t\t\t\t\talgorithm_configuration_name=\"myHnsw\",\n",
    "\t\t\t\t\tvectorizer=\"myAzureOpenAIVectorizer\",\n",
    "\t\t\t\t)\n",
    "\t\t\t],\n",
    "    \t),\n",
    "        semantic_search=create_semantic_config() # Here we add the semantic search configuration\n",
    "\t)\n",
    "\n",
    "index = create_search_index(\n",
    "    INDEX_NAME,\n",
    "    AZURE_OPENAI_ENDPOINT,\n",
    "    \"text-embedding-ada-002\", # replace with your deployment name\n",
    "    AZURE_OPENAI_API_KEY\n",
    ")\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=AZURE_SEARCH_ENDPOINT, credential=azure_search_credential\n",
    ")\n",
    "index_client.create_or_update_index(index)\n",
    "\n",
    "print(f\"Created index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Skillset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Text Splitter**: ドキュメントをチャンキングする。\n",
    "  - text_split_mode: ページ単位での分割\n",
    "  - maximum_page_length: 最大2000文字（大きすぎると精度が低下するため）\n",
    "  - page_overlap_length: コンテキストがLostを最小限にするために、ページの前後のテキストを各チャンクに含める。（25％で設定）\n",
    "- **Embeddings**: AOAIのモデルを利用してテキストデータのEmbeddingをする。\n",
    "- **Key Phrase Extraction**: ドキュメントからキーフレーズを抽出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_skillset(\n",
    "        skillset_name,\n",
    "        index_name,\n",
    "        azure_openai_endpoint,\n",
    "        azure_openai_embedding_deployment_id,\n",
    "        azure_openai_key=None,\n",
    "        text_split_mode='pages',\n",
    "        maximum_page_length=2000,\n",
    "        page_overlap_length=500):\n",
    "    return SearchIndexerSkillset(\n",
    "        name=skillset_name,\n",
    "        skills=[\n",
    "            SplitSkill(\n",
    "                name=\"Text Splitter\",\n",
    "                default_language_code=\"ja\", # replace documents language code\n",
    "                text_split_mode=text_split_mode,\n",
    "                maximum_page_length=maximum_page_length,\n",
    "                page_overlap_length=page_overlap_length,\n",
    "                context=\"/document\",\n",
    "                inputs=[\n",
    "                    InputFieldMappingEntry(\n",
    "                        name=\"text\",\n",
    "                        source=\"/document/content\"\n",
    "                    )\n",
    "                ],\n",
    "                outputs=[\n",
    "                    OutputFieldMappingEntry(\n",
    "                        name=\"textItems\",\n",
    "                        target_name=\"pages\"\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            AzureOpenAIEmbeddingSkill(\n",
    "                name=\"Embeddings\",\n",
    "                resource_uri=azure_openai_endpoint,\n",
    "                deployment_id=azure_openai_embedding_deployment_id,\n",
    "                api_key=azure_openai_key, # Optional if using RBAC authentication\n",
    "                model_name=AzureOpenAIModelName.TEXT_EMBEDDING_ADA002,\n",
    "                context=\"/document/pages/*\",\n",
    "                inputs=[\n",
    "                    InputFieldMappingEntry(\n",
    "                        name=\"text\",\n",
    "                        source=\"/document/pages/*\"\n",
    "                    )\n",
    "                ],\n",
    "                outputs=[\n",
    "                    OutputFieldMappingEntry(\n",
    "                        name=\"embedding\",\n",
    "                        target_name=\"vector\"\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            KeyPhraseExtractionSkill(\n",
    "\t\t\t\tname=\"Key Phrase Extraction\",\n",
    "\t\t\t\tcontext=\"/document/pages/*\",\n",
    "\t\t\t\tdefault_language_code=\"ja\", # replace documents language code\n",
    "                inputs=[\n",
    "                    InputFieldMappingEntry(\n",
    "                        name=\"text\",\n",
    "                        source=\"/document/pages/*\"\n",
    "                    )\n",
    "                ],\n",
    "                outputs=[\n",
    "                    OutputFieldMappingEntry(\n",
    "                        name=\"keyPhrases\",\n",
    "                        target_name=\"key_phrases\"\n",
    "                    )\n",
    "                ]\n",
    "\t\t\t)\n",
    "        ],\n",
    "        index_projections=SearchIndexerIndexProjections(\n",
    "            selectors=[\n",
    "                SearchIndexerIndexProjectionSelector(\n",
    "                    target_index_name=index_name,\n",
    "                    parent_key_field_name=\"parent_id\",\n",
    "                    source_context=\"/document/pages/*\",\n",
    "                    mappings=[\n",
    "                        InputFieldMappingEntry(\n",
    "                            name=\"chunk\",\n",
    "                            source=\"/document/pages/*\"\n",
    "                        ),\n",
    "                        InputFieldMappingEntry(\n",
    "                            name=\"vector\",\n",
    "                            source=\"/document/pages/*/vector\"\n",
    "                        ),\n",
    "                        InputFieldMappingEntry(\n",
    "                            name=\"title\",\n",
    "                            source=\"/document/metadata_storage_name\"\n",
    "                        ),\n",
    "                        InputFieldMappingEntry(\n",
    "                            name=\"key_phrases\",\n",
    "                            source=\"/document/pages/*/key_phrases\"\n",
    "                        ),\n",
    "                        InputFieldMappingEntry(\n",
    "                            name=\"metadata_storage_last_modified\",\n",
    "                            source=\"/document/metadata_storage_last_modified\"\n",
    "                        ),\n",
    "                        InputFieldMappingEntry(\n",
    "                            name=\"metadata_storage_path\",\n",
    "                            source=\"/document/metadata_storage_path\"\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            ],\n",
    "            parameters=SearchIndexerIndexProjectionsParameters(projection_mode=\"skipIndexingParentDocuments\")\n",
    "        ),\n",
    "        cognitive_services_account=CognitiveServicesAccountKey(key=AZURE_AI_MULTI_SERVICE_KEY)\n",
    "    )\n",
    "\n",
    "skillset_name = f\"{INDEX_NAME}-skillset\"\n",
    "skillset = create_search_skillset(\n",
    "    skillset_name,\n",
    "    INDEX_NAME,\n",
    "    AZURE_OPENAI_ENDPOINT,\n",
    "    \"text-embedding-ada-002\", # replace with your deployment name\n",
    "    AZURE_OPENAI_API_KEY,\n",
    "    text_split_mode='pages',\n",
    "    maximum_page_length=2000,\n",
    "    page_overlap_length=500\n",
    ")\n",
    "search_indexer_client = SearchIndexerClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=azure_search_credential)\n",
    "search_indexer_client.create_or_update_skillset(skillset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- データソースは Azure Storage Account のため、変更検出がサポートされている。\n",
    "- ここでは、サンプルのため、スケジューリングの設定はしないが、スケジューリングを設定すると、インデクサーによりドキュメントの変更（新規追加も含む）検知をして増分データのみをインデックス化することが可能になる。\n",
    "  - これにより、すべてのソースドキュメントをスキャンすることがなく、インデックス化のジョブのパフォーマンスを最適化することが可能になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_indexer(indexer_name, skillset_name, datasource_name, index_name):\n",
    "    return SearchIndexer(\n",
    "        name=indexer_name,\n",
    "        data_source_name=datasource_name,\n",
    "        target_index_name=index_name,\n",
    "        skillset_name=skillset_name\n",
    "    )\n",
    "\n",
    "indexer_name = f\"{INDEX_NAME}-indexer\"\n",
    "indexer = create_search_indexer(indexer_name, skillset_name, f\"{INDEX_NAME}-blob\", INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_indexer_client.create_or_update_indexer(indexer)\n",
    "print(f\"{indexer_name} created or updated.\")\n",
    "search_indexer_client.run_indexer(indexer_name)\n",
    "print(f\"{indexer_name} is running. If queries return no results, please wait a bit and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_Query-Design\n",
    "Tips for Azure AI Search Query-Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Search\n",
    "- ユーザのクエリのコンテキストをとらえたい、かつサービスに特化したワードがクエリに含まれる可能性が高いため、Hybrid（フルテキスト検索＋ベクトル検索）＋Semantic Ranker を採用する。\n",
    "  - Hybrid検索のスコアはAzure AI Searchでは、Reciprocal Rank Fusion (RRF) が採用される。 \n",
    "- クエリはユーザのクエリをそのまま検索インデックスのクエリに利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=INDEX_NAME, credential=azure_search_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ベクトル検索時の設定要素について教えてください\"\n",
    "\n",
    "vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=50,\n",
    "    fields=\"vector\",\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    query_type='semantic',\n",
    "    query_language='ja',\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    top=5,\n",
    "    select=\"chunk, title, key_phrases\",\n",
    "\tsearch_fields=[\"chunk\", \"title\", \"key_phrases\"],\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Expansion\n",
    "- ユーザのクエリのコンテキストをとらえたい、かつサービスに特化したワードがクエリに含まれる可能性が高いため、Hybrid（フルテキスト検索＋ベクトル検索）＋Semantic Ranker を採用する。\n",
    "  - Hybrid検索のスコアはAzure AI Searchでは、Reciprocal Rank Fusion (RRF) が採用される。 \n",
    "- また、ユーザクエリから検索クエリを新しく生成する。\n",
    "  - クエリはユーザのクエリをスタンドアローンなクエリに変換する。\n",
    "  - また、検索のカバレッジを大きくするために、類似した入力クエリを複数生成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "system_message = \"\"\"\n",
    "# Your Task\n",
    "- Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\n",
    "- You also need to extend the original question to generate 5 related queries. This is done to capture the broader context of the user's question.\n",
    "- You must output json format. In other words, You must output array of questions that length is 5.\n",
    "\n",
    "# Json format example:\n",
    "{\n",
    "\t\"questions\": [\n",
    "\t\t\"related question 1\",\n",
    "\t\t\"related question 2\",\n",
    "\t\t\"related question 3\",\n",
    "\t\t\"related question 4\",\n",
    "\t\t\"related question 5\"\n",
    "\t]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def generate_expanded_query(text):\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": system_message},\n",
    "\t\t{\"role\":\"user\",\"content\": text}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\tresponse_format={\"type\": \"json_object\"},\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ベクトル検索時の設定要素について教えてください\"\n",
    "expanded_query = generate_expanded_query(query)\n",
    "parsed_data = json.loads(expanded_query)\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in parsed_data[\"questions\"]:\n",
    "\tvector_query = VectorizableTextQuery(\n",
    "\t\ttext=question,\n",
    "\t\tk_nearest_neighbors=50,\n",
    "\t\tfields=\"vector\",\n",
    "\t)\n",
    "\t# Perform the search\n",
    "\tresults = search_client.search(\n",
    "\t\tquery_type='semantic',\n",
    "  \t\tquery_language='ja',\n",
    "    \tsemantic_configuration_name='my-semantic-config',\n",
    "\t\tsearch_text=query,\n",
    "\t\tvector_queries=[vector_query],\n",
    "\t\ttop=5,\n",
    "\t\tselect=\"chunk, title, key_phrases\",\n",
    "\t\tsearch_fields=[\"chunk\", \"title\", \"key_phrases\"],\n",
    "\t)\n",
    "\tprint(\"query: \", question)\n",
    "\tfor result in results:\n",
    "\t\tprint(result)\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE (Hypothetical Document Embeddings)\n",
    "- ユーザのクエリのコンテキストをとらえたい、かつサービスに特化したワードがクエリに含まれる可能性が高いため、Hybrid（フルテキスト検索＋ベクトル検索）＋Semantic Ranker を採用する。\n",
    "  - Hybrid検索のスコアはAzure AI Searchでは、Reciprocal Rank Fusion (RRF) が採用される。 \n",
    "- また、ユーザクエリから検索クエリを新しく生成する。\n",
    "  - クエリはユーザのクエリをスタンドアローンなクエリに変換する。\n",
    "  - また、ユーザのクエリに基づいて仮想的な応答をLLMで作成し、それをベクトル変換した結果を用いて検索をかけるHyDEを採用します。クエリを検索対象のベクトルにより近いものに変換することで、検索精度を高めることを狙った手法です。\n",
    "  - HyDEはLLMがまったく知識を持たないような領域だと役に立たない可能性があるため採用する際は注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_hypothetical_query(text):\n",
    "    hypothetical_gen_instruction = f\"\"\"Please write a passage to answer the question\n",
    "\tQuestion: {text}\n",
    "\tPassage:\n",
    "\t\"\"\"\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": \"You are an AI assistant.\"},\n",
    "\t\t{\"role\":\"user\",\"content\": hypothetical_gen_instruction}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\t# response_format={\"type\": \"json_object\"},\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ベクトル検索時の設定要素について教えてください\"\n",
    "hypothetical_answer = generate_hypothetical_query(query)\n",
    "hypothetical_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_query = VectorizableTextQuery(\n",
    "\ttext=hypothetical_answer,\n",
    "\tk_nearest_neighbors=50,\n",
    "\tfields=\"vector\",\n",
    ")\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    query_type='semantic',\n",
    "    query_language='ja',\n",
    "    semantic_configuration_name='my-semantic-config',\n",
    "\tsearch_text=hypothetical_answer,\n",
    "\tvector_queries=[vector_query],\n",
    "\ttop=5,\n",
    "\tselect=\"chunk, title, key_phrases\",\n",
    "\tsearch_fields=[\"chunk\", \"title\", \"key_phrases\"],\n",
    ")\n",
    "for result in results:\n",
    "\tprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05_Generate-Answer\n",
    "検索インデックスから取得したものをコンテキストとして与えて、それをベースにした回答を生成させるプロンプトを設定します。\n",
    "プロンプトエンジニアリングに関する包括的なガイダンスは以下を参照ください。\n",
    "\n",
    "https://learn.microsoft.com/ja-jp/azure/ai-services/openai/concepts/prompt-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    system_message = f\"\"\"\n",
    "    system:\n",
    "\tYou are an AI assistant that helps users answer questions given a specific context. You will be given a context and asked a question based on that context. Your answer should be as precise as possible and should only come from the context.\n",
    "\tPlease add citation after each sentence when possible in a form \"(Source: citation)\". \n",
    "\tcontext: {context}\n",
    "\tuser: \n",
    "\t\"\"\"\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": system_message},\n",
    "\t\t{\"role\":\"user\",\"content\": query}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\t# response_format={\"type\": \"json_object\"},\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\"\n",
    "for result in results:\n",
    "\tcontext_text += result[\"chunk\"] + \" \"\n",
    "\n",
    "answer = generate_answer(query, context_text)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06_Evaluation\n",
    "RAG の Evaluation は、「検索評価」と「生成評価」にわけて実施することが推奨される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検索評価 - 簡易評価\n",
    "- シンプルな検索評価として、検索結果の上位5件にユーザクエリを解決するための情報が含まれているかどうかを評価します。\n",
    "- クエリの種類は、想定されるエンドユーザーのクエリや、異なるドキュメントを答えとなるようなクエリを複数パターン用意します。\n",
    "  - 回答に複数の文が必要な抽象的な質問：\n",
    "  - 検索エンジンに一般的に入力されるものと同様の短縮されたクエリ：\n",
    "  - 回答が質問とは異なる単語やフレーズを使用しているクエリ：\n",
    "  - 回答が 1 つしかないクエリ\n",
    "  - 複数の内容を質問しているクエリ\n",
    "- ここではサンプルのため5パターンのクエリを用意しますが、包括的な評価をするためには、100件以上のパターンを用意することが推奨されます。（もちろん、ドキュメントの量やユーザのタスクによって異なるため、それぞれの要件にあわせて設計が必要です）\n",
    "  - [参考：評価用データセットの作成](https://github.com/microsoft/promptflow-resource-hub/blob/main/sample_gallery/golden_dataset/copilot-golden-dataset-creation-guidance.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query set\n",
    "queries = [\n",
    "\t\"AI Search について勉強しています。ベクトル検索時の設定要素について教えてください\",\n",
    "\t\"ハイブリッド検索　メリット\",\n",
    "\t\"Azure AI Search には、リランクのモデルが利用できるか？\",\n",
    "\t\"フルテキスト検索の取得は最大何件か\",\n",
    "\t\"Hybrid検索とセマンティックランカーの特徴と違いはなんですか？\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### シンプルクエリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クエリを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Test for simple query\n",
    "for query in queries:\n",
    "    vector_query = VectorizableTextQuery(\n",
    "        text=query,\n",
    "        k_nearest_neighbors=50,\n",
    "        fields=\"vector\",\n",
    "    )\n",
    "\n",
    "    # Perform the search\n",
    "    results = search_client.search(\n",
    "        query_type='semantic',\n",
    "        query_language='ja',\n",
    "    \tsemantic_configuration_name='my-semantic-config',\n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        top=5,\n",
    "        select=\"chunk, title, key_phrases\",\n",
    "        search_fields=[\"chunk\", \"title\", \"key_phrases\"],\n",
    "    )\n",
    "\n",
    "    print(\"Query:\")\n",
    "    pprint.pprint(query)\n",
    "    print(\"\\nResults:\")\n",
    "    for result in results:\n",
    "        pprint.pprint(result)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 回答に複数の文が必要な抽象的な質問  \n",
    "**質問:**  \n",
    "*AI Search について勉強しています。ベクトル検索時の設定要素について教えてください*\n",
    "\n",
    "- **評価:** 部分的に合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 3位\n",
    "  - **理由:** クエリに対応するベクトル検索の設定要素が具体的に記載されており、質問に直接対応していました。\n",
    "\n",
    "##### 2. 検索エンジンに一般的に入力されるものと同様の短縮されたクエリ  \n",
    "**質問:**  \n",
    "*ハイブリッド検索 メリット*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位\n",
    "  - **理由:** ハイブリッド検索のメリットや具体的な使用例、関連性の向上についての記述が見られ、クエリに対して的確に回答しています。\n",
    "\n",
    "##### 3. 回答が質問とは異なる単語やフレーズを使用しているクエリ  \n",
    "**質問:**  \n",
    "*Azure AI Search には、リランクのモデルが利用できるか？*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 2位\n",
    "  - **理由:** セマンティックランク付けについての詳細な説明があり、リランクに関する情報が提供されていますが、さらに具体的な言及が期待されます。\n",
    "\n",
    "##### 4. 回答が 1 つしかないクエリ  \n",
    "**質問:**  \n",
    "*フルテキスト検索の取得は最大何件か*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位\n",
    "  - **理由:** フルテキスト検索の最大取得件数に関する直接的な情報が提供され、クエリに対して明確に回答しています。\n",
    "\n",
    "##### 5. 複数の内容を質問しているクエリ  \n",
    "**質問:**  \n",
    "*Hybrid検索とセマンティックランカーの特徴と違いはなんですか？*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位、2位\n",
    "  - **理由:** ハイブリッド検索とセマンティックランカーの機能に関する比較が含まれており、クエリに対して適切な情報が提供されています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HyDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クエリを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Test for simple query\n",
    "for query in queries:\n",
    "    hypothetical_answer = generate_hypothetical_query(query)   \n",
    "    vector_query = VectorizableTextQuery(\n",
    "\t\ttext=hypothetical_answer,\n",
    "\t\tk_nearest_neighbors=50,\n",
    "\t\tfields=\"vector\",\n",
    "\t)\n",
    "\n",
    "    # Perform the search\n",
    "    results = search_client.search(\n",
    "        query_type='semantic',\n",
    "        query_language='ja',\n",
    "    \tsemantic_configuration_name='my-semantic-config',\n",
    "        search_text=hypothetical_answer,\n",
    "        vector_queries=[vector_query],\n",
    "        top=5,\n",
    "        select=\"chunk, title, key_phrases\",\n",
    "        search_fields=[\"chunk\", \"title\", \"key_phrases\"],\n",
    "    )\n",
    "\n",
    "    print(\"Query:\")\n",
    "    pprint.pprint(query)\n",
    "    print(\"\\nHyDE Query:\")\n",
    "    pprint.pprint(hypothetical_answer)\n",
    "    print(\"\\nResults:\")\n",
    "    for result in results:\n",
    "        pprint.pprint(result)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 回答に複数の文が必要な抽象的な質問  \n",
    "**質問:**  \n",
    "*AI Search について勉強しています。ベクトル検索時の設定要素について教えてください*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位\n",
    "  - **理由:** ベクトル検索に関連する設定要素（次元数、距離測定方法、インデックス構築など）が説明されています。\n",
    "\n",
    "##### 2. 検索エンジンに一般的に入力されるものと同様の短縮されたクエリ  \n",
    "**質問:**  \n",
    "*ハイブリッド検索 メリット*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 2位\n",
    "  - **理由:** ハイブリッド検索のメリット（精度の向上、セマンティックランク付けの効果、ベクトル検索とキーワード検索の統合など）が言及されています。\n",
    "\n",
    "##### 3. 回答が質問とは異なる単語やフレーズを使用しているクエリ  \n",
    "**質問:**  \n",
    "*Azure AI Search には、リランクのモデルが利用できるか？*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位\n",
    "  - **理由:** Azure AI Searchでセマンティックランク付けや再ランク付けのプロセスに関する情報が提供されています。\n",
    "\n",
    "##### 4. 回答が 1 つしかないクエリ  \n",
    "**質問:**  \n",
    "*フルテキスト検索の取得は最大何件か*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 2位\n",
    "  - **理由:** フルテキスト検索結果の取得件数に関する言及（既定の50件、最大1000件まで）が含まれています。\n",
    "\n",
    "##### 5. 複数の内容を質問しているクエリ  \n",
    "**質問:**  \n",
    "*Hybrid検索とセマンティックランカーの特徴と違いはなんですか？*\n",
    "\n",
    "- **評価:** 合致\n",
    "  - **意図した回答が含まれるチャンクの順位:** 1位, 2位, 3位\n",
    "  - **理由:** ハイブリッド検索とセマンティックランク付けに関するそれぞれの特徴についての言及が見られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検索評価 - 厳密な評価\n",
    "TBD\n",
    "- 評価指標の設計\n",
    "- クエリパターン\n",
    "- 評価用データの選定・設計"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成評価 - 簡易評価\n",
    "- シンプルな検索評価として、検索結果の上位5件にユーザクエリを解決するための情報が含まれているかどうかを評価します。\n",
    "- クエリの種類は、想定されるエンドユーザーのクエリや、異なるドキュメントを答えとなるようなクエリを複数パターン用意します。\n",
    "  - 回答に複数の文が必要な抽象的な質問：\n",
    "  - 検索エンジンに一般的に入力されるものと同様の短縮されたクエリ：\n",
    "  - 回答が質問とは異なる単語やフレーズを使用しているクエリ：\n",
    "  - 回答が 1 つしかないクエリ\n",
    "  - 複数の内容を質問しているクエリ\n",
    "- ここではサンプルのため5パターンのクエリを用意しますが、包括的な評価をするためには、100件以上のパターンを用意することが推奨されます。（もちろん、ドキュメントの量やユーザのタスクによって異なるため、それぞれの要件にあわせて設計が必要です）\n",
    "  - [参考：評価用データセットの作成](https://github.com/microsoft/promptflow-resource-hub/blob/main/sample_gallery/golden_dataset/copilot-golden-dataset-creation-guidance.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG Pipeline\n",
    "def rag_pipeline_with_hyde(query):\n",
    "\thypothetical_answer = generate_hypothetical_query(query)\n",
    "\tvector_query = VectorizableTextQuery(\n",
    "\t\ttext=hypothetical_answer,\n",
    "\t\tk_nearest_neighbors=50,\n",
    "\t\tfields=\"vector\",\n",
    "\t)\n",
    "\t# Perform the search\n",
    "\tresults = search_client.search(\n",
    "\t\tquery_type='semantic',\n",
    "\t\tquery_language='ja',\n",
    "    \tsemantic_configuration_name='my-semantic-config',\n",
    "\t\tsearch_text=hypothetical_answer,\n",
    "\t\tvector_queries=[vector_query],\n",
    "\t\ttop=5,\n",
    "\t\tselect=\"chunk, title, key_phrases\",\n",
    "\t\tsearch_fields=[\"chunk\", \"title\", \"key_phrases\"],\n",
    "\t)\n",
    "\t\n",
    "\tcontext_text = \"\"\n",
    "\tfor result in results:\n",
    "\t\tcontext_text += result[\"chunk\"] + \" \"\n",
    "\n",
    "\treturn generate_answer(query, context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query set\n",
    "queries = [\n",
    "\t\"AI Search について勉強しています。ベクトル検索時の設定要素について教えてください\",\n",
    "\t\"ハイブリッド検索　メリット\",\n",
    "\t\"Azure AI Search には、リランクのモデルが利用できるか？\",\n",
    "\t\"フルテキスト検索の取得は最大何件か\",\n",
    "\t\"Hybrid検索とセマンティックランカーの特徴と違いはなんですか？\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for query in queries:\n",
    "    start_time = time.time()\n",
    "    answer = rag_pipeline_with_hyde(query)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
