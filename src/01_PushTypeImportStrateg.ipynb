{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 PUSH Type Import Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case\n",
    "- Azure AI Search の サービス仕様ドキュメントをインプットにする。\n",
    "  - https://learn.microsoft.com/ja-jp/azure/search/\n",
    "- ドキュメントは OCR が必要。\n",
    "- 開発者マニュアルは、構造化されたセクションとなっている。\n",
    "- 各セクションは非常に詳細かつ専門性の高い技術解説が記載されており、ドキュメントサイズも大きい。\n",
    "- ドキュメントには、テキスト、テーブル、図、グラフなどが含まれるが、ここでは、テキスト、テーブルデータのみを扱う。\n",
    "\n",
    "## チャンキング設計\n",
    "- Document Intelligence で、Markdown形式でテキストデータを抽出済み。\n",
    "- 1つのドキュメントに大量のコンテキストが含まれており、ドキュメントサイズも大きいため、チャンキングを実施する。\n",
    "- ドキュメントは、技術要素ごとに明確なセクションわけがされており、各セクションで見るとLLMが扱えないレベルのデータサイズではない。そのため、セクション単位でチャンキングする。\n",
    "- 各チャンキングのContentはEmbeddingする。\n",
    "- Overlapping は行わない。\n",
    "- 広い意味のコンテキストを保持するために、上位2つのヘッダー（Markdown形式：#, ##）をメタデータとして保持する。例えば、検索インデックスに関する記載があった場合に、それが「キーワード検索」に属する情報なのか、「ベクトル検索」に属する情報なのかを判断するために保持する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-dotenv langchain langchain-community langchain-openai langchainhub openai tiktoken azure-ai-documentintelligence azure-identity azure-search-documents==11.6.0b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code loads environment variables using the `dotenv` library and sets the necessary environment variables for Azure services.\n",
    "The environment variables are loaded from the `.env` file in the same directory as this notebook.\n",
    "\"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_content = read_file('../output/azure-search-concept.md')\n",
    "print(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting with Langchain Text Splitter\n",
    "https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n",
    "\n",
    "We use `MarkdownHeaderTextSplitter` for markdown document.\n",
    "  - https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/markdown_header_metadata/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into chunks base on markdown headers.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "# Include the headers in the splits.\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "splits = text_splitter.split_text(markdown_content)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(splits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_Index-Design\n",
    "Tips for Azure AI Search Index-Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUSH型\n",
    "- シンプルなパターンとして、ローカルのデータを検索インデックスにインポートさせる。\n",
    "- ユーザのコンテキストを捉えるために、ベクトル検索を採用する。そのため、Embedding フィールドを構成する。\n",
    "  - Embedding Model: `text-embedding-ada-002`を採用する。\n",
    "- ドキュメントのヘッダー情報を`Metadata`フィールドに含める。\n",
    "- ドキュメントのタイトルを`Title`フィールドに含める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    ScoringProfile,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    TextWeights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2024-02-01\",\n",
    ")\n",
    "\n",
    "vector_store_address: str = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "vector_store_password: str = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        filterable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "        analyzer_name=\"ja.microsoft\"\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(aoai_embeddings.embed_query(\"Text\")),\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "        analyzer_name=\"ja.microsoft\"\n",
    "    ),\n",
    "    # Additional field to store the title\n",
    "    SearchableField(\n",
    "        name=\"title\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "        analyzer_name=\"ja.microsoft\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "index_name: str = \"rag-search-index-0619\"\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=aoai_embeddings.embed_query,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改善\n",
    "不要な情報を検索インデックスに含めないようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the specific chapter from the index\n",
    "keywords = [\"関連項目\", \"次のステップ\"]\n",
    "for split in splits:\n",
    "    if any(keyword in value for value in split.metadata.values() for keyword in keywords):\n",
    "        print(split.metadata)\n",
    "        continue\n",
    "    vector_store.add_texts(\n",
    "\t\t[split.page_content],\n",
    "  \t\t[\n",
    "        \t{\"title\": f\"{os.path.basename('../data/azure-search-concept.pdf')}\"}\n",
    "        ]\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_Query-Design\n",
    "Tips for Azure AI Search Query-Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Search\n",
    "- ユーザのクエリのコンテキストをとらえたい、かつサービスに特化したワードがクエリに含まれる可能性が高いため、フルテキスト検索＋ベクトル検索を組み合わせたHybrid検索を採用する。\n",
    "  - Hybrid検索のスコアはAzure AI Searchでは、Reciprocal Rank Fusion (RRF) が採用される。 \n",
    "- クエリはユーザのクエリをそのまま検索インデックスのクエリに利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ベクトル検索時の設定要素について教えてください\"\n",
    "res_simple_query = vector_store.similarity_search(\n",
    "    \tquery=query, k=3, search_type=\"hybrid\"\n",
    ")\n",
    "res_simple_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Extensions\n",
    "- ユーザのクエリのコンテキストをとらえたい、かつサービスに特化したワードがクエリに含まれる可能性が高いため、フルテキスト検索＋ベクトル検索を組み合わせたHybrid検索を採用する。\n",
    "  - Hybrid検索のスコアはAzure AI Searchでは、Reciprocal Rank Fusion (RRF) が採用される。 \n",
    "- また、ユーザクエリから検索クエリを新しく生成する。\n",
    "  - クエリはユーザのクエリをスタンドアローンなクエリに変換する。\n",
    "  - また、検索のカバレッジを大きくするために、類似した入力クエリを複数生成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "system_message = \"\"\"\n",
    "# Your Task\n",
    "- Given the following conversation history and the users next question,rephrase the question to be a stand alone question.\n",
    "- You also need to extend the original question to generate 5 related queries. This is done to capture the broader context of the user's question.\n",
    "- You must output json format. In other words, You must output array of questions that length is 5.\n",
    "\n",
    "# Json format example:\n",
    "{\n",
    "\t\"questions\": [\n",
    "\t\t\"related question 1\",\n",
    "\t\t\"related question 2\",\n",
    "\t\t\"related question 3\",\n",
    "\t\t\"related question 4\",\n",
    "\t\t\"related question 5\"\n",
    "\t]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def correct_text_gpt(text):\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": system_message},\n",
    "\t\t{\"role\":\"user\",\"content\": text}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\tresponse_format={\"type\": \"json_object\"},\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ベクトル検索時の設定要素について教えてください\"\n",
    "expanded_query = correct_text_gpt(query)\n",
    "parsed_data = json.loads(expanded_query)\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in parsed_data[\"questions\"]:\n",
    "\tres_simple_query = vector_store.similarity_search(\n",
    "\t\tquery=question, k=3, search_type=\"hybrid\"\n",
    "\t)\n",
    "\tprint(res_simple_query)\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class ParaphrasedQuery(BaseModel):\n",
    "    \"\"\"You have performed query expansion to generate a paraphrasing of a question.\"\"\"\n",
    "\n",
    "    paraphrased_query: str = Field(\n",
    "        ...,\n",
    "        description=\"A unique paraphrasing of the original question.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "\n",
    "Perform query expansion. If there are multiple common ways of phrasing a user question \\\n",
    "or common synonyms for key words in the question, make sure to return multiple versions \\\n",
    "of the query with the different phrasings.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "\n",
    "Return at least 3 versions of the question.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "\topenai_api_version=\"2024-02-01\",\n",
    "    azure_deployment=\"gpt-4o\",\n",
    ")\n",
    "llm_with_tools = llm.bind_tools([ParaphrasedQuery])\n",
    "query_analyzer = prompt | llm_with_tools | PydanticToolsParser(tools=[ParaphrasedQuery])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_query = query_analyzer.invoke(\n",
    "    {\n",
    "        \"question\": query,\n",
    "    }\n",
    ")\n",
    "expanded_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in expanded_query:\n",
    "\tres_simple_query = vector_store.similarity_search(\n",
    "\t\tquery=question.paraphrased_query, k=3, search_type=\"hybrid\"\n",
    "\t)\n",
    "\tprint(res_simple_query)\n",
    "\tprint(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE (Hypothetical Document Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "\n",
    "\n",
    "def correct_text_gpt(text):\n",
    "    hypothetical_gen_instruction = f\"\"\"Please write a passage to answer the question\n",
    "\tQuestion: {text}\n",
    "\tPassage:\n",
    "\t\"\"\"\n",
    "    message_text = [\n",
    "\t\t{\"role\":\"system\",\"content\": \"You are an AI assistant.\"},\n",
    "\t\t{\"role\":\"user\",\"content\": hypothetical_gen_instruction}\n",
    "\t]\n",
    "    completion = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o\", # model = \"deployment_name\"\n",
    "\t\tmessages = message_text,\n",
    "\t\t# response_format={\"type\": \"json_object\"},\n",
    "\t\ttemperature=0,\n",
    "\t\t)\n",
    "    return completion.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"ベクトル検索時の設定要素について教えてください\"\n",
    "hypothetical_answer = correct_text_gpt(query)\n",
    "hypothetical_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_simple_query = vector_store.similarity_search(\n",
    "\t\tquery=hypothetical_answer, k=3, search_type=\"hybrid\"\n",
    "\t)\n",
    "res_simple_query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
